{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-28 16:39:12.972866 model.py code start\n",
      "NVIDIA GeForce RTX 3080 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "from ignite.contrib.metrics.regression import *\n",
    "from ignite.contrib.metrics import *\n",
    "from ignite.handlers import *\n",
    "from ignite.metrics import *\n",
    "from ignite.engine import *\n",
    "from ignite.utils import *\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "\n",
    "from active_learning.active_learning import ActiveLearner\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from collections import OrderedDict\n",
    "from scipy import signal\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pickle\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "\n",
    "print(datetime.datetime.now(), \"model.py code start\")\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "EPOCH = 100\n",
    "LEARNING_RATE = 0.2\n",
    "ANNEALING_RATE = 0.999\n",
    "HIDDEN_UNITS = [180, 200, 250, 80, 100, 120]\n",
    "K_FOLD = 1\n",
    "VISIBLE_NUM = 50\n",
    "MAT_PATH = \"C:/Users/HILAB_Labtop_02/Desktop/insung/ecg-dbn/data/mit.mat\"\n",
    "\n",
    "PATH = \"./pickle/mit\"\n",
    "DB1_LIST = [101, 106, 108, 109, 112, 114, 116, 118, 119, 122, 124, 201, 203, 205, 207, 208, 209, 215, 220, 221, 223, 230]\n",
    "DB2_LIST = [100, 103, 105, 111, 113, 117, 121, 123, 200, 202, 210, 212, 213, 214, 219, 221, 222, 228, 231, 232, 233, 234]\n",
    "\n",
    "db1_sig, db1_ann = list(), list()\n",
    "db1_resampled_sig, db1_resampled_ann = list(), list()\n",
    "db3_resampled_sig, db3_resampled_ann = list(), list()\n",
    "\n",
    "device = torch.device('cuda')\n",
    "print(torch.cuda.get_device_name(device))\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBM(nn.Module): \n",
    "    with torch.cuda.device(0):\n",
    "        def __init__(self, n_vis, n_hid, k, batch):\n",
    "            super(RBM, self).__init__()\n",
    "            self.W      = nn.Parameter(torch.randn(1, batch, device=device) * 1e-2)\n",
    "            self.n_vis  = n_vis\n",
    "            self.n_hid  = n_hid\n",
    "            self.k      = k\n",
    "            self.batch  = batch\n",
    "            self.v_bias = nn.Parameter(torch.zeros(n_vis, 1, device=device))\n",
    "            self.h_bias = nn.Parameter(torch.zeros(n_hid, 1, device=device))\n",
    "        \n",
    "        def sample_from_p(self, p):\n",
    "            return F.relu(\n",
    "                torch.sign(\n",
    "                    p - Variable(torch.randn(p.size(), device=device))\n",
    "                )\n",
    "            ).to(device=device)\n",
    "\n",
    "        ''' ISSUE PART '''\n",
    "        def v_to_h(self, v):\n",
    "            w = (self.W.clone())\n",
    "            # print(\"v -> h\", v.size(), w.size())\n",
    "            \n",
    "            p_h = F.sigmoid(\n",
    "                F.linear(v, w)\n",
    "            ).to(device=device)\n",
    "\n",
    "            sample_h = self.sample_from_p(p_h)\n",
    "            return p_h, sample_h\n",
    "\n",
    "        def h_to_v(self, h):\n",
    "            w = self.W.t().clone()\n",
    "            # print(\"h -> v\", h.size(), w.size())\n",
    "\n",
    "            p_v = F.sigmoid(\n",
    "                F.linear(h, w)\n",
    "            ).to(device=device)\n",
    "\n",
    "            sample_v = self.sample_from_p(p_v)\n",
    "            return p_v, sample_v\n",
    "        \n",
    "        def forward(self, v):\n",
    "            pre_h1, h1 = self.v_to_h(v)\n",
    "            h_ = h1\n",
    "\n",
    "            for _ in range(self.k):\n",
    "                pre_v_, v_ = self.h_to_v(h_)\n",
    "                pre_h_, h_ = self.v_to_h(v_)\n",
    "            return v, v_\n",
    "        \n",
    "        def get_weight(self):\n",
    "            return self.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_step(engine, batch):\n",
    "    return batch\n",
    "\n",
    "default_model = nn.Sequential(OrderedDict([\n",
    "    ('base', nn.Linear(4, 2)),\n",
    "    ('fc', nn.Linear(2, 1))\n",
    "]))\n",
    "\n",
    "default_evaluator = Engine(eval_step)\n",
    "\n",
    "def get_acc(y_true, y_pred):\n",
    "    metric = Accuracy()\n",
    "    metric.attach(default_evaluator, \"accuracy\")\n",
    "    state = default_evaluator.run([[y_pred, y_true]])\n",
    "    return state.metrics[\"accuracy\"]\n",
    "    \n",
    "def to_categorical(y, num_classes):\n",
    "    return np.eye(num_classes, dtype='uint8')[y]\n",
    "\n",
    "def svm(x, w, b):\n",
    "    h = (w*x).sum(1) + b\n",
    "    return h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM_Model(nn.Module):\n",
    "    def __init__ (self, D_in, D_out):\n",
    "        super(SVM_Model, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, D_out, device=device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 22\n",
      "22 22\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(DB1_LIST)):\n",
    "    pickle_path = PATH + str(DB1_LIST[i]) + \".pkl\"\n",
    "    with open(pickle_path, 'rb') as f:\n",
    "        db1_sig.append(pickle.load(f)[0])\n",
    "        \n",
    "    with open(pickle_path, 'rb') as f:\n",
    "        db1_ann.append(pickle.load(f)[1])\n",
    "        \n",
    "print(len(db1_sig), len(db1_ann))\n",
    "\n",
    "temp_sg, temp_ann = list(), list()\n",
    "\n",
    "for i in range(len(db1_sig)):\n",
    "    for j in range(len(db1_sig[i])):\n",
    "        temp_sg.append(signal.resample(db1_sig[i][j], 50))\n",
    "    db1_resampled_sig.append(temp_sg)\n",
    "\n",
    "for i in range(len(db1_ann)):\n",
    "    for j in range(len(db1_ann[i])):\n",
    "        if db1_ann[i][j] == \"N\":\n",
    "            # temp_ann.append(db1_ann[i][j])\n",
    "            temp_ann.append(0)\n",
    "        \n",
    "        elif db1_ann[i][j] == \"S\" or \"V\" or \"F\":\n",
    "            # temp_ann.append(db1_ann[i][j])\n",
    "            temp_ann.append(1)\n",
    "        \n",
    "        else:\n",
    "            temp_ann.append(1)\n",
    "    \n",
    "    db1_resampled_ann.append(temp_ann)\n",
    "\n",
    "print(len(db1_resampled_sig), len(db1_resampled_ann))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RSLT]\t\t\t Export records ...\n",
      "100 100\n",
      "100 100\n"
     ]
    }
   ],
   "source": [
    "PATH = \"./svdb_pic/\"\n",
    "\n",
    "with open(PATH + 'RECORDS') as f:\n",
    "    record_lines = f.readlines()\n",
    "\n",
    "pre_records = []\n",
    "for x in record_lines:\n",
    "    pre_records.append(x.strip())\n",
    "print(\"[RSLT]\\t\\t\\t Export records ...\")\n",
    "\n",
    "for i in range(len(pre_records)):\n",
    "    pickle_path = PATH + \"svdb\" + str(pre_records[i]) + \".pkl\"\n",
    "    with open(pickle_path, 'rb') as f:\n",
    "        db1_sig.append(pickle.load(f)[0])\n",
    "        \n",
    "    with open(pickle_path, 'rb') as f:\n",
    "        db1_ann.append(pickle.load(f)[1])\n",
    "        \n",
    "print(len(db1_sig), len(db1_ann))\n",
    "\n",
    "temp_sg, temp_ann = list(), list()\n",
    "\n",
    "for i in range(len(db1_sig)):\n",
    "    for j in range(len(db1_sig[i])):\n",
    "        temp_sg.append(signal.resample(db1_sig[i][j], 50))\n",
    "    db3_resampled_sig.append(temp_sg)\n",
    "\n",
    "for i in range(len(db1_ann)):\n",
    "    for j in range(len(db1_ann[i])):\n",
    "        if db1_ann[i][j] == \"N\":\n",
    "            # temp_ann.append(db1_ann[i][j])\n",
    "            temp_ann.append(0)\n",
    "        \n",
    "        elif db1_ann[i][j] == \"S\" or \"V\" or \"F\":\n",
    "            # temp_ann.append(db1_ann[i][j])\n",
    "            temp_ann.append(1)\n",
    "        \n",
    "        else:\n",
    "            temp_ann.append(\"F\")\n",
    "    \n",
    "    db3_resampled_ann.append(temp_ann)\n",
    "\n",
    "print(len(db3_resampled_sig), len(db3_resampled_ann))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50])\n"
     ]
    }
   ],
   "source": [
    "bbrbm_first = RBM(n_vis=VISIBLE_NUM, n_hid=HIDDEN_UNITS[3], k=K_FOLD, batch=BATCH_SIZE).to(device=device)\n",
    "bbrbm_second = RBM(n_vis=VISIBLE_NUM, n_hid=HIDDEN_UNITS[4], k=K_FOLD, batch=BATCH_SIZE).to(device=device)\n",
    "bbrbm_third = RBM(n_vis=VISIBLE_NUM, n_hid=HIDDEN_UNITS[5], k=K_FOLD, batch=BATCH_SIZE).to(device=device)\n",
    "\n",
    "gbrbm_first = RBM(n_vis=VISIBLE_NUM, n_hid=HIDDEN_UNITS[0], k=K_FOLD, batch=BATCH_SIZE).to(device=device)\n",
    "gbrbm_second = RBM(n_vis=VISIBLE_NUM, n_hid=HIDDEN_UNITS[1], k=K_FOLD, batch=BATCH_SIZE).to(device=device)\n",
    "gbrbm_third = RBM(n_vis=VISIBLE_NUM, n_hid=HIDDEN_UNITS[2], k=K_FOLD, batch=BATCH_SIZE).to(device=device)\n",
    "\n",
    "first_train_op = optim.Adagrad(bbrbm_first.parameters(), LEARNING_RATE)\n",
    "second_train_op = optim.Adagrad(bbrbm_second.parameters(), LEARNING_RATE)\n",
    "third_train_op = optim.Adagrad(bbrbm_third.parameters(), LEARNING_RATE)\n",
    "\n",
    "gb_first_train_op = optim.Adagrad(gbrbm_first.parameters(), LEARNING_RATE)\n",
    "gb_second_train_op = optim.Adagrad(gbrbm_second.parameters(), LEARNING_RATE)\n",
    "gb_third_train_op = optim.Adagrad(gbrbm_third.parameters(), LEARNING_RATE)\n",
    "\n",
    "omse_loss = list()\n",
    "output_gb = list()\n",
    "best_acc = float()\n",
    "svm_best_acc = float()\n",
    "mse_loss = nn.MSELoss().to(device=device)\n",
    "        \n",
    "svm_model = SVM_Model(50, 2)\n",
    "criterion = torch.nn.MSELoss(reduction='sum').to(device=device)\n",
    "svm_optimizer = torch.optim.SGD(params=svm_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# gaussian_std = torch.arange(1, 0, -0.00537, device=device)\n",
    "gaussian_std = torch.arange(1, 0, -0.02, device=device)\n",
    "print(gaussian_std.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = DataLoader(db1_resampled_sig,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              num_workers=0,\n",
    "                              shuffle=True)\n",
    "\n",
    "print(len(train_dataloader.dataset))\n",
    "\n",
    "test_dataloader = DataLoader(db3_resampled_sig,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             num_workers=0,\n",
    "                             shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RBM START!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HILAB_~1\\AppData\\Local\\Temp/ipykernel_10220/574457983.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data = Variable(torch.tensor(data[0].to(device), dtype=torch.float32))\n",
      "C:\\Users\\HILAB_~1\\AppData\\Local\\Temp/ipykernel_10220/574457983.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data = Variable(torch.tensor(data, dtype=torch.float32))\n",
      "C:\\Users\\HILAB_~1\\AppData\\Local\\Temp/ipykernel_10220/574457983.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data = Variable(torch.tensor(data).type(torch.float32))\n",
      "C:\\Users\\HILAB_~1\\AppData\\Local\\Temp/ipykernel_10220/574457983.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data = Variable(torch.tensor(v3_e, dtype=torch.float32)).uniform_(0, 1)\n",
      "C:\\Users\\HILAB_~1\\AppData\\Local\\Temp/ipykernel_10220/574457983.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data = Variable(torch.tensor(v1, dtype=torch.float32)).uniform_(0, 1)\n",
      "C:\\Users\\HILAB_~1\\AppData\\Local\\Temp/ipykernel_10220/574457983.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data = Variable(torch.tensor(v2, dtype=torch.float32)).uniform_(0, 1)\n",
      "C:\\Users\\HILAB_~1\\AppData\\Local\\Temp/ipykernel_10220/574457983.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(v3, dtype=torch.float)\n",
      "c:\\Users\\HILAB_Labtop_02\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:529: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([583154, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GB-DBN Training loss for 1th epoch 0.4399999976158142\tEstimate time : 75.27152419090271\tAcc : 66.0\t\tBest Acc : 66.0\tSVM Acc & Predicted: nan, tensor([nan, nan], device='cuda:0')\n",
      "GB-DBN Training loss for 2th epoch 0.5799999833106995\tEstimate time : 74.5737977027893\tAcc : 56.00000000000001\t\tBest Acc : 66.0\tSVM Acc & Predicted: nan, tensor([nan, nan], device='cuda:0')\n",
      "GB-DBN Training loss for 3th epoch 0.4399999976158142\tEstimate time : 75.32550835609436\tAcc : 84.0\t\tBest Acc : 84.0\tSVM Acc & Predicted: nan, tensor([nan, nan], device='cuda:0')\n",
      "GB-DBN Training loss for 4th epoch 0.4599999785423279\tEstimate time : 74.87954068183899\tAcc : 78.0\t\tBest Acc : 84.0\tSVM Acc & Predicted: nan, tensor([nan, nan], device='cuda:0')\n",
      "GB-DBN Training loss for 5th epoch 0.47999998927116394\tEstimate time : 78.4739089012146\tAcc : 68.0\t\tBest Acc : 84.0\tSVM Acc & Predicted: nan, tensor([nan, nan], device='cuda:0')\n",
      "GB-DBN Training loss for 6th epoch 0.4399999976158142\tEstimate time : 78.29322957992554\tAcc : 74.0\t\tBest Acc : 84.0\tSVM Acc & Predicted: nan, tensor([nan, nan], device='cuda:0')\n",
      "GB-DBN Training loss for 7th epoch 0.5\tEstimate time : 77.94491195678711\tAcc : 52.0\t\tBest Acc : 84.0\tSVM Acc & Predicted: nan, tensor([nan, nan], device='cuda:0')\n",
      "GB-DBN Training loss for 8th epoch 0.4599999785423279\tEstimate time : 79.16034650802612\tAcc : 82.0\t\tBest Acc : 84.0\tSVM Acc & Predicted: nan, tensor([nan, nan], device='cuda:0')\n",
      "GB-DBN Training loss for 9th epoch 0.3799999952316284\tEstimate time : 77.21852731704712\tAcc : 62.0\t\tBest Acc : 84.0\tSVM Acc & Predicted: nan, tensor([nan, nan], device='cuda:0')\n",
      "GB-DBN Training loss for 10th epoch 0.5600000023841858\tEstimate time : 75.35447192192078\tAcc : 72.0\t\tBest Acc : 84.0\tSVM Acc & Predicted: nan, tensor([nan, nan], device='cuda:0')\n",
      "GB-DBN Training loss for 11th epoch 0.5399999618530273\tEstimate time : 74.54015564918518\tAcc : 72.0\t\tBest Acc : 84.0\tSVM Acc & Predicted: nan, tensor([nan, nan], device='cuda:0')\n",
      "GB-DBN Training loss for 12th epoch 0.5199999809265137\tEstimate time : 76.3064386844635\tAcc : 76.0\t\tBest Acc : 84.0\tSVM Acc & Predicted: nan, tensor([nan, nan], device='cuda:0')\n",
      "GB-DBN Training loss for 13th epoch 0.4599999785423279\tEstimate time : 79.23878860473633\tAcc : 54.0\t\tBest Acc : 84.0\tSVM Acc & Predicted: nan, tensor([nan, nan], device='cuda:0')\n",
      "GB-DBN Training loss for 14th epoch 0.5199999809265137\tEstimate time : 76.89574360847473\tAcc : 72.0\t\tBest Acc : 84.0\tSVM Acc & Predicted: nan, tensor([nan, nan], device='cuda:0')\n",
      "GB-DBN Training loss for 15th epoch 0.35999998450279236\tEstimate time : 74.3060212135315\tAcc : 74.0\t\tBest Acc : 84.0\tSVM Acc & Predicted: nan, tensor([nan, nan], device='cuda:0')\n",
      "GB-DBN Training loss for 16th epoch 0.35999998450279236\tEstimate time : 76.31653475761414\tAcc : 70.0\t\tBest Acc : 84.0\tSVM Acc & Predicted: nan, tensor([nan, nan], device='cuda:0')\n",
      "GB-DBN Training loss for 17th epoch 0.5799999833106995\tEstimate time : 77.25683331489563\tAcc : 86.0\t\tBest Acc : 86.0\tSVM Acc & Predicted: nan, tensor([nan, nan], device='cuda:0')\n",
      "GB-DBN Training loss for 18th epoch 0.35999998450279236\tEstimate time : 76.18895411491394\tAcc : 64.0\t\tBest Acc : 86.0\tSVM Acc & Predicted: nan, tensor([nan, nan], device='cuda:0')\n",
      "GB-DBN Training loss for 19th epoch 0.5399999618530273\tEstimate time : 80.8198516368866\tAcc : 74.0\t\tBest Acc : 86.0\tSVM Acc & Predicted: nan, tensor([nan, nan], device='cuda:0')\n",
      "GB-DBN Training loss for 20th epoch 0.4599999785423279\tEstimate time : 76.61083602905273\tAcc : 64.0\t\tBest Acc : 86.0\tSVM Acc & Predicted: nan, tensor([nan, nan], device='cuda:0')\n",
      "GB-DBN Training loss for 21th epoch 0.3999999761581421\tEstimate time : 78.0172975063324\tAcc : 66.0\t\tBest Acc : 86.0\tSVM Acc & Predicted: nan, tensor([nan, nan], device='cuda:0')\n",
      "GB-DBN Training loss for 22th epoch 0.5799999833106995\tEstimate time : 79.0381453037262\tAcc : 64.0\t\tBest Acc : 86.0\tSVM Acc & Predicted: nan, tensor([nan, nan], device='cuda:0')\n",
      "GB-DBN Training loss for 23th epoch 0.5199999809265137\tEstimate time : 77.00200653076172\tAcc : 70.0\t\tBest Acc : 86.0\tSVM Acc & Predicted: nan, tensor([nan, nan], device='cuda:0')\n",
      "GB-DBN Training loss for 24th epoch 0.41999998688697815\tEstimate time : 75.37195158004761\tAcc : 72.0\t\tBest Acc : 86.0\tSVM Acc & Predicted: nan, tensor([nan, nan], device='cuda:0')\n",
      "GB-DBN Training loss for 25th epoch 0.5399999618530273\tEstimate time : 75.21089053153992\tAcc : 66.0\t\tBest Acc : 86.0\tSVM Acc & Predicted: nan, tensor([nan, nan], device='cuda:0')\n",
      "GB-DBN Training loss for 26th epoch 0.5999999642372131\tEstimate time : 75.76255297660828\tAcc : 64.0\t\tBest Acc : 86.0\tSVM Acc & Predicted: nan, tensor([nan, nan], device='cuda:0')\n",
      "GB-DBN Training loss for 27th epoch 0.47999998927116394\tEstimate time : 73.9981837272644\tAcc : 68.0\t\tBest Acc : 86.0\tSVM Acc & Predicted: nan, tensor([nan, nan], device='cuda:0')\n",
      "GB-DBN Training loss for 28th epoch 0.4599999785423279\tEstimate time : 77.5219054222107\tAcc : 72.0\t\tBest Acc : 86.0\tSVM Acc & Predicted: nan, tensor([nan, nan], device='cuda:0')\n",
      "GB-DBN Training loss for 29th epoch 0.41999998688697815\tEstimate time : 77.75969958305359\tAcc : 76.0\t\tBest Acc : 86.0\tSVM Acc & Predicted: nan, tensor([nan, nan], device='cuda:0')\n",
      "GB-DBN Training loss for 30th epoch 0.4399999976158142\tEstimate time : 77.90973567962646\tAcc : 68.0\t\tBest Acc : 86.0\tSVM Acc & Predicted: nan, tensor([nan, nan], device='cuda:0')\n",
      "GB-DBN Training loss for 31th epoch 0.5999999642372131\tEstimate time : 77.61362624168396\tAcc : 78.0\t\tBest Acc : 86.0\tSVM Acc & Predicted: nan, tensor([nan, nan], device='cuda:0')\n",
      "GB-DBN Training loss for 32th epoch 0.47999998927116394\tEstimate time : 77.74109864234924\tAcc : 62.0\t\tBest Acc : 86.0\tSVM Acc & Predicted: nan, tensor([nan, nan], device='cuda:0')\n",
      "GB-DBN Training loss for 33th epoch 0.5\tEstimate time : 77.60264158248901\tAcc : 70.0\t\tBest Acc : 86.0\tSVM Acc & Predicted: nan, tensor([nan, nan], device='cuda:0')\n",
      "GB-DBN Training loss for 34th epoch 0.4599999785423279\tEstimate time : 77.82939600944519\tAcc : 72.0\t\tBest Acc : 86.0\tSVM Acc & Predicted: nan, tensor([nan, nan], device='cuda:0')\n",
      "GB-DBN Training loss for 35th epoch 0.3999999761581421\tEstimate time : 77.91285276412964\tAcc : 74.0\t\tBest Acc : 86.0\tSVM Acc & Predicted: nan, tensor([nan, nan], device='cuda:0')\n",
      "GB-DBN Training loss for 36th epoch 0.5\tEstimate time : 77.58102798461914\tAcc : 68.0\t\tBest Acc : 86.0\tSVM Acc & Predicted: nan, tensor([nan, nan], device='cuda:0')\n",
      "GB-DBN Training loss for 37th epoch 0.47999998927116394\tEstimate time : 77.63493132591248\tAcc : 76.0\t\tBest Acc : 86.0\tSVM Acc & Predicted: nan, tensor([nan, nan], device='cuda:0')\n",
      "GB-DBN Training loss for 38th epoch 0.4599999785423279\tEstimate time : 77.72734141349792\tAcc : 82.0\t\tBest Acc : 86.0\tSVM Acc & Predicted: nan, tensor([nan, nan], device='cuda:0')\n",
      "GB-DBN Training loss for 39th epoch 0.35999998450279236\tEstimate time : 77.74728679656982\tAcc : 70.0\t\tBest Acc : 86.0\tSVM Acc & Predicted: nan, tensor([nan, nan], device='cuda:0')\n",
      "GB-DBN Training loss for 40th epoch 0.5199999809265137\tEstimate time : 77.91918516159058\tAcc : 60.0\t\tBest Acc : 86.0\tSVM Acc & Predicted: nan, tensor([nan, nan], device='cuda:0')\n",
      "GB-DBN Training loss for 41th epoch 0.5\tEstimate time : 78.29918789863586\tAcc : 70.0\t\tBest Acc : 86.0\tSVM Acc & Predicted: nan, tensor([nan, nan], device='cuda:0')\n",
      "GB-DBN Training loss for 42th epoch 0.47999998927116394\tEstimate time : 78.06668138504028\tAcc : 62.0\t\tBest Acc : 86.0\tSVM Acc & Predicted: nan, tensor([nan, nan], device='cuda:0')\n",
      "GB-DBN Training loss for 43th epoch 0.5799999833106995\tEstimate time : 78.05913519859314\tAcc : 68.0\t\tBest Acc : 86.0\tSVM Acc & Predicted: nan, tensor([nan, nan], device='cuda:0')\n",
      "GB-DBN Training loss for 44th epoch 0.4399999976158142\tEstimate time : 78.1966245174408\tAcc : 68.0\t\tBest Acc : 86.0\tSVM Acc & Predicted: nan, tensor([nan, nan], device='cuda:0')\n",
      "GB-DBN Training loss for 45th epoch 0.5199999809265137\tEstimate time : 77.71919822692871\tAcc : 84.0\t\tBest Acc : 86.0\tSVM Acc & Predicted: nan, tensor([nan, nan], device='cuda:0')\n",
      "GB-DBN Training loss for 46th epoch 0.3999999761581421\tEstimate time : 76.67011499404907\tAcc : 78.0\t\tBest Acc : 86.0\tSVM Acc & Predicted: nan, tensor([nan, nan], device='cuda:0')\n",
      "GB-DBN Training loss for 47th epoch 0.5\tEstimate time : 79.7300238609314\tAcc : 82.0\t\tBest Acc : 86.0\tSVM Acc & Predicted: nan, tensor([nan, nan], device='cuda:0')\n",
      "GB-DBN Training loss for 48th epoch 0.5799999833106995\tEstimate time : 78.62376642227173\tAcc : 74.0\t\tBest Acc : 86.0\tSVM Acc & Predicted: nan, tensor([nan, nan], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "'''BBRBM Train Part'''\n",
    "\n",
    "loss_ = []\n",
    "output_bb = []\n",
    "temp_v1 = []\n",
    "model_path_str = str()\n",
    "\n",
    "print(\"RBM START!\")\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    tmp_acc = float()\n",
    "    run_acc = float()\n",
    "    start = time.time()\n",
    "\n",
    "    for i, (data) in enumerate(train_dataloader):\n",
    "        data = Variable(torch.tensor(data[0].to(device), dtype=torch.float32))\n",
    "        sample_data = torch.normal(mean=data, std=gaussian_std).view(-1, 50).to(device=device)\n",
    "        \n",
    "        # tensor binary\n",
    "        vog_first, v1 = gbrbm_first(sample_data)\n",
    "        temp_v1.append(v1)\n",
    "        \n",
    "        omse_loss = mse_loss(vog_first, v1)\n",
    "        \n",
    "        gb_first_train_op.zero_grad()\n",
    "        gb_first_train_op.step()\n",
    "        omse_loss.backward()\n",
    "\n",
    "    for _, (data) in enumerate(v1):         \n",
    "        data = Variable(torch.tensor(data, dtype=torch.float32))\n",
    "        sample_data = torch.normal(mean=data, std=gaussian_std).to(device=device)\n",
    "\n",
    "        # tensor binary\n",
    "        vog_second, v2 = gbrbm_second(sample_data)\n",
    "        omse_loss = mse_loss(vog_second, v2)\n",
    "\n",
    "        gb_second_train_op.zero_grad()\n",
    "        omse_loss.backward()\n",
    "        gb_second_train_op.step()\n",
    "\n",
    "    for _, (data) in enumerate(v2):\n",
    "        data = Variable(torch.tensor(data).type(torch.float32))\n",
    "        sample_data = torch.normal(mean=data, std=gaussian_std).to(device=device)\n",
    "\n",
    "        vog_third, v3_e = gbrbm_third(sample_data)\n",
    "        omse_loss = mse_loss(vog_third, v3_e)\n",
    "        \n",
    "        gb_third_train_op.zero_grad()\n",
    "        gb_third_train_op.step()\n",
    "        omse_loss.backward()\n",
    "\n",
    "        '''First bbrbm'''\n",
    "        ''' ISSUE PART '''\n",
    "        data = Variable(torch.tensor(v3_e, dtype=torch.float32)).uniform_(0, 1)\n",
    "        sample_data = torch.bernoulli(data).to(device=device)\n",
    "        \n",
    "        # tensor binary\n",
    "        fvog_first, v1 = bbrbm_first(sample_data)\n",
    "        omse_loss = mse_loss(fvog_first, v1)\n",
    "\n",
    "        first_train_op.zero_grad()\n",
    "        first_train_op.step()\n",
    "        omse_loss.backward()\n",
    "\n",
    "        #  **** Second BBRBM **** #\n",
    "        data = Variable(torch.tensor(v1, dtype=torch.float32)).uniform_(0, 1)\n",
    "        sample_data = torch.bernoulli(data).to(device=device)\n",
    "\n",
    "        # tensor binary\n",
    "        vog_second, v2 = bbrbm_second(sample_data)\n",
    "        omse_loss = mse_loss(vog_second, v2)\n",
    "\n",
    "        second_train_op.zero_grad()\n",
    "        omse_loss.backward()\n",
    "        second_train_op.step()\n",
    "\n",
    "        # **** THIRD BBRBM **** #\n",
    "        data = Variable(torch.tensor(v2, dtype=torch.float32)).uniform_(0, 1)\n",
    "\n",
    "        sample_data = torch.bernoulli(data).to(device=device)\n",
    "\n",
    "        vog_third, v3 = bbrbm_third(sample_data)\n",
    "        omse_loss = mse_loss(vog_third, v3)\n",
    "        \n",
    "        third_train_op.zero_grad()\n",
    "        omse_loss.backward()\n",
    "        third_train_op.step()\n",
    "\n",
    "        run_acc += (sample_data == v3).sum().item()\n",
    "\n",
    "        for i in range(len(v3)):\n",
    "            x = torch.tensor(v3, dtype=torch.float)\n",
    "            y = torch.FloatTensor(db1_resampled_ann).view(-1, 2).to(device=device)\n",
    "\n",
    "            y_pred = svm_model(x)\n",
    "            sloss = criterion(y, y_pred)\n",
    "\n",
    "            svm_optimizer.zero_grad()\n",
    "            sloss.backward()\n",
    "            svm_optimizer.step()\n",
    "            \n",
    "            # svm_acc = get_acc(y, y_pred)\n",
    "            \n",
    "    svm_acc = \"?\"\n",
    "    acc_v = (vog_third >= 0).float()\n",
    "    acc = get_acc(\n",
    "        acc_v, v3\n",
    "    ) * 100\n",
    "    \n",
    "    if acc > best_acc:\n",
    "        best_acc = acc    \n",
    "        \n",
    "        path = \"./say_cheese/ahh_saveMode_through_\" + str(epoch) + \"_\" + str(acc) + \"GBRBM.pth\"\n",
    "        model_path_str = path\n",
    "        torch.save(gbrbm_third.state_dict(), path)\n",
    "    output_gb.append(v3)\n",
    "\n",
    "    print(\"GB-DBN Training loss for {0}th epoch {1}\\tEstimate time : {2}\\tAcc : {3}\\t\\tBest Acc : {4}\\tSVM Acc & Predicted: {5}, {6}\".format(epoch + 1, omse_loss, time.time() - start, acc, best_acc, sloss, y_pred.data))\n",
    "    \n",
    "    gc.collect()\n",
    "print(v3.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm_model(db1_resampled_sig).fit()\n",
    "\n",
    "AL = ActiveLearner(strategy='entropy')\n",
    "AL.rank(clf, db1_resampled_sig, num_queries=20)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6c3cabcf2f29820bdd7faae982b59d335e0d215fb5382d93f3312fa3292e9b7f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
