{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ignite.contrib.metrics.regression import *\n",
    "from ignite.contrib.metrics import *\n",
    "from ignite.handlers import *\n",
    "from ignite.metrics import *\n",
    "from ignite.engine import *\n",
    "from ignite.utils import *\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import data.read_samples as rs\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "\n",
    "print(datetime.datetime.now(), \"model.py code start\")\n",
    "\n",
    "BATCH_SIZE = 107\n",
    "EPOCH = 400\n",
    "LEARNING_RATE = 0.2\n",
    "ANNEALING_RATE = 0.999\n",
    "HIDDEN_UNITS = [180, 200, 250, 80, 100, 120]\n",
    "K_FOLD = 1\n",
    "MAT_PATH = \"C:/Users/HILAB_Labtop_02/Desktop/insung/ecg-dbn/data/mit.mat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "print(torch.cuda.get_device_name(device))\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RBM set-up as object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBM(nn.Module): \n",
    "    with torch.cuda.device(0):\n",
    "        def __init__(self, n_vis, n_hid, k, batch):\n",
    "            super(RBM, self).__init__()\n",
    "            self.W      = nn.Parameter(torch.randn(1, batch, device=device) * 1e-2)\n",
    "            self.n_vis  = n_vis\n",
    "            self.n_hid  = n_hid\n",
    "            self.k      = k\n",
    "            self.batch  = batch\n",
    "            self.v_bias = nn.Parameter(torch.zeros(n_vis, 1, device=device))\n",
    "            self.h_bias = nn.Parameter(torch.zeros(n_hid, 1, device=device))\n",
    "        \n",
    "        def sample_from_p(self, p):\n",
    "            return F.relu(\n",
    "                torch.sign(\n",
    "                    p - Variable(torch.randn(p.size(), device=device))\n",
    "                )\n",
    "            ).to(device=device)\n",
    "\n",
    "        ''' ISSUE PART '''\n",
    "        def v_to_h(self, v):\n",
    "            w = (self.W.clone())\n",
    "\n",
    "            p_h = F.sigmoid(\n",
    "                F.linear(v, w)\n",
    "            ).to(device=device)\n",
    "\n",
    "            sample_h = self.sample_from_p(p_h)\n",
    "            return p_h, sample_h\n",
    "\n",
    "        def h_to_v(self, h):\n",
    "            w = self.W.t().clone()\n",
    "\n",
    "            p_v = F.sigmoid(\n",
    "                F.linear(h, w)\n",
    "            ).to(device=device)\n",
    "\n",
    "            sample_v = self.sample_from_p(p_v)\n",
    "            return p_v, sample_v\n",
    "        \n",
    "        def forward(self, v):\n",
    "            pre_h1, h1 = self.v_to_h(v)\n",
    "            h_ = h1\n",
    "\n",
    "            for _ in range(self.k):\n",
    "                pre_v_, v_ = self.h_to_v(h_)\n",
    "                pre_h_, h_ = self.v_to_h(v_)\n",
    "            return v, v_\n",
    "        \n",
    "        def get_weight(self):\n",
    "            return self.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM(nn.Module):\n",
    "    with torch.cuda.device(0):\n",
    "        def __init__(self, lr, n_x):\n",
    "            super(SVM, self).__init__()\n",
    "            self.lr = lr\n",
    "            self.fully = nn.Linear(n_x, 1).to(device=device)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            fwd = self.fully(x)\n",
    "            return fwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init of custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_step(engine, batch):\n",
    "    return batch\n",
    "\n",
    "default_model = nn.Sequential(OrderedDict([\n",
    "    ('base', nn.Linear(4, 2)),\n",
    "    ('fc', nn.Linear(2, 1))\n",
    "]))\n",
    "\n",
    "default_evaluator = Engine(eval_step)\n",
    "\n",
    "def get_acc(y_true, y_pred):\n",
    "    metric = Accuracy()\n",
    "    metric.attach(default_evaluator, \"accuracy\")\n",
    "    state = default_evaluator.run([[y_pred, y_true]])\n",
    "    return state.metrics[\"accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(y, num_classes):\n",
    "    return np.eye(num_classes, dtype='uint8')[y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mit = io.loadmat(MAT_PATH)\n",
    "# mit = mit['mit']\n",
    "# df_mit = pd.DataFrame(data = mit)\n",
    "\n",
    "# print(df_mit[428].value_counts(), '\\n')\n",
    "# # 0 = N\n",
    "# # 1 = Q\n",
    "# # 2 = S\n",
    "# # 3 = V\n",
    "# # 4 = F\n",
    "\n",
    "# Y = np.array(df_mit[428].values).astype(np.int8)\n",
    "# X = np.array(df_mit[list(range(428))].values)[..., np.newaxis]\n",
    "\n",
    "# oneHot = LabelEncoder()\n",
    "# oneHot.fit(Y)\n",
    "# Y = oneHot.transform(Y)\n",
    "\n",
    "# X = X.reshape(-1, 428, 1)\n",
    "# Y = to_categorical(Y, 5)\n",
    "\n",
    "# X_train, X_val, Y_train, Y_val = train_test_split(X, Y,\n",
    "#                                                 test_size=0.3,\n",
    "#                                                 random_state=42,\n",
    "#                                                 stratify=Y)\n",
    "\n",
    "# print(\"X_train shape: \", X_train.shape)\n",
    "# print(\"Y_train shape: \", Y_train.shape)\n",
    "# print(\"X_val shape: \", X_val.shape)\n",
    "# print(\"Y_val shape: \", Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[MODL] Model main code is starting....\")\n",
    "\n",
    "print(\"[INFO] Read train data, cross-vaildation data and test data from median filtering code\")\n",
    "db1_sig, db1_label, db2_sig, db2_label, db3_sig, db3_label = rs.return_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(db1_sig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(db1_sig,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              num_workers=0,\n",
    "                              shuffle=True)\n",
    "\n",
    "test_dataloader = DataLoader(db3_sig,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             num_workers=0,\n",
    "                             shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call models, probabilties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbrbm_first = RBM(n_vis=187, n_hid=HIDDEN_UNITS[0], k=K_FOLD, batch=BATCH_SIZE).to(device=device)\n",
    "bbrbm_second = RBM(n_vis=187, n_hid=HIDDEN_UNITS[1], k=K_FOLD, batch=BATCH_SIZE).to(device=device)\n",
    "bbrbm_third = RBM(n_vis=187, n_hid=HIDDEN_UNITS[2], k=K_FOLD, batch=BATCH_SIZE).to(device=device)\n",
    "\n",
    "gbrbm_first = RBM(n_vis=187, n_hid=HIDDEN_UNITS[3], k=K_FOLD, batch=BATCH_SIZE).to(device=device)\n",
    "gbrbm_second = RBM(n_vis=187, n_hid=HIDDEN_UNITS[4], k=K_FOLD, batch=BATCH_SIZE).to(device=device)\n",
    "gbrbm_third = RBM(n_vis=187, n_hid=HIDDEN_UNITS[5], k=K_FOLD, batch=BATCH_SIZE).to(device=device)\n",
    "\n",
    "first_train_op = optim.Adagrad(bbrbm_first.parameters(), LEARNING_RATE)\n",
    "second_train_op = optim.Adagrad(bbrbm_second.parameters(), LEARNING_RATE)\n",
    "third_train_op = optim.Adagrad(bbrbm_third.parameters(), LEARNING_RATE)\n",
    "\n",
    "gb_first_train_op = optim.Adagrad(gbrbm_first.parameters(), LEARNING_RATE)\n",
    "gb_second_train_op = optim.Adagrad(gbrbm_second.parameters(), LEARNING_RATE)\n",
    "gb_third_train_op = optim.Adagrad(gbrbm_third.parameters(), LEARNING_RATE)\n",
    "\n",
    "omse_loss = list()\n",
    "output_gb = list()\n",
    "best_acc = float()\n",
    "svm_best_acc = float()\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "# gaussian_std = torch.arange(1, 0, -0.00537, device=device)\n",
    "gaussian_std = torch.arange(1, 0, -0.0094, device=device)\n",
    "print(gaussian_std.size())\n",
    "\n",
    "svm_model = SVM(lr=LEARNING_RATE, n_x=107)\n",
    "svm_optimizer = optim.Adagrad(svm_model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "Always careful at this part. Some of setting are can be occure system errors like cuda stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''BBRBM Train Part'''\n",
    "\n",
    "loss_ = []\n",
    "output_bb = []\n",
    "model_path_str = str()\n",
    "\n",
    "print(\"RBM START!\")\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    tmp_acc = float()\n",
    "    run_acc = float()\n",
    "    start = time.time()\n",
    "    '''First bbrbm'''\n",
    "    for i, (data) in enumerate(train_dataloader):\n",
    "        if i == 10051:\n",
    "            break\n",
    "\n",
    "        data = Variable(torch.tensor(data.uniform_(0, 1), dtype=torch.float32))\n",
    "        \n",
    "        sample_data = torch.bernoulli(data).view(-1, BATCH_SIZE).to(device=device)\n",
    "        fs_data = sample_data\n",
    "        \n",
    "        # tensor binary\n",
    "        fvog_first, v1 = bbrbm_first(sample_data)\n",
    "        omse_loss = mse_loss(fvog_first, v1)\n",
    "        \n",
    "        first_train_op.zero_grad()\n",
    "        first_train_op.step()\n",
    "        omse_loss.backward()\n",
    "    \n",
    "    for _, (data) in enumerate(v1): \n",
    "        data = Variable(\n",
    "                torch.tensor(data, dtype=torch.float32)\n",
    "        ).uniform_(0, 1)\n",
    "\n",
    "        sample_data = torch.bernoulli(data).to(device=device)\n",
    "\n",
    "        # tensor binary\n",
    "        vog_second, v2 = bbrbm_second(sample_data)\n",
    "        omse_loss = mse_loss(vog_second, v2)\n",
    "\n",
    "        second_train_op.zero_grad()\n",
    "        omse_loss.backward()\n",
    "        second_train_op.step()\n",
    "    \n",
    "    for _, (data) in enumerate(v2):\n",
    "        start = time.time()\n",
    "        data = Variable(\n",
    "                torch.tensor(data, dtype=torch.float32)\n",
    "        ).uniform_(0, 1)\n",
    "\n",
    "        sample_data = torch.bernoulli(data).to(device=device)\n",
    "\n",
    "        vog_third, v3 = bbrbm_third(sample_data)\n",
    "        omse_loss = mse_loss(vog_third, v3)\n",
    "        \n",
    "        third_train_op.zero_grad()\n",
    "        omse_loss.backward()\n",
    "        third_train_op.step()\n",
    "\n",
    "        run_acc += (sample_data == v3).sum().item()\n",
    "    \n",
    "    '''\n",
    "        GBRBM GBRBM GBRBM GBRBM GBRBM GBRBM GBRBM \n",
    "    '''\n",
    "\n",
    "    for i, (data) in enumerate(output_bb):\n",
    "        data = Variable(\n",
    "                torch.tensor(data, dtype=torch.float32)\n",
    "        ).uniform_(0, 1)\n",
    "        \n",
    "        sample_data = torch.normal(mean=data, std=gaussian_std).to(device=device)\n",
    "\n",
    "        # tensor binary\n",
    "        vog_first, v1 = gbrbm_first(sample_data)\n",
    "        omse_loss = mse_loss(vog_first, v1)\n",
    "\n",
    "        gb_first_train_op.zero_grad()\n",
    "        gb_first_train_op.step()\n",
    "        omse_loss.backward()\n",
    "\n",
    "    for _, (data) in enumerate(v1): \n",
    "        data = Variable(\n",
    "                torch.tensor(data, dtype=torch.float32)\n",
    "        ).uniform_(0, 1)\n",
    "\n",
    "        sample_data = torch.normal(mean=data, std=gaussian_std).to(device=device)\n",
    "\n",
    "        # tensor binary\n",
    "        vog_second, v2 = gbrbm_second(sample_data)\n",
    "        omse_loss = mse_loss(vog_second, v2)\n",
    "\n",
    "        gb_second_train_op.zero_grad()\n",
    "        omse_loss.backward()\n",
    "        gb_second_train_op.step()\n",
    "\n",
    "    for _, (data) in enumerate(v2):\n",
    "        start = time.time()\n",
    "        data = Variable(\n",
    "                torch.tensor(data, dtype=torch.float32)\n",
    "        ).uniform_(0, 1)\n",
    "\n",
    "        sample_data = torch.normal(mean=data, std=gaussian_std).to(device=device)\n",
    "\n",
    "        vog_third, v3_e = gbrbm_third(sample_data)\n",
    "        omse_loss = mse_loss(vog_third, v3_e)\n",
    "        \n",
    "        gb_third_train_op.zero_grad()\n",
    "        omse_loss.backward()\n",
    "        gb_third_train_op.step()\n",
    "\n",
    "    svm_X = torch.tensor(v3_e, dtype=torch.float32, device=device)\n",
    "    svm_Y = torch.tensor(Y, dtype=torch.float32, device=device)\n",
    "    N = len(svm_Y)\n",
    "\n",
    "    perm = torch.randperm(N, device=device)\n",
    "\n",
    "    for i in range(0, N, BATCH_SIZE):\n",
    "        correct = float()\n",
    "\n",
    "        x = torch.tensor(svm_X.clone().detach(), device=device)\n",
    "        y = torch.tensor(svm_Y.clone().detach(), device=device)\n",
    "\n",
    "        # Forward\n",
    "        output = svm_model(x)\n",
    "        \n",
    "        # Backward\n",
    "        svm_optimizer.zero_grad()        \n",
    "        svm_optimizer.step()\n",
    "\n",
    "        predicted = torch.tensor(output.data >= 0, dtype=torch.float32)\n",
    "\n",
    "        svm_acc = output.data >= predicted\n",
    "        \n",
    "    svm_best_acc = svm_acc\n",
    "    svm_path = \"./mat_svm_model/\" + str(epoch) + \"_Train_svm_model_acc__.pth\"\n",
    "    torch.save(svm_model.state_dict(), svm_path)\n",
    "\n",
    "    acc_v = (vog_third >= 0).float()\n",
    "    acc = get_acc(\n",
    "        acc_v, v3_e\n",
    "    ) * 100\n",
    "    \n",
    "    if acc > best_acc:\n",
    "        best_acc = acc    \n",
    "        \n",
    "        path = \"./say_cheese/ahh_saveMode_through_\" + str(epoch) + \"_\" + str(acc) + \"GBRBM.pth\"\n",
    "        model_path_str = path\n",
    "        torch.save(gbrbm_third.state_dict(), path)\n",
    "    output_gb.append(v3_e)\n",
    "\n",
    "    print(\"GB-DBN Training loss for {0}th epoch {1}\\tEstimate time : {2}\\tAcc : {3}\\t\\tBest Acc : {4}\\tSVM Acc & Predicted: {5}, {6}\".format(epoch + 1, omse_loss, time.time() - start, acc, best_acc, svm_acc, predicted))\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(output_gb, len(output_gb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_PATH = model_path_str\n",
    "\n",
    "load_model = RBM(n_vis=187, n_hid=120, k=K_FOLD, batch=BATCH_SIZE)\n",
    "load_model.load_state_dict((torch.load(LOAD_PATH)))\n",
    "load_model.to(device=device)\n",
    "\n",
    "for i, (data) in enumerate(test_dataloader):\n",
    "    if i == 939:\n",
    "        continue\n",
    "\n",
    "    data = Variable(\n",
    "            torch.tensor(data, dtype=torch.float32)\n",
    "    ).uniform_(0, 1)\n",
    "    \n",
    "    sample_data = torch.bernoulli(data).view(-1, 107).to(device=device)\n",
    "    \n",
    "    # tensor binary\n",
    "    vog_first, v1 = load_model(sample_data)\n",
    "    omse_loss = mse_loss(vog_first, v1)\n",
    "    \n",
    "    first_train_op.zero_grad()\n",
    "    first_train_op.step()\n",
    "    omse_loss.backward()\n",
    "\n",
    "print(\"Load model: \", LOAD_PATH)\n",
    "print(\"Acc : \", get_acc(vog_first, v1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_LOAD_PATH = \"./mat_svm_model/7_Train_svm_model_acc__.pth\"\n",
    "\n",
    "svm_model_load = SVM(lr=LEARNING_RATE, n_x=107)\n",
    "svm_model_load.load_state_dict(torch.load(SVM_LOAD_PATH))\n",
    "svm_model_load.to(device=device)\n",
    "\n",
    "best_svm_acc = 0.\n",
    "\n",
    "for i in range(0, N, BATCH_SIZE):\n",
    "        correct = float()\n",
    "\n",
    "        x = torch.tensor(svm_X.clone().detach(), device=device)\n",
    "        y = torch.tensor(svm_Y.clone().detach(), device=device)\n",
    "\n",
    "        # Forward\n",
    "        output = svm_model(x)\n",
    "        \n",
    "        # Backward\n",
    "        svm_optimizer.zero_grad()        \n",
    "        svm_optimizer.step()\n",
    "\n",
    "        predicted = torch.tensor(output.data >= 0, dtype=torch.float32)\n",
    "        svm_acc = output.data >= predicted\n",
    "\n",
    "        if svm_acc >= best_svm_acc:\n",
    "                best_svm_acc = svm_acc\n",
    "\n",
    "print(\"SVM Model Predicted: \", predicted, \"Accuracy: \", best_svm_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6c3cabcf2f29820bdd7faae982b59d335e0d215fb5382d93f3312fa3292e9b7f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
