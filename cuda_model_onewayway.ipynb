{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-15 14:46:00.968876 model.py code start\n"
     ]
    }
   ],
   "source": [
    "from ignite.contrib.metrics.regression import *\n",
    "from ignite.contrib.metrics import *\n",
    "from ignite.handlers import *\n",
    "from ignite.metrics import *\n",
    "from ignite.engine import *\n",
    "from ignite.utils import *\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import data.read_samples as rs\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVR\n",
    "import numpy as np\n",
    "import datetime\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "\n",
    "print(datetime.datetime.now(), \"model.py code start\")\n",
    "\n",
    "BATCH_SIZE = 10\n",
    "EPOCH = 30\n",
    "LEARNING_RATE = 0.2\n",
    "ANNEALING_RATE = 0.999\n",
    "VISIBLE_UNITS = [180, 200, 250]\n",
    "HIDDEN_UNITS = [80, 100, 120]\n",
    "K_FOLD = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/_8/q_cwh5hn0s1dxsrzq2d040p80000gn/T/ipykernel_56531/3799810620.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'KMP_DUPLICATE_LIB_OK'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'True'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mget_device_name\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mname\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     \"\"\"\n\u001b[0;32m--> 326\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_device_properties\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mget_device_properties\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0m_CudaDeviceProperties\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mproperties\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m     \"\"\"\n\u001b[0;32m--> 356\u001b[0;31m     \u001b[0m_lazy_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# will define _get_device_properties\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_device_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptional\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \"multiprocessing, you must use the 'spawn' start method\")\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_cuda_getDeviceCount'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             raise AssertionError(\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "print(torch.cuda.get_device_name(device))\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBM(nn.Module): \n",
    "    with torch.cuda.device(0):\n",
    "        def __init__(self, n_vis, n_hid, k, batch):\n",
    "            super(RBM, self).__init__()\n",
    "            self.W      = nn.Parameter(torch.randn(1, batch, device=device) * 1e-2)\n",
    "            self.n_vis  = n_vis\n",
    "            self.n_hid  = n_hid\n",
    "            self.k      = k\n",
    "            self.batch  = batch\n",
    "            self.v_bias = nn.Parameter(torch.zeros(n_vis, device=device))\n",
    "            self.h_bias = nn.Parameter(torch.zeros(n_hid, device=device))\n",
    "        \n",
    "        def sample_from_p(self, p):\n",
    "            return F.relu(\n",
    "                torch.sign(\n",
    "                    p - Variable(torch.randn(p.size(), device=device))\n",
    "                )\n",
    "            ).to(device=device)\n",
    "\n",
    "        ''' ISSUE PART '''\n",
    "        def v_to_h(self, v):\n",
    "            w = (self.W.clone())\n",
    "\n",
    "            p_h = F.sigmoid(\n",
    "                # F.linear(v, w, self.h_bias)\n",
    "                F.linear(v, w)\n",
    "            ).to(device=device)\n",
    "\n",
    "            sample_h = self.sample_from_p(p_h)\n",
    "            return p_h, sample_h\n",
    "\n",
    "        def h_to_v(self, h):\n",
    "            w = self.W.t().clone()\n",
    "\n",
    "            p_v = F.sigmoid(\n",
    "                # F.linear(h, w, self.v_bias)\n",
    "                F.linear(h, w)\n",
    "            ).to(device=device)\n",
    "\n",
    "            sample_v = self.sample_from_p(p_v)\n",
    "            return p_v, sample_v\n",
    "        \n",
    "        def forward(self, v):\n",
    "            pre_h1, h1 = self.v_to_h(v)\n",
    "            h_ = h1\n",
    "\n",
    "            for _ in range(self.k):\n",
    "                pre_v_, v_ = self.h_to_v(h_)\n",
    "                pre_h_, h_ = self.v_to_h(v_)\n",
    "            return v, v_\n",
    "        \n",
    "        def get_weight(self):\n",
    "            return self.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM(nn.Module):\n",
    "    def __init__ (self, epoch, n_feat, n_out, batch=10, lr=0.999, c=0.01):\n",
    "        super(SVM, self).__init__()\n",
    "        self.epoch = epoch\n",
    "        self.n_feat = n_feat\n",
    "        self.n_out = n_out\n",
    "        self.batch = batch\n",
    "        self.lr = lr\n",
    "        self.c = c\n",
    "\n",
    "    def get_accuracy(self, model, data):\n",
    "            loader = torch.utils.data.DataLoader(data, batch_size=self.batch)\n",
    "            correct, total = 0, 0\n",
    "\n",
    "            for xs, ts in loader:\n",
    "                zs = model(xs)\n",
    "                pred = zs.max(1, keepdim=True)[1]\n",
    "                correct += pred.eq(ts.view_as(pred)).sum().item()\n",
    "                total += int(ts.shape[0])\n",
    "                return correct / total        \n",
    "    \n",
    "    def plot(self, xl, yl, xls, yls, title=\"Linear SVM Model Result\"):\n",
    "        plt.title(title)\n",
    "        plt.plot(xl, yl)\n",
    "        plt.xlabel(\"Iterations\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.show()\n",
    "        \n",
    "        plt.title(\"Training Curve (batch_size={}, lr={})\".format(self.batch, self.lr))\n",
    "        plt.plot(xls, yls)\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()\n",
    "\n",
    "    def train(self, x):\n",
    "        iters, loss_ = [], []\n",
    "        iters_sub, train_acc = [], []\n",
    "        \n",
    "        model = nn.Linear(\n",
    "            self.n_feat, self.n_out\n",
    "        ).to(device=device)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss().to(device=device)\n",
    "        optimizer = optim.Adagrad(model.parameters(), lr=self.lr, weight_decay=self.c)\n",
    "        \n",
    "        weight, bias = list(model.parameters())\n",
    "        y = torch.sigmoid(\n",
    "            model(x)\n",
    "        ).to(device=device)\n",
    "\n",
    "        svm_train_dataloader = DataLoader(x,\n",
    "                                          batch_size=self.batch,\n",
    "                                          shuffle=True)\n",
    "        \n",
    "        n = 0\n",
    "        for epoch in range(self.epoch):\n",
    "            for xs, ts in enumerate(svm_train_dataloader):\n",
    "                if len(ts) != self.batch:\n",
    "                    continue\n",
    "                zs = model(xs)\n",
    "\n",
    "                loss = criterion(zs, ts)\n",
    "                loss.backward()\n",
    "                \n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                iters.append(n)\n",
    "                loss_.append(float(loss) / self.batch)\n",
    "                train_acc.append(self.get_accuracy(model, x))\n",
    "                n += 1\n",
    "        \n",
    "        # self.plot(iters, loss_, iters_sub, train_acc)\n",
    "        torch.save(model, \"svm_model.pth\")\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class test_SVM(nn.Module):\n",
    "    def __init__ (self, epoch, n_feat, n_out, batch=10, lr=0.999, c=0.01, lw=2):\n",
    "        super(test_SVM, self).__init__()\n",
    "        self.epoch = epoch\n",
    "        self.n_feat = n_feat\n",
    "        self.n_out = n_out\n",
    "        self.batch = batch\n",
    "        self.lr = lr\n",
    "        self.c = c\n",
    "        self.lw = lw\n",
    "\n",
    "    def plot(self, X, y):\n",
    "        svr_lin = SVR(kernel=\"linear\", C=100, gamma=\"auto\")\n",
    "\n",
    "        _, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 15), sharey=True)\n",
    "        axes[svr_lin].plot(\n",
    "            X,\n",
    "            svr_lin.fit(X, y).predict(X),\n",
    "            lw=self.lw,\n",
    "        )\n",
    "\n",
    "        axes[svr_lin].scatter(\n",
    "            X[svr_lin.support_],\n",
    "            y[svr_lin.support_],\n",
    "            facecolor=\"none\",\n",
    "            s=50,\n",
    "        )\n",
    "\n",
    "        axes[svr_lin].scatter(\n",
    "            X[np.setdiff1d(np.arange(len(X)), svr_lin.support_)],\n",
    "            y[np.setdiff1d(np.arange(len(X)), svr_lin.support_)],\n",
    "            facecolor=\"none\",\n",
    "            edgecolor=\"k\",\n",
    "            s=50,\n",
    "            label=\"other training data\",\n",
    "        )\n",
    "\n",
    "        axes[svr_lin].legend(\n",
    "            loc=\"upper center\",\n",
    "            bbox_to_anchor=(0.5, 1.1),\n",
    "            ncol=1,\n",
    "            fancybox=True,\n",
    "            shadow=True,\n",
    "        )\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_step(engine, batch):\n",
    "    return batch\n",
    "\n",
    "default_model = nn.Sequential(OrderedDict([\n",
    "    ('base', nn.Linear(4, 2)),\n",
    "    ('fc', nn.Linear(2, 1))\n",
    "]))\n",
    "\n",
    "default_evaluator = Engine(eval_step)\n",
    "\n",
    "def get_acc(y_true, y_pred):\n",
    "    metric = Accuracy()\n",
    "    metric.attach(default_evaluator, \"accuracy\")\n",
    "    state = default_evaluator.run([[y_pred, y_true]])\n",
    "    return state.metrics[\"accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MODL] Model main code is starting....\n",
      "[INFO] Read train data, cross-vaildation data and test data from median filtering code\n",
      "[INFO] Read records file from  ./data/db1/\n",
      "[RSLT]\t\t\t Export records ...\n",
      "\t\t ['101', '106', '108', '109', '112', '114', '115', '116', '118', '119', '122', '124', '201', '203', '205', '207', '208', '209', '215', '220', '223', '230']\n",
      "[INFO]./rdsamp commending start\n",
      "[IWIP]\t\trdsamp Converting 101\n",
      "[IWIP]\t\trdsamp Converting 106\n",
      "[IWIP]\t\trdsamp Converting 108\n",
      "[IWIP]\t\trdsamp Converting 109\n",
      "[IWIP]\t\trdsamp Converting 112\n",
      "[IWIP]\t\trdsamp Converting 114\n",
      "[IWIP]\t\trdsamp Converting 115\n",
      "[IWIP]\t\trdsamp Converting 116\n",
      "[IWIP]\t\trdsamp Converting 118\n",
      "[IWIP]\t\trdsamp Converting 119\n",
      "[IWIP]\t\trdsamp Converting 122\n",
      "[IWIP]\t\trdsamp Converting 124\n",
      "[IWIP]\t\trdsamp Converting 201\n",
      "[IWIP]\t\trdsamp Converting 203\n",
      "[IWIP]\t\trdsamp Converting 205\n",
      "[IWIP]\t\trdsamp Converting 207\n",
      "[IWIP]\t\trdsamp Converting 208\n",
      "[IWIP]\t\trdsamp Converting 209\n",
      "[IWIP]\t\trdsamp Converting 215\n",
      "[IWIP]\t\trdsamp Converting 220\n",
      "[IWIP]\t\trdsamp Converting 223\n",
      "[IWIP]\t\trdsamp Converting 230\n",
      "[INFO] Read records file from  ./data/db2/\n",
      "[RSLT]\t\t\t Export records ...\n",
      "\t\t ['100', '103', '105', '111', '113', '117', '121', '123', '200', '202', '210', '212', '213', '214', '219', '221', '222', '228', '231', '232', '233', '234']\n",
      "[INFO]./rdsamp commending start\n",
      "[IWIP]\t\trdsamp Converting 100 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 103 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 105 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 111 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 113 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 117 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 121 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 123 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 200 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 202 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 210 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 212 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 213 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 214 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 219 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 221 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 222 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 228 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 231 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 232 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 233 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 234 ./data/db2/\n",
      "[INFO] Read records file from  ./data/db3/svdb/\n",
      "[RSLT]\t\t\t Export records ...\n",
      "\t\t ['800', '801', '802', '803', '804', '805', '806', '807', '808', '809', '810', '811', '812', '820', '821', '822', '823', '824', '825', '826', '827', '828', '829', '840', '841', '842', '843', '844', '845', '846', '847', '848', '849', '850', '851', '852', '853', '854', '855', '856', '857', '858', '859', '860', '861', '862', '863', '864', '865', '866', '867', '868', '869', '870', '871', '872', '873', '874', '875', '876', '877', '878', '879', '880', '881', '882', '883', '884', '885', '886', '887', '888', '889', '890', '891', '892', '893', '894']\n",
      "[INFO]./rdsamp commending start\n",
      "[IWIP]\t\trdsamp Converting 800 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 801 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 802 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 803 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 804 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 805 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 806 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 807 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 808 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 809 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 810 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 811 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 812 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 820 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 821 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 822 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 823 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 824 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 825 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 826 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 827 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 828 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 829 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 840 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 841 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 842 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 843 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 844 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 845 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 846 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 847 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 848 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 849 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 850 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 851 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 852 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 853 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 854 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 855 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 856 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 857 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 858 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 859 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 860 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 861 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 862 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 863 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 864 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 865 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 866 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 867 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 868 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 869 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 870 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 871 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 872 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 873 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 874 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 875 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 876 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 877 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 878 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 879 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 880 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 881 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 882 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 883 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 884 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 885 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 886 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 887 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 888 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 889 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 890 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 891 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 892 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 893 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 894 ./data/db3/svdb/\n",
      "[INFO] DB1 Filtering...\n",
      "[INFO] DB2 Filtering...\n",
      "[INFO] DB3 Filtering...\n",
      "DB1 butter size : 22, DB1 Anno size : 22\n",
      " DB2 butter size : 22, DB2 Anno size : 22\n",
      " DB3 butter size : 78, DB3 Anno size : 78\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"[MODL] Model main code is starting....\")\n",
    "\n",
    "print(\"[INFO] Read train data, cross-vaildation data and test data from median filtering code\")\n",
    "db1_sig, db1_label, db2_sig, db2_label, db3_sig, db3_label = rs.return_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = []\n",
    "cross_dataset = []\n",
    "test_dataset = []\n",
    "\n",
    "for i in range(len(db1_sig)):\n",
    "    train_dataset.append([db1_sig[i], db1_label[i]])\n",
    "\n",
    "for i in range(len(db2_sig)):\n",
    "    cross_dataset.append([db2_sig[i], db2_label[i]])\n",
    "\n",
    "for i in range(len(db3_sig)):\n",
    "    test_dataset.append([db3_sig[i], db3_label[i]])\n",
    "\n",
    "train_dataloader = DataLoader(db1_sig,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              num_workers=0, \n",
    "                              collate_fn=lambda x: x,\n",
    "                              shuffle=True)\n",
    "\n",
    "cross_dataloader = DataLoader(db2_sig,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              num_workers=0,\n",
    "                              collate_fn=lambda x: x,\n",
    "                              shuffle=True)  \n",
    "                            \n",
    "test_dataloader = DataLoader(db3_sig,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             num_workers=0, \n",
    "                             collate_fn=lambda x: x,\n",
    "                             shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbm_first = RBM(n_vis=VISIBLE_UNITS[0], n_hid=HIDDEN_UNITS[0], k=K_FOLD, batch=BATCH_SIZE).to(device=device)\n",
    "rbm_second = RBM(n_vis=VISIBLE_UNITS[1], n_hid=HIDDEN_UNITS[1], k=K_FOLD, batch=BATCH_SIZE).to(device=device)\n",
    "rbm_third = RBM(n_vis=VISIBLE_UNITS[2], n_hid=HIDDEN_UNITS[2], k=K_FOLD, batch=BATCH_SIZE).to(device=device)\n",
    "\n",
    "first_train_op = optim.Adagrad(rbm_first.parameters(), LEARNING_RATE)\n",
    "second_train_op = optim.Adagrad(rbm_second.parameters(), LEARNING_RATE)\n",
    "third_train_op = optim.Adagrad(rbm_third.parameters(), LEARNING_RATE)\n",
    "\n",
    "gb_first_train_op = optim.Adagrad(rbm_first.parameters(), LEARNING_RATE)\n",
    "gb_second_train_op = optim.Adagrad(rbm_second.parameters(), LEARNING_RATE)\n",
    "gb_third_train_op = optim.Adagrad(rbm_third.parameters(), LEARNING_RATE)\n",
    "\n",
    "omse_loss = list()\n",
    "output_gb = list()\n",
    "best_acc = float()\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "gaussian_std = torch.arange(1, 0, -0.1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BBRBM START!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HILAB_Labtop_02\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First BBRBM Passed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HILAB_~1\\AppData\\Local\\Temp/ipykernel_16580/1082265942.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(data, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "'''BBRBM Train Part'''\n",
    "\n",
    "loss_ = []\n",
    "output_bb = []\n",
    "print(\"BBRBM START!\")\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    tmp_acc = float()\n",
    "    run_acc = float()\n",
    "    start = time.time()\n",
    "    '''First bbrbm'''\n",
    "    for i, (data) in enumerate(train_dataloader):\n",
    "        data = Variable(\n",
    "                torch.tensor(data, dtype=torch.float32)\n",
    "        ).uniform_(0, 1)\n",
    "        \n",
    "        sample_data = torch.bernoulli(data).view(-1, 10).to(device=device)\n",
    "\n",
    "        # tensor binary\n",
    "        vog_first, v1 = rbm_first(sample_data)\n",
    "        omse_loss = mse_loss(vog_first, v1)\n",
    "\n",
    "        first_train_op.zero_grad()\n",
    "        first_train_op.step()\n",
    "        omse_loss.backward()\n",
    "    print(\"First BBRBM Passed\")\n",
    "\n",
    "    for _, (data) in enumerate(v1): \n",
    "        data = Variable(\n",
    "                torch.tensor(data, dtype=torch.float32)\n",
    "        ).uniform_(0, 1)\n",
    "\n",
    "        sample_data = torch.bernoulli(data).view(-1, 10).to(device=device)\n",
    "\n",
    "        # tensor binary\n",
    "        vog_second, v2 = rbm_second(sample_data)\n",
    "        omse_loss = mse_loss(vog_second, v2)\n",
    "\n",
    "        second_train_op.zero_grad()\n",
    "        omse_loss.backward()\n",
    "        second_train_op.step()\n",
    "    print(\"Second BBRBM Passed\")\n",
    "    \n",
    "    for _, (data) in enumerate(v2):\n",
    "        start = time.time()\n",
    "        data = Variable(\n",
    "                torch.tensor(data, dtype=torch.float32)\n",
    "        ).uniform_(0, 1)\n",
    "\n",
    "        sample_data = torch.bernoulli(data).view(-1, 10).to(device=device)\n",
    "\n",
    "        vog_third, v3 = rbm_third(sample_data)\n",
    "        omse_loss = mse_loss(vog_third, v3)\n",
    "        \n",
    "        third_train_op.zero_grad()\n",
    "        omse_loss.backward()\n",
    "        third_train_op.step()\n",
    "\n",
    "        run_acc += (sample_data == v3).sum().item()\n",
    "    \n",
    "    print(\"Third BBRBM Passed\")\n",
    "    acc = (run_acc / v3.size()[1]) * 100\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "\n",
    "    path = \"./saveMode_through_BBRBM.pth\"\n",
    "    torch.save(rbm_second.state_dict(), path)\n",
    "    output_bb.append(v3)\n",
    "\n",
    "    for i, (data) in enumerate(output_bb):\n",
    "        data = Variable(\n",
    "                torch.tensor(data, dtype=torch.float32)\n",
    "        ).uniform_(0, 1)\n",
    "        \n",
    "        sample_data = torch.normal(mean=data, std=gaussian_std).view(-1, 10).to(device=device)\n",
    "\n",
    "        # tensor binary\n",
    "        vog_first, v1 = rbm_first(sample_data)\n",
    "        omse_loss = mse_loss(vog_first, v1)\n",
    "\n",
    "        first_train_op.zero_grad()\n",
    "        first_train_op.step()\n",
    "        omse_loss.backward()\n",
    "    print(\"First GBRBM Passed\")\n",
    "\n",
    "    for _, (data) in enumerate(v1): \n",
    "        data = Variable(\n",
    "                torch.tensor(data, dtype=torch.float32)\n",
    "        ).uniform_(0, 1)\n",
    "\n",
    "        sample_data = torch.normal(mean=data, std=gaussian_std).view(-1, 10).to(device=device)\n",
    "\n",
    "        # tensor binary\n",
    "        vog_second, v2 = rbm_second(sample_data)\n",
    "        omse_loss = mse_loss(vog_second, v2)\n",
    "\n",
    "        second_train_op.zero_grad()\n",
    "        omse_loss.backward()\n",
    "        second_train_op.step()\n",
    "    print(\"Second GBRBM Passed\")\n",
    "\n",
    "\n",
    "    for _, (data) in enumerate(v2):\n",
    "        start = time.time()\n",
    "        data = Variable(\n",
    "                torch.tensor(data, dtype=torch.float32)\n",
    "        ).uniform_(0, 1)\n",
    "\n",
    "        sample_data = torch.bernoulli(data).view(-1, 10).to(device=device)\n",
    "\n",
    "        vog_third, v3 = rbm_third(sample_data)\n",
    "        omse_loss = mse_loss(vog_third, v3)\n",
    "        \n",
    "        third_train_op.zero_grad()\n",
    "        omse_loss.backward()\n",
    "        third_train_op.step()\n",
    "        run_acc += (torch.bernoulli(data).view(-1, 10).to(device=device) == v3).sum().item()\n",
    "    print(\"Third GBRBM Passed\")\n",
    "\n",
    "    acc = (run_acc / v3.size()[1]) * 100\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        \n",
    "    tmp_acc = get_acc(vog_third, v3)\n",
    "\n",
    "    path = \"./saveMode_through_GBRBM.pth\"\n",
    "    torch.save(rbm_third.state_dict(), path)\n",
    "    output_gb.append(v3)\n",
    "\n",
    "    print(\"GB-DBN Training loss for {0}th epoch {1}\\tEstimate time : {2}\\tAcc : {3}\\tBest Acc : {4}\\t\\tIgnite Acc: {5}\" \\\n",
    "        .format(epoch + 1, omse_loss, time.time() - start, acc, best_acc, tmp_acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Last Accuracy : \", acc, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVM(EPOCH, 10, 5, batch=BATCH_SIZE, lr=LEARNING_RATE)\n",
    "for i in range(len(output_gb)):\n",
    "    svm.train(\n",
    "        output_gb[i]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_acc = float()\n",
    "best_acc = float()\n",
    "for i, (data) in enumerate(test_dataloader):\n",
    "        data = Variable(\n",
    "                torch.tensor(data, dtype=torch.float32)\n",
    "        ).uniform_(0, 1)\n",
    "        \n",
    "        sample_data = torch.bernoulli(data).view(-1, 10).to(device=device)\n",
    "\n",
    "        # tensor binary\n",
    "        vog_first, v1 = rbm_first(sample_data)\n",
    "        omse_loss = mse_loss(vog_first, v1)\n",
    "\n",
    "        first_train_op.zero_grad()\n",
    "        first_train_op.step()\n",
    "        omse_loss.backward()\n",
    "        run_acc += (sample_data == v1).sum().item()\n",
    "acc = (run_acc / v1.size()[0])\n",
    "if acc > best_acc:\n",
    "        best_acc = acc\n",
    "\n",
    "print(\"loss : {0}\\tEstimate time : {1}\\tAcc : {2}\\tBest Acc : {3}\" \\\n",
    "    .format(omse_loss, time.time() - start, acc, best_acc))\n",
    "\n",
    "'''GUIDE'''\n",
    "run_acc = float()\n",
    "for i, (data) in enumerate(v1):\n",
    "        data = Variable(\n",
    "                torch.tensor(data, dtype=torch.float32)\n",
    "        ).uniform_(0, 1)\n",
    "        \n",
    "        sample_data = torch.bernoulli(data).view(-1, 10).to(device=device)\n",
    "\n",
    "        # tensor binary\n",
    "        vog_second, v2 = rbm_first(sample_data)\n",
    "        omse_loss = mse_loss(vog_second, v2)\n",
    "\n",
    "        second_train_op.zero_grad()\n",
    "        second_train_op.step()\n",
    "        omse_loss.backward()\n",
    "        run_acc += (sample_data == v2).sum().item()\n",
    "print(acc)\n",
    "print(v2.size()[0])\n",
    "acc = (run_acc / v2.size()[0])\n",
    "if acc > best_acc:\n",
    "        best_acc = acc\n",
    "\n",
    "print(\"loss : {0}\\tEstimate time : {1}\\tAcc : {2}\\tBest Acc : {3}\" \\\n",
    "    .format(omse_loss, time.time() - start, acc, best_acc))\n",
    "    \n",
    "'''GUIDE'''\n",
    "run_acc = float()\n",
    "for i, (data) in enumerate(v2):\n",
    "        data = Variable(\n",
    "                torch.tensor(data, dtype=torch.float32)\n",
    "        ).uniform_(0, 1)\n",
    "        \n",
    "        sample_data = torch.bernoulli(data).view(-1, 10).to(device=device)\n",
    "\n",
    "        # tensor binary\n",
    "        vog_second, v3 = rbm_first(sample_data)\n",
    "        omse_loss = mse_loss(vog_second, v3)\n",
    "\n",
    "        second_train_op.zero_grad()\n",
    "        second_train_op.step()\n",
    "        omse_loss.backward()\n",
    "        run_acc += (sample_data == v3).sum().item()\n",
    " \n",
    "acc = (run_acc / v3.size()[0])\n",
    "if acc > best_acc:\n",
    "        best_acc = acc\n",
    "\n",
    "print(\"loss : {0}\\tEstimate time : {1}\\tAcc : {2}\\tBest Acc : {3}\" \\\n",
    "    .format(omse_loss, time.time() - start, acc, best_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_acc = float()\n",
    "for _, (data) in enumerate(v3): \n",
    "        data = Variable(\n",
    "                torch.tensor(data, dtype=torch.float32)\n",
    "        ).uniform_(0, 1)\n",
    "\n",
    "        sample_data = torch.normal(mean=data, std=gaussian_std).view(-1, 10).to(device=device)\n",
    "\n",
    "        # tensor binary\n",
    "        vog_second, v1 = rbm_first(sample_data)\n",
    "        omse_loss = mse_loss(vog_second, v1)\n",
    "\n",
    "        second_train_op.zero_grad()\n",
    "        omse_loss.backward()\n",
    "        second_train_op.step()\n",
    "        run_acc += (torch.bernoulli(data).view(-1, 10).to(device=device) == v1).sum().item()\n",
    "\n",
    "print(\"loss : {0}\\tEstimate time : {1}\\tAcc : {2}\\tBest Acc : {3}\" \\\n",
    "    .format(omse_loss, time.time() - start, acc, best_acc))\n",
    "\n",
    "run_acc = float()\n",
    "for _, (data) in enumerate(v1): \n",
    "        data = Variable(\n",
    "                torch.tensor(data, dtype=torch.float32)\n",
    "        ).uniform_(0, 1)\n",
    "\n",
    "        sample_data = torch.normal(mean=data, std=gaussian_std).view(-1, 10).to(device=device)\n",
    "\n",
    "        # tensor binary\n",
    "        vog_second, v2 = rbm_second(sample_data)\n",
    "        omse_loss = mse_loss(vog_second, v2)\n",
    "\n",
    "        second_train_op.zero_grad()\n",
    "        omse_loss.backward()\n",
    "        second_train_op.step()\n",
    "        run_acc += (torch.bernoulli(data).view(-1, 10).to(device=device) == v2).sum().item()\n",
    "\n",
    "print(\"loss : {0}\\tEstimate time : {1}\\tAcc : {2}\\tBest Acc : {3}\" \\\n",
    "    .format(omse_loss, time.time() - start, acc, best_acc))\n",
    "\n",
    "'''GUIDE'''\n",
    "run_acc = float()\n",
    "for _, (data) in enumerate(v2): \n",
    "        data = Variable(\n",
    "                torch.tensor(data, dtype=torch.float32)\n",
    "        ).uniform_(0, 1)\n",
    "\n",
    "        sample_data = torch.normal(mean=data, std=gaussian_std).view(-1, 10).to(device=device)\n",
    "\n",
    "        # tensor binary\n",
    "        vog_second, v3 = rbm_third(sample_data)\n",
    "        omse_loss = mse_loss(vog_second, v3)\n",
    "\n",
    "        second_train_op.zero_grad()\n",
    "        omse_loss.backward()\n",
    "        second_train_op.step()\n",
    "        run_acc += (torch.bernoulli(data).view(-1, 10).to(device=device) == v1).sum().item()\n",
    "\n",
    "print(\"loss : {0}\\tEstimate time : {1}\\tAcc : {2}\\tBest Acc : {3}\" \\\n",
    "    .format(omse_loss, time.time() - start, acc, best_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model_cp = torch.load(\"svm_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = SVM(epoch=EPOCH, n_feat=v3.size()[0], n_out=5, batch=BATCH_SIZE, lr=LEARNING_RATE)\n",
    "svm_optim = optim.Adagrad(svm_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "svm_model.load_state_dict(svm_model_cp['model_state_dict'])\n",
    "svm_optim.load_state_dict(svm_model_cp['optimizer_state_dict'])\n",
    "epoch = svm_model_cp['epoch']\n",
    "loss = svm_model_cp['loss']\n",
    "\n",
    "svm_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "155d6f2e0f64686ab4bfd14ea62d28fe51bc71031495ea0caf798feb858e6597"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
