{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Pre-processing for make clean\n",
      "2022-05-18 20:29:29.604120 model.py code start\n"
     ]
    }
   ],
   "source": [
    "from ignite.contrib.metrics.regression import *\n",
    "from ignite.contrib.metrics import *\n",
    "from ignite.handlers import *\n",
    "from ignite.metrics import *\n",
    "from ignite.engine import *\n",
    "from ignite.utils import *\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import data.read_samples as rs\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "\n",
    "from collections import OrderedDict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "\n",
    "print(datetime.datetime.now(), \"model.py code start\")\n",
    "\n",
    "TRAIN_CSV_PATH = \"./data/mitbih_train.csv\"\n",
    "TEST_CSV_PATH = \"./data/mitbih_test.csv\"\n",
    "\n",
    "BATCH_SIZE = 54\n",
    "EPOCH = 100\n",
    "LEARNING_RATE = 0.2\n",
    "ANNEALING_RATE = 0.999\n",
    "VISIBLE_UNITS = [180, 200, 250]\n",
    "HIDDEN_UNITS = [80, 100, 120]\n",
    "K_FOLD = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 3080 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "print(torch.cuda.get_device_name(device))\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBM(nn.Module): \n",
    "    with torch.cuda.device(0):\n",
    "        def __init__(self, n_vis, n_hid, k, batch):\n",
    "            super(RBM, self).__init__()\n",
    "            self.W      = nn.Parameter(torch.randn(1, batch, device=device) * 1e-2)\n",
    "            self.n_vis  = n_vis\n",
    "            self.n_hid  = n_hid\n",
    "            self.k      = k\n",
    "            self.batch  = batch\n",
    "            self.v_bias = nn.Parameter(torch.zeros(n_vis, device=device))\n",
    "            self.h_bias = nn.Parameter(torch.zeros(n_hid, device=device))\n",
    "        \n",
    "        def sample_from_p(self, p):\n",
    "            return F.relu(\n",
    "                torch.sign(\n",
    "                    p - Variable(torch.randn(p.size(), device=device))\n",
    "                )\n",
    "            ).to(device=device)\n",
    "\n",
    "        ''' ISSUE PART '''\n",
    "        def v_to_h(self, v):\n",
    "            w = (self.W.clone())\n",
    "\n",
    "            p_h = F.sigmoid(\n",
    "                F.linear(v, w)\n",
    "            ).to(device=device)\n",
    "\n",
    "            sample_h = self.sample_from_p(p_h)\n",
    "            return p_h, sample_h\n",
    "\n",
    "        def h_to_v(self, h):\n",
    "            w = self.W.t().clone()\n",
    "\n",
    "            p_v = F.sigmoid(\n",
    "                F.linear(h, w)\n",
    "            ).to(device=device)\n",
    "\n",
    "            sample_v = self.sample_from_p(p_v)\n",
    "            return p_v, sample_v\n",
    "        \n",
    "        def forward(self, v):\n",
    "            pre_h1, h1 = self.v_to_h(v)\n",
    "            h_ = h1\n",
    "\n",
    "            for _ in range(self.k):\n",
    "                pre_v_, v_ = self.h_to_v(h_)\n",
    "                pre_h_, h_ = self.v_to_h(v_)\n",
    "            return v, v_\n",
    "        \n",
    "        def get_weight(self):\n",
    "            return self.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM(nn.Module):\n",
    "    def __init__(self, lr, n_x):\n",
    "        super(SVM, self).__init__()\n",
    "        self.lr = lr\n",
    "        self.fully = nn.Linear(n_x, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        fwd = self.fully(x)\n",
    "        return fwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_step(engine, batch):\n",
    "    return batch\n",
    "\n",
    "default_model = nn.Sequential(OrderedDict([\n",
    "    ('base', nn.Linear(4, 2)),\n",
    "    ('fc', nn.Linear(2, 1))\n",
    "]))\n",
    "\n",
    "default_evaluator = Engine(eval_step)\n",
    "\n",
    "def get_acc(y_true, y_pred):\n",
    "    metric = Accuracy()\n",
    "    metric.attach(default_evaluator, \"accuracy\")\n",
    "    state = default_evaluator.run([[y_pred, y_true]])\n",
    "    return state.metrics[\"accuracy\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MODL] Model main code is starting....\n",
      "[INFO] Read train data, cross-vaildation data and test data from median filtering code\n",
      "87554 87554 21892 21892\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(\"[MODL] Model main code is starting....\")\n",
    "print(\"[INFO] Read train data, cross-vaildation data and test data from median filtering code\")\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_CSV_PATH, header=None).sample(frac=1)\n",
    "test_df = pd.read_csv(TEST_CSV_PATH, header=None)\n",
    "\n",
    "Y = np.array(train_df[187].values).astype(np.int8)\n",
    "X = np.array(train_df[list(range(187))].values)[..., np.newaxis]\n",
    "\n",
    "Y_test = np.array(test_df[187].values).astype(np.int8)\n",
    "X_test = np.array(test_df[list(range(187))].values)[..., np.newaxis]\n",
    "\n",
    "print(len(X), len(Y), len(X_test), len(Y_test))\n",
    "print(type(X), type(Y), type(X_test), type(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = []\n",
    "cross_dataset = []\n",
    "test_dataset = []\n",
    "\n",
    "train_dataloader = DataLoader(X,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              num_workers=0,\n",
    "                              shuffle=True)\n",
    "\n",
    "test_dataloader = DataLoader(Y,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             num_workers=0,\n",
    "                             shuffle=True)\n",
    "                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([187])\n"
     ]
    }
   ],
   "source": [
    "rbm_first = RBM(n_vis=VISIBLE_UNITS[0], n_hid=HIDDEN_UNITS[0], k=K_FOLD, batch=BATCH_SIZE).to(device=device)\n",
    "rbm_second = RBM(n_vis=VISIBLE_UNITS[1], n_hid=HIDDEN_UNITS[1], k=K_FOLD, batch=BATCH_SIZE).to(device=device)\n",
    "rbm_third = RBM(n_vis=VISIBLE_UNITS[2], n_hid=HIDDEN_UNITS[2], k=K_FOLD, batch=BATCH_SIZE).to(device=device)\n",
    "\n",
    "first_train_op = optim.Adagrad(rbm_first.parameters(), LEARNING_RATE)\n",
    "second_train_op = optim.Adagrad(rbm_second.parameters(), LEARNING_RATE)\n",
    "third_train_op = optim.Adagrad(rbm_third.parameters(), LEARNING_RATE)\n",
    "\n",
    "gb_first_train_op = optim.Adagrad(rbm_first.parameters(), LEARNING_RATE)\n",
    "gb_second_train_op = optim.Adagrad(rbm_second.parameters(), LEARNING_RATE)\n",
    "gb_third_train_op = optim.Adagrad(rbm_third.parameters(), LEARNING_RATE)\n",
    "\n",
    "omse_loss = list()\n",
    "output_gb = list()\n",
    "best_acc = float()\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "gaussian_std = torch.arange(1, 0, -0.00537, device=device)\n",
    "print(gaussian_std.size())\n",
    "\n",
    "svm_X = torch.FloatTensor(output_gb)\n",
    "svm_Y = torch.FloatTensor(Y)\n",
    "N = len(svm_Y)\n",
    "\n",
    "svm_model = SVM(lr=LEARNING_RATE, n_x=2577)\n",
    "svm_optimizer = optim.Adagrad(svm_model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load_model = RBM(VISIBLE_UNITS[2], VISIBLE_UNITS[2], K_FOLD, BATCH_SIZE)\n",
    "# # load_model.load_state_dict(torch.load(\"./saveMode_through_99_epoch_90.0_acc_GBDBN.pth\"))\n",
    "\n",
    "# LOAD_PATH = \"./saveMode_through_99_epoch_90.0_acc_GBDBN.pth\"\n",
    "\n",
    "# load_model = RBM(VISIBLE_UNITS[2], HIDDEN_UNITS[2], K_FOLD, 10)\n",
    "# load_model.load_state_dict((torch.load(LOAD_PATH)))\n",
    "# load_model.to(device=device)\n",
    "\n",
    "# for i, (data) in enumerate(test_dataloader):\n",
    "#         if(data.size()[0] == 4):\n",
    "#             break\n",
    "#         data = Variable(\n",
    "#                 torch.tensor(data, dtype=torch.float32)\n",
    "#         ).uniform_(0, 1)\n",
    "        \n",
    "#         sample_data = torch.bernoulli(data).view(-1, 10).to(device=device)\n",
    "\n",
    "#         # tensor binary\n",
    "#         vog_first, v1 = load_model(sample_data)\n",
    "#         omse_loss = mse_loss(vog_first, v1)\n",
    "\n",
    "#         first_train_op.zero_grad()\n",
    "#         first_train_op.step()\n",
    "#         omse_loss.backward()\n",
    "# get_acc(vog_first, v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# GB-DBN Train Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RBM START!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HILAB_~1\\AppData\\Local\\Temp/ipykernel_12512/1254344248.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data = Variable(torch.tensor(data, dtype=torch.float32))\n",
      "C:\\Users\\HILAB_~1\\AppData\\Local\\Temp/ipykernel_12512/1254344248.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(data, dtype=torch.float32)\n",
      "C:\\Users\\HILAB_~1\\AppData\\Local\\Temp/ipykernel_12512/1254344248.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(data, dtype=torch.float32)\n",
      "C:\\Users\\HILAB_~1\\AppData\\Local\\Temp/ipykernel_12512/1254344248.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(data, dtype=torch.float32)\n",
      "C:\\Users\\HILAB_~1\\AppData\\Local\\Temp/ipykernel_12512/1254344248.py:84: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(data, dtype=torch.float32)\n",
      "C:\\Users\\HILAB_~1\\AppData\\Local\\Temp/ipykernel_12512/1254344248.py:100: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(data, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GB-DBN Training loss for 1th epoch 0.677359938621521\tEstimate time : 0.23400115966796875\tAcc : 70.58823529411765\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 2th epoch 0.6911712884902954\tEstimate time : 0.2460002899169922\tAcc : 63.63636363636363\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 3th epoch 0.6992408037185669\tEstimate time : 0.2539999485015869\tAcc : 57.21925133689839\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 4th epoch 0.6538617014884949\tEstimate time : 0.25699853897094727\tAcc : 64.70588235294117\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 5th epoch 0.6333507299423218\tEstimate time : 0.2609977722167969\tAcc : 66.31016042780749\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 6th epoch 0.5944482088088989\tEstimate time : 0.252000093460083\tAcc : 65.77540106951871\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 7th epoch 0.6320518851280212\tEstimate time : 0.28200221061706543\tAcc : 66.31016042780749\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 8th epoch 0.675806999206543\tEstimate time : 0.2649974822998047\tAcc : 58.82352941176471\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 9th epoch 0.6128644347190857\tEstimate time : 0.26199984550476074\tAcc : 67.37967914438502\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 10th epoch 0.6770418882369995\tEstimate time : 0.26199984550476074\tAcc : 60.962566844919785\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 11th epoch 0.6857746243476868\tEstimate time : 0.2690000534057617\tAcc : 63.101604278074866\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 12th epoch 0.7120356559753418\tEstimate time : 0.28600049018859863\tAcc : 65.77540106951871\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 13th epoch 0.7946670055389404\tEstimate time : 0.27499985694885254\tAcc : 61.49732620320856\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 14th epoch 0.5782495141029358\tEstimate time : 0.27199769020080566\tAcc : 66.84491978609626\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 15th epoch 0.5801125168800354\tEstimate time : 0.2793717384338379\tAcc : 65.24064171122996\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 16th epoch 0.6856778264045715\tEstimate time : 0.28600001335144043\tAcc : 62.03208556149733\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 17th epoch 0.6264134645462036\tEstimate time : 0.29401087760925293\tAcc : 63.63636363636363\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 18th epoch 0.6820517182350159\tEstimate time : 0.29000210762023926\tAcc : 59.893048128342244\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 19th epoch 0.6616053581237793\tEstimate time : 0.3130185604095459\tAcc : 58.288770053475936\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 20th epoch 0.6907743811607361\tEstimate time : 0.2990000247955322\tAcc : 63.101604278074866\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 21th epoch 0.6545036435127258\tEstimate time : 0.3060016632080078\tAcc : 60.962566844919785\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 22th epoch 0.6047577857971191\tEstimate time : 0.29400014877319336\tAcc : 64.1711229946524\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 23th epoch 0.7412383556365967\tEstimate time : 0.3100006580352783\tAcc : 66.31016042780749\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 24th epoch 0.607904314994812\tEstimate time : 0.3120002746582031\tAcc : 62.03208556149733\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 25th epoch 0.5210989117622375\tEstimate time : 0.3000175952911377\tAcc : 63.101604278074866\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 26th epoch 0.699234127998352\tEstimate time : 0.3040013313293457\tAcc : 59.893048128342244\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 27th epoch 0.8114399909973145\tEstimate time : 0.3220374584197998\tAcc : 64.70588235294117\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 28th epoch 0.6247847080230713\tEstimate time : 0.32000017166137695\tAcc : 57.75401069518716\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 29th epoch 0.6028103828430176\tEstimate time : 0.3078899383544922\tAcc : 63.101604278074866\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 30th epoch 0.5520808100700378\tEstimate time : 0.30299925804138184\tAcc : 66.84491978609626\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 31th epoch 0.7337011694908142\tEstimate time : 0.3175058364868164\tAcc : 57.75401069518716\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 32th epoch 0.6830387115478516\tEstimate time : 0.34200000762939453\tAcc : 60.962566844919785\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 33th epoch 0.6091709136962891\tEstimate time : 0.3170003890991211\tAcc : 60.962566844919785\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 34th epoch 0.6254671216011047\tEstimate time : 0.31445908546447754\tAcc : 62.56684491978609\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 35th epoch 0.7013593912124634\tEstimate time : 0.31800079345703125\tAcc : 63.63636363636363\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 36th epoch 0.6764826774597168\tEstimate time : 0.30900025367736816\tAcc : 67.9144385026738\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 37th epoch 0.6827898025512695\tEstimate time : 0.31603193283081055\tAcc : 62.03208556149733\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 38th epoch 0.6110847592353821\tEstimate time : 0.3009965419769287\tAcc : 63.63636363636363\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 39th epoch 0.7109460234642029\tEstimate time : 0.33899593353271484\tAcc : 63.101604278074866\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 40th epoch 0.6810619235038757\tEstimate time : 0.341524600982666\tAcc : 56.68449197860963\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 41th epoch 0.6042459607124329\tEstimate time : 0.3470022678375244\tAcc : 65.24064171122996\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 42th epoch 0.7731978893280029\tEstimate time : 0.32962512969970703\tAcc : 57.75401069518716\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 43th epoch 0.7209627032279968\tEstimate time : 0.36894917488098145\tAcc : 62.56684491978609\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 44th epoch 0.7444964051246643\tEstimate time : 0.35399889945983887\tAcc : 58.82352941176471\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 45th epoch 0.6819854974746704\tEstimate time : 0.3649406433105469\tAcc : 61.49732620320856\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 46th epoch 0.7118610143661499\tEstimate time : 0.36550474166870117\tAcc : 62.56684491978609\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 47th epoch 0.5497515797615051\tEstimate time : 0.367999792098999\tAcc : 64.1711229946524\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 48th epoch 0.6478100419044495\tEstimate time : 0.3570241928100586\tAcc : 63.101604278074866\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 49th epoch 0.6499280333518982\tEstimate time : 0.38867616653442383\tAcc : 64.1711229946524\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 50th epoch 0.5925825834274292\tEstimate time : 0.36188602447509766\tAcc : 68.98395721925134\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 51th epoch 0.6368234753608704\tEstimate time : 0.3319997787475586\tAcc : 64.70588235294117\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 52th epoch 0.6584416627883911\tEstimate time : 0.33699846267700195\tAcc : 53.475935828877006\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 53th epoch 0.6719185709953308\tEstimate time : 0.353013277053833\tAcc : 63.63636363636363\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 54th epoch 0.5919371843338013\tEstimate time : 0.3690965175628662\tAcc : 62.56684491978609\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 55th epoch 0.5170462727546692\tEstimate time : 0.35851311683654785\tAcc : 63.101604278074866\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 56th epoch 0.8014999628067017\tEstimate time : 0.37200236320495605\tAcc : 57.75401069518716\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 57th epoch 0.710626482963562\tEstimate time : 0.3910031318664551\tAcc : 60.42780748663101\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 58th epoch 0.7630202770233154\tEstimate time : 0.3809976577758789\tAcc : 65.24064171122996\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 59th epoch 0.7342175245285034\tEstimate time : 0.39201807975769043\tAcc : 61.49732620320856\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 60th epoch 0.650428831577301\tEstimate time : 0.3832395076751709\tAcc : 63.101604278074866\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 61th epoch 0.7452217936515808\tEstimate time : 0.36899781227111816\tAcc : 58.288770053475936\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 62th epoch 0.7914378046989441\tEstimate time : 0.3655571937561035\tAcc : 58.288770053475936\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 63th epoch 0.6250072717666626\tEstimate time : 0.38499975204467773\tAcc : 66.84491978609626\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 64th epoch 0.6532103419303894\tEstimate time : 0.4050014019012451\tAcc : 63.63636363636363\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 65th epoch 0.6736140251159668\tEstimate time : 0.388012170791626\tAcc : 68.44919786096256\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 66th epoch 0.562558650970459\tEstimate time : 0.3970003128051758\tAcc : 63.63636363636363\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 67th epoch 0.6500371694564819\tEstimate time : 0.36603307723999023\tAcc : 64.70588235294117\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 68th epoch 0.6286713480949402\tEstimate time : 0.40601563453674316\tAcc : 67.9144385026738\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 69th epoch 0.7173248529434204\tEstimate time : 0.4070000648498535\tAcc : 64.1711229946524\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 70th epoch 0.6565600037574768\tEstimate time : 0.40800023078918457\tAcc : 61.49732620320856\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 71th epoch 0.6024240851402283\tEstimate time : 0.3940002918243408\tAcc : 64.70588235294117\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 72th epoch 0.5289198160171509\tEstimate time : 0.3925209045410156\tAcc : 66.84491978609626\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 73th epoch 0.5777702927589417\tEstimate time : 0.41106557846069336\tAcc : 66.31016042780749\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 74th epoch 0.5847278833389282\tEstimate time : 0.3815329074859619\tAcc : 63.101604278074866\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 75th epoch 0.8149803280830383\tEstimate time : 0.40400052070617676\tAcc : 64.1711229946524\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 76th epoch 0.7954187989234924\tEstimate time : 0.4140002727508545\tAcc : 56.68449197860963\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 77th epoch 0.6897867321968079\tEstimate time : 0.4220001697540283\tAcc : 63.101604278074866\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 78th epoch 0.720404326915741\tEstimate time : 0.4180004596710205\tAcc : 58.82352941176471\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 79th epoch 0.5698291063308716\tEstimate time : 0.40200185775756836\tAcc : 67.37967914438502\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 80th epoch 0.6083706617355347\tEstimate time : 0.4385831356048584\tAcc : 54.01069518716578\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 81th epoch 0.5517361164093018\tEstimate time : 0.44400620460510254\tAcc : 58.288770053475936\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 82th epoch 0.5789347290992737\tEstimate time : 0.4250001907348633\tAcc : 65.77540106951871\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 83th epoch 0.5792461037635803\tEstimate time : 0.4330003261566162\tAcc : 62.56684491978609\t\tBest Acc : 70.58823529411765\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 84th epoch 0.6156604290008545\tEstimate time : 0.4483335018157959\tAcc : 71.12299465240642\t\tBest Acc : 71.12299465240642\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 85th epoch 0.7404295802116394\tEstimate time : 0.42800045013427734\tAcc : 58.82352941176471\t\tBest Acc : 71.12299465240642\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 86th epoch 0.7699205875396729\tEstimate time : 0.45799827575683594\tAcc : 58.82352941176471\t\tBest Acc : 71.12299465240642\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 87th epoch 0.7028036713600159\tEstimate time : 0.40200233459472656\tAcc : 64.1711229946524\t\tBest Acc : 71.12299465240642\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 88th epoch 0.6961815357208252\tEstimate time : 0.4220008850097656\tAcc : 63.63636363636363\t\tBest Acc : 71.12299465240642\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 89th epoch 0.6833855509757996\tEstimate time : 0.4210057258605957\tAcc : 64.1711229946524\t\tBest Acc : 71.12299465240642\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 90th epoch 0.587159276008606\tEstimate time : 0.41500020027160645\tAcc : 59.893048128342244\t\tBest Acc : 71.12299465240642\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 91th epoch 0.6560223698616028\tEstimate time : 0.45600223541259766\tAcc : 65.77540106951871\t\tBest Acc : 71.12299465240642\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 92th epoch 0.7459095120429993\tEstimate time : 0.45363330841064453\tAcc : 54.54545454545454\t\tBest Acc : 71.12299465240642\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 93th epoch 0.7153207063674927\tEstimate time : 0.43201184272766113\tAcc : 56.68449197860963\t\tBest Acc : 71.12299465240642\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 94th epoch 0.6731049418449402\tEstimate time : 0.4380002021789551\tAcc : 58.82352941176471\t\tBest Acc : 71.12299465240642\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 95th epoch 0.7346949577331543\tEstimate time : 0.4671306610107422\tAcc : 56.14973262032086\t\tBest Acc : 71.12299465240642\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 96th epoch 0.7451836466789246\tEstimate time : 0.4350006580352783\tAcc : 62.56684491978609\t\tBest Acc : 71.12299465240642\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 97th epoch 0.6216503381729126\tEstimate time : 0.45200037956237793\tAcc : 61.49732620320856\t\tBest Acc : 71.12299465240642\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 98th epoch 0.688160240650177\tEstimate time : 0.42960476875305176\tAcc : 65.77540106951871\t\tBest Acc : 71.12299465240642\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 99th epoch 0.5826318860054016\tEstimate time : 0.46283769607543945\tAcc : 63.101604278074866\t\tBest Acc : 71.12299465240642\t\tIgnite Acc: 0.0\n",
      "GB-DBN Training loss for 100th epoch 0.5940771698951721\tEstimate time : 0.4479997158050537\tAcc : 62.03208556149733\t\tBest Acc : 71.12299465240642\t\tIgnite Acc: 0.0\n"
     ]
    }
   ],
   "source": [
    "'''BBRBM Train Part'''\n",
    "\n",
    "loss_ = []\n",
    "output_bb = []\n",
    "print(\"RBM START!\")\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    tmp_acc = float()\n",
    "    run_acc = float()\n",
    "    start = time.time()\n",
    "    '''First bbrbm'''\n",
    "    for i, (data) in enumerate(train_dataloader):\n",
    "        data = Variable(torch.tensor(data, dtype=torch.float32))\n",
    "        if(data.size()[0] == 4):\n",
    "            break\n",
    "        \n",
    "        sample_data = torch.bernoulli(data).view(-1, BATCH_SIZE).to(device=device)\n",
    "        fs_data = sample_data\n",
    "        # tensor binary\n",
    "        fvog_first, v1 = rbm_first(sample_data)\n",
    "        omse_loss = mse_loss(fvog_first, v1)\n",
    "        \n",
    "        first_train_op.zero_grad()\n",
    "        first_train_op.step()\n",
    "        omse_loss.backward()\n",
    "    \n",
    "    for _, (data) in enumerate(v1): \n",
    "        data = Variable(\n",
    "                torch.tensor(data, dtype=torch.float32)\n",
    "        ).uniform_(0, 1)\n",
    "\n",
    "        sample_data = torch.bernoulli(data).view(-1, BATCH_SIZE).to(device=device)\n",
    "\n",
    "        # tensor binary\n",
    "        vog_second, v2 = rbm_second(sample_data)\n",
    "        omse_loss = mse_loss(vog_second, v2)\n",
    "\n",
    "        second_train_op.zero_grad()\n",
    "        omse_loss.backward()\n",
    "        second_train_op.step()\n",
    "    \n",
    "    for _, (data) in enumerate(v2):\n",
    "        start = time.time()\n",
    "        data = Variable(\n",
    "                torch.tensor(data, dtype=torch.float32)\n",
    "        ).uniform_(0, 1)\n",
    "\n",
    "        sample_data = torch.bernoulli(data).view(-1, BATCH_SIZE).to(device=device)\n",
    "\n",
    "        vog_third, v3 = rbm_third(sample_data)\n",
    "        omse_loss = mse_loss(vog_third, v3)\n",
    "        \n",
    "        third_train_op.zero_grad()\n",
    "        omse_loss.backward()\n",
    "        third_train_op.step()\n",
    "\n",
    "        run_acc += (sample_data == v3).sum().item()\n",
    "    \n",
    "    path = \"./saveMode_through_BBRBM.pth\"\n",
    "    torch.save(rbm_second.state_dict(), path)\n",
    "    output_bb.append(v3)\n",
    "\n",
    "    '''\n",
    "GBRBM GBRBM GBRBM GBRBM GBRBM GBRBM GBRBM \n",
    "    '''\n",
    "\n",
    "    for i, (data) in enumerate(output_bb):\n",
    "        data = Variable(\n",
    "                torch.tensor(data, dtype=torch.float32)\n",
    "        ).uniform_(0, 1)\n",
    "        \n",
    "        sample_data = torch.normal(mean=data, std=gaussian_std).view(-1, BATCH_SIZE).to(device=device)\n",
    "\n",
    "        # tensor binary\n",
    "        vog_first, v1 = rbm_first(sample_data)\n",
    "        omse_loss = mse_loss(vog_first, v1)\n",
    "\n",
    "        first_train_op.zero_grad()\n",
    "        first_train_op.step()\n",
    "        omse_loss.backward()\n",
    "\n",
    "    for _, (data) in enumerate(v1): \n",
    "        data = Variable(\n",
    "                torch.tensor(data, dtype=torch.float32)\n",
    "        ).uniform_(0, 1)\n",
    "\n",
    "        sample_data = torch.normal(mean=data, std=gaussian_std).view(-1, BATCH_SIZE).to(device=device)\n",
    "\n",
    "        # tensor binary\n",
    "        vog_second, v2 = rbm_second(sample_data)\n",
    "        omse_loss = mse_loss(vog_second, v2)\n",
    "\n",
    "        second_train_op.zero_grad()\n",
    "        omse_loss.backward()\n",
    "        second_train_op.step()\n",
    "\n",
    "    for _, (data) in enumerate(v2):\n",
    "        start = time.time()\n",
    "        data = Variable(\n",
    "                torch.tensor(data, dtype=torch.float32)\n",
    "        ).uniform_(0, 1)\n",
    "\n",
    "        sample_data = torch.normal(mean=data, std=gaussian_std).view(-1, BATCH_SIZE).to(device=device)\n",
    "\n",
    "        vog_third, v3_e = rbm_third(sample_data)\n",
    "        omse_loss = mse_loss(vog_third, v3_e)\n",
    "        \n",
    "        third_train_op.zero_grad()\n",
    "        omse_loss.backward()\n",
    "        third_train_op.step()\n",
    "        \n",
    "    # perm = torch.randperm(N)\n",
    "    # for i in range(0, N, BATCH_SIZE):\n",
    "    #     correct = 0.\n",
    "\n",
    "    #     x = svm_X[perm[i:i + BATCH_SIZE]]\n",
    "    #     y = svm_Y[perm[i:i + BATCH_SIZE]]\n",
    "\n",
    "    #     x = torch.tensor(x.clone().detach())\n",
    "    #     y = torch.tensor(y.clone().detach())\n",
    "\n",
    "    #     # Forward\n",
    "    #     output = svm_model(x)\n",
    "        \n",
    "    #     # Backward\n",
    "    #     svm_optimizer.zero_grad()        \n",
    "    #     svm_optimizer.step()\n",
    "\n",
    "    #     predicted = output.data >= 0\n",
    "    #     correct += float(\n",
    "    #         predicted.view(-1) == output.data\n",
    "    #     )\n",
    "    # torch.save(svm_model, \"Train_svm_model.pth\")\n",
    "    \n",
    "    acc_v = (vog_third >= 0).float()\n",
    "    acc = get_acc(\n",
    "        acc_v, v3_e\n",
    "    ) * 100\n",
    "    \n",
    "    if acc > best_acc:\n",
    "        best_acc = acc    \n",
    "        path = \"./say_cheese/ahh_saveMode_through_\" + str(epoch) + \"_\" + str(acc) + \"GBRBM.pth\"\n",
    "        torch.save(rbm_third.state_dict(), path)\n",
    "    output_gb.append(v3_e)\n",
    "\n",
    "    print(\"GB-DBN Training loss for {0}th epoch {1}\\tEstimate time : {2}\\tAcc : {3}\\t\\tBest Acc : {4}\\t\\tIgnite Acc: {5}\" \\\n",
    "        .format(epoch + 1, omse_loss, time.time() - start, acc, best_acc, tmp_acc))\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Accuracy :  62.03208556149733 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Last Accuracy : \", acc, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HILAB_~1\\AppData\\Local\\Temp/ipykernel_12512/148538284.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(data, dtype=torch.float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.46524064171123"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load_model = RBM(VISIBLE_UNITS[2], VISIBLE_UNITS[2], K_FOLD, BATCH_SIZE)\n",
    "# load_model.load_state_dict(torch.load(\"./saveMode_through_99_epoch_90.0_acc_GBDBN.pth\"))\n",
    "\n",
    "LOAD_PATH = \"./say_cheese/ahh_saveMode_through_83_71.12299465240642GBRBM.pth\"\n",
    "\n",
    "load_model = RBM(VISIBLE_UNITS[2], HIDDEN_UNITS[2], K_FOLD, BATCH_SIZE)\n",
    "load_model.load_state_dict((torch.load(LOAD_PATH)))\n",
    "load_model.to(device=device)\n",
    "\n",
    "print(len(test_dataloader))\n",
    "\n",
    "for i, (data) in enumerate(test_dataloader):\n",
    "        data = Variable(\n",
    "                torch.tensor(data, dtype=torch.float32)\n",
    "        ).uniform_(0, 1)\n",
    "        \n",
    "        sample_data = torch.bernoulli(data).to(device=device)\n",
    "\n",
    "        # tensor binary\n",
    "        vog_first, v1 = load_model(sample_data)\n",
    "        omse_loss = mse_loss(vog_first, v1)\n",
    "\n",
    "        first_train_op.zero_grad()\n",
    "        first_train_op.step()\n",
    "        omse_loss.backward()\n",
    "get_acc(vog_first, v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_gb_cp = list()\n",
    "cpu = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_acc = float()\n",
    "best_acc = float()\n",
    "print(\"Test Code GB-DBN Start\")\n",
    "for i, (data) in enumerate(test_dataloader):\n",
    "        data = Variable(\n",
    "                torch.tensor(data, dtype=torch.float32)\n",
    "        ).uniform_(0, 1)\n",
    "        \n",
    "        sample_data = torch.bernoulli(data).view(-1, 10).to(device=device)\n",
    "\n",
    "        # tensor binary\n",
    "        vog_first, v1 = rbm_first(sample_data)\n",
    "        omse_loss = mse_loss(vog_first, v1)\n",
    "\n",
    "        first_train_op.zero_grad()\n",
    "        first_train_op.step()\n",
    "        omse_loss.backward()\n",
    "\n",
    "for i, (data) in enumerate(v1):\n",
    "        data = Variable(\n",
    "                torch.tensor(data, dtype=torch.float32)\n",
    "        ).uniform_(0, 1)\n",
    "        \n",
    "        sample_data = torch.bernoulli(data).view(-1, 10).to(device=device)\n",
    "\n",
    "        # tensor binary\n",
    "        vog_second, v2 = rbm_first(sample_data)\n",
    "        omse_loss = mse_loss(vog_second, v2)\n",
    "\n",
    "        second_train_op.zero_grad()\n",
    "        second_train_op.step()\n",
    "        omse_loss.backward()\n",
    "\n",
    "for i, (data) in enumerate(v2):\n",
    "        data = Variable(\n",
    "                torch.tensor(data, dtype=torch.float32)\n",
    "        ).uniform_(0, 1)\n",
    "        \n",
    "        sample_data = torch.bernoulli(data).view(-1, 10).to(device=device)\n",
    "\n",
    "        # tensor binary\n",
    "        vog_second, v3 = rbm_first(sample_data)\n",
    "        omse_loss = mse_loss(vog_second, v3)\n",
    "\n",
    "        second_train_op.zero_grad()\n",
    "        second_train_op.step()\n",
    "        omse_loss.backward()\n",
    "        run_acc += (sample_data == v3).sum().item()\n",
    " \n",
    "for _, (data) in enumerate(v3): \n",
    "        data = Variable(\n",
    "                torch.tensor(data, dtype=torch.float32)\n",
    "        ).uniform_(0, 1)\n",
    "\n",
    "        sample_data = torch.normal(mean=data, std=gaussian_std).view(-1, 10).to(device=device)\n",
    "\n",
    "        # tensor binary\n",
    "        vog_second, v1 = rbm_first(sample_data)\n",
    "        omse_loss = mse_loss(vog_second, v1)\n",
    "\n",
    "        second_train_op.zero_grad()\n",
    "        omse_loss.backward()\n",
    "        second_train_op.step()\n",
    "\n",
    "for _, (data) in enumerate(v1): \n",
    "        data = Variable(\n",
    "                torch.tensor(data, dtype=torch.float32)\n",
    "        ).uniform_(0, 1)\n",
    "\n",
    "        sample_data = torch.normal(mean=data, std=gaussian_std).view(-1, 10).to(device=device)\n",
    "\n",
    "        # tensor binary\n",
    "        vog_second, v2 = rbm_second(sample_data)\n",
    "        omse_loss = mse_loss(vog_second, v2)\n",
    "\n",
    "        second_train_op.zero_grad()\n",
    "        omse_loss.backward()\n",
    "        second_train_op.step()\n",
    "        \n",
    "for _, (data) in enumerate(v2): \n",
    "        data = Variable(\n",
    "                torch.tensor(data, dtype=torch.float32)\n",
    "        ).uniform_(0, 1)\n",
    "\n",
    "        sample_data = torch.normal(mean=data, std=gaussian_std).view(-1, 10).to(device=device)\n",
    "\n",
    "        # tensor binary\n",
    "        vog_second, v3 = rbm_third(sample_data)\n",
    "        omse_loss = mse_loss(vog_second, v3)\n",
    "\n",
    "        second_train_op.zero_grad()\n",
    "        omse_loss.backward()\n",
    "        second_train_op.step()\n",
    "\n",
    "print(\"GB-DBN Training loss: {0}\\tEstimate time : {1}\\tAcc : {2}\" .format(omse_loss, time.time() - start, acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_v = torch.tensor(v3.clone().detach(), device=torch.device('cpu'))\n",
    "print(svm_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.FloatTensor(svm_v)\n",
    "Y = torch.FloatTensor(svm_v)\n",
    "N = len(Y)\n",
    "\n",
    "model = SVM(lr=LEARNING_RATE, n_x=10)\n",
    "optimizer = optim.Adagrad(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "correct = 0.\n",
    "cnt_tot = 0\n",
    "\n",
    "x = torch.tensor(X)\n",
    "y = torch.tensor(Y)\n",
    "\n",
    "# Forward\n",
    "output = model(x)\n",
    "\n",
    "# Backward\n",
    "optimizer.zero_grad()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6c3cabcf2f29820bdd7faae982b59d335e0d215fb5382d93f3312fa3292e9b7f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
