{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Pre-processing for make clean\n",
      "2022-05-29 21:55:58.294123 model.py code start\n",
      "NVIDIA GeForce RTX 3080 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "from ignite.contrib.metrics.regression import *\n",
    "from ignite.contrib.metrics import *\n",
    "from ignite.handlers import *\n",
    "from ignite.metrics import *\n",
    "from ignite.engine import *\n",
    "from ignite.utils import *\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "\n",
    "from active_learning.active_learning import ActiveLearner\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "import read_samples as rs\n",
    "\n",
    "from collections import OrderedDict\n",
    "from scipy import signal\n",
    "import numpy as np\n",
    "import datetime\n",
    "import warnings\n",
    "import pickle\n",
    "import shutup\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "\n",
    "print(datetime.datetime.now(), \"model.py code start\")\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "EPOCH = 100\n",
    "LEARNING_RATE = 0.2\n",
    "ANNEALING_RATE = 0.999\n",
    "HIDDEN_UNITS = [180, 200, 250, 80, 100, 120]\n",
    "K_FOLD = 1\n",
    "VISIBLE_NUM = 50\n",
    "MAT_PATH = \"C:/Users/HILAB_Labtop_02/Desktop/insung/ecg-dbn/data/mit.mat\"\n",
    "\n",
    "PATH = \"./pickle/mit\"\n",
    "DB1_LIST = [101, 106, 108, 109, 112, 114, 116, 118, 119, 122, 124, 201, 203, 205, 207, 208, 209, 215, 220, 221, 223, 230]\n",
    "DB2_LIST = [100, 103, 105, 111, 113, 117, 121, 123, 200, 202, 210, 212, 213, 214, 219, 221, 222, 228, 231, 232, 233, 234]\n",
    "\n",
    "db1_sig, db1_ann = list(), list()\n",
    "db1_resampled_sig, db1_resampled_ann = list(), list()\n",
    "db3_resampled_sig, db3_resampled_ann = list(), list()\n",
    "\n",
    "device = torch.device('cuda')\n",
    "cpu = torch.device('cpu')\n",
    "print(torch.cuda.get_device_name(device))\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "shutup.please()\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBM(nn.Module): \n",
    "    with torch.cuda.device(0):\n",
    "        def __init__(self, n_vis, n_hid, k, batch):\n",
    "            super(RBM, self).__init__()\n",
    "            self.W      = nn.Parameter(torch.randn(1, batch, device=device) * 1e-2)\n",
    "            self.n_vis  = n_vis\n",
    "            self.n_hid  = n_hid\n",
    "            self.k      = k\n",
    "            self.batch  = batch\n",
    "            self.v_bias = nn.Parameter(torch.zeros(n_vis, 1, device=device))\n",
    "            self.h_bias = nn.Parameter(torch.zeros(n_hid, 1, device=device))\n",
    "        \n",
    "        def sample_from_p(self, p):\n",
    "            return F.relu(\n",
    "                torch.sign(\n",
    "                    p - Variable(torch.randn(p.size(), device=device))\n",
    "                )\n",
    "            ).to(device=device)\n",
    "\n",
    "        ''' ISSUE PART '''\n",
    "        def v_to_h(self, v):\n",
    "            w = (self.W.clone())\n",
    "            # print(\"v -> h\", v.size(), w.size())\n",
    "            \n",
    "            p_h = F.sigmoid(\n",
    "                F.linear(v, w)\n",
    "            ).to(device=device)\n",
    "\n",
    "            sample_h = self.sample_from_p(p_h)\n",
    "            return p_h, sample_h\n",
    "\n",
    "        def h_to_v(self, h):\n",
    "            w = self.W.t().clone()\n",
    "            # print(\"h -> v\", h.size(), w.size())\n",
    "\n",
    "            p_v = F.sigmoid(\n",
    "                F.linear(h, w)\n",
    "            ).to(device=device)\n",
    "\n",
    "            sample_v = self.sample_from_p(p_v)\n",
    "            return p_v, sample_v\n",
    "        \n",
    "        def forward(self, v):\n",
    "            pre_h1, h1 = self.v_to_h(v)\n",
    "            h_ = h1\n",
    "\n",
    "            for _ in range(self.k):\n",
    "                pre_v_, v_ = self.h_to_v(h_)\n",
    "                pre_h_, h_ = self.v_to_h(v_)\n",
    "            return v, v_\n",
    "        \n",
    "        def get_weight(self):\n",
    "            return self.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_step(engine, batch):\n",
    "    return batch\n",
    "\n",
    "default_model = nn.Sequential(OrderedDict([\n",
    "    ('base', nn.Linear(4, 2)),\n",
    "    ('fc', nn.Linear(2, 1))\n",
    "]))\n",
    "\n",
    "default_evaluator = Engine(eval_step)\n",
    "\n",
    "def get_acc(y_true, y_pred):\n",
    "    metric = Accuracy()\n",
    "    metric.attach(default_evaluator, \"accuracy\")\n",
    "    state = default_evaluator.run([[y_pred, y_true]])\n",
    "    return state.metrics[\"accuracy\"]\n",
    "    \n",
    "def to_categorical(y, num_classes):\n",
    "    return np.eye(num_classes, dtype='uint8')[y]\n",
    "\n",
    "def svm(x, w, b):\n",
    "    h = (w*x).sum(1) + b\n",
    "    return h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM_Model(nn.Module):\n",
    "    def __init__ (self, D_in, D_out):\n",
    "        super(SVM_Model, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, D_out, device=device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 22\n",
      "22 22\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(DB1_LIST)):\n",
    "    pickle_path = PATH + str(DB1_LIST[i]) + \".pkl\"\n",
    "    with open(pickle_path, 'rb') as f:\n",
    "        db1_sig.append(pickle.load(f)[0])\n",
    "        \n",
    "    with open(pickle_path, 'rb') as f:\n",
    "        db1_ann.append(pickle.load(f)[1])\n",
    "        \n",
    "print(len(db1_sig), len(db1_ann))\n",
    "\n",
    "temp_sg, temp_ann = list(), list()\n",
    "\n",
    "for i in range(len(db1_sig)):\n",
    "    for j in range(len(db1_sig[i])):\n",
    "        temp_sg.append(signal.resample(db1_sig[i][j], 50))\n",
    "    db1_resampled_sig.append(temp_sg)\n",
    "\n",
    "for i in range(len(db1_ann)):\n",
    "    for j in range(len(db1_ann[i])):\n",
    "        if db1_ann[i][j] == \"N\":\n",
    "            # temp_ann.append(db1_ann[i][j])\n",
    "            temp_ann.append(0)\n",
    "        \n",
    "        elif db1_ann[i][j] == \"S\" or \"V\" or \"F\":\n",
    "            # temp_ann.append(db1_ann[i][j])\n",
    "            temp_ann.append(1)\n",
    "        \n",
    "        else:\n",
    "            temp_ann.append(1)\n",
    "    \n",
    "    db1_resampled_ann.append(temp_ann)\n",
    "\n",
    "print(len(db1_resampled_sig), len(db1_resampled_ann))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RSLT]\t\t\t Export records ...\n",
      "100 100\n",
      "100 100\n"
     ]
    }
   ],
   "source": [
    "PATH = \"./svdb_pic/\"\n",
    "\n",
    "with open(PATH + 'RECORDS') as f:\n",
    "    record_lines = f.readlines()\n",
    "\n",
    "pre_records = []\n",
    "for x in record_lines:\n",
    "    pre_records.append(x.strip())\n",
    "print(\"[RSLT]\\t\\t\\t Export records ...\")\n",
    "\n",
    "for i in range(len(pre_records)):\n",
    "    pickle_path = PATH + \"svdb\" + str(pre_records[i]) + \".pkl\"\n",
    "    with open(pickle_path, 'rb') as f:\n",
    "        db1_sig.append(pickle.load(f)[0])\n",
    "        \n",
    "    with open(pickle_path, 'rb') as f:\n",
    "        db1_ann.append(pickle.load(f)[1])\n",
    "        \n",
    "print(len(db1_sig), len(db1_ann))\n",
    "\n",
    "temp_sg, temp_ann = list(), list()\n",
    "\n",
    "for i in range(len(db1_sig)):\n",
    "    for j in range(len(db1_sig[i])):\n",
    "        temp_sg.append(signal.resample(db1_sig[i][j], 50))\n",
    "    db3_resampled_sig.append(temp_sg)\n",
    "\n",
    "for i in range(len(db1_ann)):\n",
    "    for j in range(len(db1_ann[i])):\n",
    "        if db1_ann[i][j] == \"N\":\n",
    "            # temp_ann.append(db1_ann[i][j])\n",
    "            temp_ann.append(0)\n",
    "        \n",
    "        elif db1_ann[i][j] == \"S\" or \"V\" or \"F\":\n",
    "            # temp_ann.append(db1_ann[i][j])\n",
    "            temp_ann.append(1)\n",
    "        \n",
    "        else:\n",
    "            temp_ann.append(\"F\")\n",
    "    \n",
    "    db3_resampled_ann.append(temp_ann)\n",
    "\n",
    "print(len(db3_resampled_sig), len(db3_resampled_ann))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50])\n"
     ]
    }
   ],
   "source": [
    "bbrbm_first = RBM(n_vis=VISIBLE_NUM, n_hid=HIDDEN_UNITS[3], k=K_FOLD, batch=BATCH_SIZE).to(device=device)\n",
    "bbrbm_second = RBM(n_vis=VISIBLE_NUM, n_hid=HIDDEN_UNITS[4], k=K_FOLD, batch=BATCH_SIZE).to(device=device)\n",
    "bbrbm_third = RBM(n_vis=VISIBLE_NUM, n_hid=HIDDEN_UNITS[5], k=K_FOLD, batch=BATCH_SIZE).to(device=device)\n",
    "\n",
    "gbrbm_first = RBM(n_vis=VISIBLE_NUM, n_hid=HIDDEN_UNITS[0], k=K_FOLD, batch=BATCH_SIZE).to(device=device)\n",
    "gbrbm_second = RBM(n_vis=VISIBLE_NUM, n_hid=HIDDEN_UNITS[1], k=K_FOLD, batch=BATCH_SIZE).to(device=device)\n",
    "gbrbm_third = RBM(n_vis=VISIBLE_NUM, n_hid=HIDDEN_UNITS[2], k=K_FOLD, batch=BATCH_SIZE).to(device=device)\n",
    "\n",
    "first_train_op = optim.Adagrad(bbrbm_first.parameters(), LEARNING_RATE)\n",
    "second_train_op = optim.Adagrad(bbrbm_second.parameters(), LEARNING_RATE)\n",
    "third_train_op = optim.Adagrad(bbrbm_third.parameters(), LEARNING_RATE)\n",
    "\n",
    "gb_first_train_op = optim.Adagrad(gbrbm_first.parameters(), LEARNING_RATE)\n",
    "gb_second_train_op = optim.Adagrad(gbrbm_second.parameters(), LEARNING_RATE)\n",
    "gb_third_train_op = optim.Adagrad(gbrbm_third.parameters(), LEARNING_RATE)\n",
    "\n",
    "omse_loss = list()\n",
    "output_gb = list()\n",
    "best_acc = float()\n",
    "svm_best_acc = float()\n",
    "mse_loss = nn.MSELoss().to(device=device)\n",
    "        \n",
    "clf = make_pipeline(StandardScaler(), SVC(gamma='auto', probability=True))\n",
    "\n",
    "gaussian_std = torch.arange(1, 0, -0.02, device=device)\n",
    "print(gaussian_std.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = DataLoader(db1_resampled_sig,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              num_workers=0,\n",
    "                              shuffle=True)\n",
    "\n",
    "print(len(train_dataloader.dataset))\n",
    "\n",
    "test_dataloader = DataLoader(db3_resampled_sig,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             num_workers=0,\n",
    "                             shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RBM START!\n",
      "GB-DBN Training loss for 1th epoch 0.47999998927116394\tEstimate time : 2.786309003829956\tAcc : 72.0\t\tBest Acc : 82.0\tSVM Predict : ?\tTrue : ?\n",
      "GB-DBN Training loss for 2th epoch 0.5\tEstimate time : 2.8188743591308594\tAcc : 68.0\t\tBest Acc : 82.0\tSVM Predict : ?\tTrue : ?\n",
      "GB-DBN Training loss for 3th epoch 0.3999999761581421\tEstimate time : 2.8201355934143066\tAcc : 68.0\t\tBest Acc : 82.0\tSVM Predict : ?\tTrue : ?\n",
      "GB-DBN Training loss for 4th epoch 0.5600000023841858\tEstimate time : 2.7900874614715576\tAcc : 70.0\t\tBest Acc : 82.0\tSVM Predict : Normal\tTrue : Normal\n",
      "GB-DBN Training loss for 5th epoch 0.5799999833106995\tEstimate time : 4.536893606185913\tAcc : 78.0\t\tBest Acc : 82.0\tSVM Predict : ?\tTrue : ?\n",
      "GB-DBN Training loss for 6th epoch 0.5999999642372131\tEstimate time : 4.5602827072143555\tAcc : 70.0\t\tBest Acc : 82.0\tSVM Predict : Normal\tTrue : Normal\n",
      "GB-DBN Training loss for 7th epoch 0.41999998688697815\tEstimate time : 2.988515615463257\tAcc : 64.0\t\tBest Acc : 82.0\tSVM Predict : ?\tTrue : ?\n",
      "GB-DBN Training loss for 8th epoch 0.4599999785423279\tEstimate time : 3.097508668899536\tAcc : 74.0\t\tBest Acc : 82.0\tSVM Predict : ?\tTrue : ?\n",
      "GB-DBN Training loss for 9th epoch 0.6399999856948853\tEstimate time : 3.264901638031006\tAcc : 62.0\t\tBest Acc : 82.0\tSVM Predict : Normal\tTrue : Normal\n",
      "GB-DBN Training loss for 10th epoch 0.5199999809265137\tEstimate time : 3.231227159500122\tAcc : 78.0\t\tBest Acc : 82.0\tSVM Predict : Normal\tTrue : Normal\n",
      "GB-DBN Training loss for 11th epoch 0.5399999618530273\tEstimate time : 3.1424202919006348\tAcc : 82.0\t\tBest Acc : 82.0\tSVM Predict : ?\tTrue : ?\n",
      "GB-DBN Training loss for 12th epoch 0.3999999761581421\tEstimate time : 3.063004970550537\tAcc : 64.0\t\tBest Acc : 82.0\tSVM Predict : ?\tTrue : ?\n",
      "GB-DBN Training loss for 13th epoch 0.5600000023841858\tEstimate time : 3.004988193511963\tAcc : 62.0\t\tBest Acc : 82.0\tSVM Predict : ?\tTrue : ?\n",
      "GB-DBN Training loss for 14th epoch 0.5199999809265137\tEstimate time : 2.9469568729400635\tAcc : 66.0\t\tBest Acc : 82.0\tSVM Predict : ?\tTrue : ?\n",
      "GB-DBN Training loss for 15th epoch 0.5999999642372131\tEstimate time : 2.8868091106414795\tAcc : 82.0\t\tBest Acc : 82.0\tSVM Predict : Normal\tTrue : Normal\n",
      "GB-DBN Training loss for 16th epoch 0.5799999833106995\tEstimate time : 2.9784719944000244\tAcc : 66.0\t\tBest Acc : 82.0\tSVM Predict : ?\tTrue : ?\n",
      "GB-DBN Training loss for 17th epoch 0.4599999785423279\tEstimate time : 2.928823471069336\tAcc : 74.0\t\tBest Acc : 82.0\tSVM Predict : ?\tTrue : ?\n",
      "GB-DBN Training loss for 18th epoch 0.6399999856948853\tEstimate time : 2.8685247898101807\tAcc : 74.0\t\tBest Acc : 82.0\tSVM Predict : ?\tTrue : ?\n",
      "GB-DBN Training loss for 19th epoch 0.3999999761581421\tEstimate time : 2.9098641872406006\tAcc : 68.0\t\tBest Acc : 82.0\tSVM Predict : ?\tTrue : ?\n",
      "GB-DBN Training loss for 20th epoch 0.47999998927116394\tEstimate time : 2.8566102981567383\tAcc : 70.0\t\tBest Acc : 82.0\tSVM Predict : Normal\tTrue : Normal\n",
      "GB-DBN Training loss for 21th epoch 0.5999999642372131\tEstimate time : 2.882699728012085\tAcc : 56.00000000000001\t\tBest Acc : 82.0\tSVM Predict : ?\tTrue : ?\n",
      "GB-DBN Training loss for 22th epoch 0.5999999642372131\tEstimate time : 2.922342538833618\tAcc : 72.0\t\tBest Acc : 82.0\tSVM Predict : ?\tTrue : ?\n",
      "GB-DBN Training loss for 23th epoch 0.4399999976158142\tEstimate time : 2.9045207500457764\tAcc : 52.0\t\tBest Acc : 82.0\tSVM Predict : ?\tTrue : ?\n",
      "GB-DBN Training loss for 24th epoch 0.5\tEstimate time : 2.9122085571289062\tAcc : 76.0\t\tBest Acc : 82.0\tSVM Predict : ?\tTrue : ?\n",
      "GB-DBN Training loss for 25th epoch 0.4399999976158142\tEstimate time : 2.942355155944824\tAcc : 74.0\t\tBest Acc : 82.0\tSVM Predict : ?\tTrue : ?\n",
      "GB-DBN Training loss for 26th epoch 0.5399999618530273\tEstimate time : 2.872457265853882\tAcc : 57.99999999999999\t\tBest Acc : 82.0\tSVM Predict : ?\tTrue : ?\n",
      "GB-DBN Training loss for 27th epoch 0.5\tEstimate time : 2.911475658416748\tAcc : 70.0\t\tBest Acc : 82.0\tSVM Predict : ?\tTrue : ?\n",
      "GB-DBN Training loss for 28th epoch 0.3999999761581421\tEstimate time : 3.025073528289795\tAcc : 70.0\t\tBest Acc : 82.0\tSVM Predict : ?\tTrue : ?\n",
      "GB-DBN Training loss for 29th epoch 0.6399999856948853\tEstimate time : 2.970595359802246\tAcc : 66.0\t\tBest Acc : 82.0\tSVM Predict : ?\tTrue : ?\n",
      "GB-DBN Training loss for 30th epoch 0.35999998450279236\tEstimate time : 2.8801190853118896\tAcc : 68.0\t\tBest Acc : 82.0\tSVM Predict : ?\tTrue : ?\n",
      "GB-DBN Training loss for 31th epoch 0.5\tEstimate time : 2.9446544647216797\tAcc : 72.0\t\tBest Acc : 82.0\tSVM Predict : ?\tTrue : ?\n",
      "GB-DBN Training loss for 32th epoch 0.5600000023841858\tEstimate time : 2.9328043460845947\tAcc : 60.0\t\tBest Acc : 82.0\tSVM Predict : Normal\tTrue : Normal\n",
      "GB-DBN Training loss for 33th epoch 0.5600000023841858\tEstimate time : 2.9286694526672363\tAcc : 66.0\t\tBest Acc : 82.0\tSVM Predict : ?\tTrue : ?\n",
      "GB-DBN Training loss for 34th epoch 0.5199999809265137\tEstimate time : 2.89404296875\tAcc : 78.0\t\tBest Acc : 82.0\tSVM Predict : ?\tTrue : ?\n",
      "GB-DBN Training loss for 35th epoch 0.5799999833106995\tEstimate time : 2.9229886531829834\tAcc : 72.0\t\tBest Acc : 82.0\tSVM Predict : ?\tTrue : ?\n",
      "GB-DBN Training loss for 36th epoch 0.5399999618530273\tEstimate time : 2.9153943061828613\tAcc : 74.0\t\tBest Acc : 82.0\tSVM Predict : ?\tTrue : ?\n",
      "GB-DBN Training loss for 37th epoch 0.5799999833106995\tEstimate time : 2.9622786045074463\tAcc : 56.00000000000001\t\tBest Acc : 82.0\tSVM Predict : ?\tTrue : ?\n",
      "GB-DBN Training loss for 38th epoch 0.3999999761581421\tEstimate time : 2.9121451377868652\tAcc : 66.0\t\tBest Acc : 82.0\tSVM Predict : ?\tTrue : ?\n",
      "GB-DBN Training loss for 39th epoch 0.4399999976158142\tEstimate time : 2.915363073348999\tAcc : 72.0\t\tBest Acc : 82.0\tSVM Predict : Normal\tTrue : Normal\n",
      "GB-DBN Training loss for 40th epoch 0.5399999618530273\tEstimate time : 3.0241293907165527\tAcc : 74.0\t\tBest Acc : 82.0\tSVM Predict : Normal\tTrue : Normal\n",
      "GB-DBN Training loss for 41th epoch 0.3999999761581421\tEstimate time : 2.9516286849975586\tAcc : 64.0\t\tBest Acc : 82.0\tSVM Predict : Normal\tTrue : Normal\n",
      "GB-DBN Training loss for 42th epoch 0.5399999618530273\tEstimate time : 2.9389419555664062\tAcc : 72.0\t\tBest Acc : 82.0\tSVM Predict : Abnormal\tTrue : Abnormal\n",
      "GB-DBN Training loss for 43th epoch 0.41999998688697815\tEstimate time : 2.902965784072876\tAcc : 74.0\t\tBest Acc : 82.0\tSVM Predict : Normal\tTrue : Normal\n",
      "GB-DBN Training loss for 44th epoch 0.5\tEstimate time : 2.8913891315460205\tAcc : 62.0\t\tBest Acc : 82.0\tSVM Predict : Normal\tTrue : Normal\n",
      "GB-DBN Training loss for 45th epoch 0.3999999761581421\tEstimate time : 2.90716814994812\tAcc : 66.0\t\tBest Acc : 82.0\tSVM Predict : Normal\tTrue : Normal\n",
      "GB-DBN Training loss for 46th epoch 0.5600000023841858\tEstimate time : 2.8900277614593506\tAcc : 72.0\t\tBest Acc : 82.0\tSVM Predict : ?\tTrue : ?\n",
      "GB-DBN Training loss for 47th epoch 0.47999998927116394\tEstimate time : 2.8697378635406494\tAcc : 74.0\t\tBest Acc : 82.0\tSVM Predict : ?\tTrue : ?\n",
      "GB-DBN Training loss for 48th epoch 0.5199999809265137\tEstimate time : 2.913064956665039\tAcc : 60.0\t\tBest Acc : 82.0\tSVM Predict : Normal\tTrue : Normal\n",
      "GB-DBN Training loss for 49th epoch 0.4599999785423279\tEstimate time : 2.8860630989074707\tAcc : 70.0\t\tBest Acc : 82.0\tSVM Predict : Normal\tTrue : Normal\n",
      "GB-DBN Training loss for 50th epoch 0.5799999833106995\tEstimate time : 2.887441873550415\tAcc : 66.0\t\tBest Acc : 82.0\tSVM Predict : Normal\tTrue : Normal\n",
      "GB-DBN Training loss for 51th epoch 0.5799999833106995\tEstimate time : 2.8946685791015625\tAcc : 72.0\t\tBest Acc : 82.0\tSVM Predict : ?\tTrue : ?\n",
      "GB-DBN Training loss for 52th epoch 0.5199999809265137\tEstimate time : 2.907747507095337\tAcc : 66.0\t\tBest Acc : 82.0\tSVM Predict : Normal\tTrue : Normal\n",
      "GB-DBN Training loss for 53th epoch 0.6399999856948853\tEstimate time : 2.897498369216919\tAcc : 78.0\t\tBest Acc : 82.0\tSVM Predict : Normal\tTrue : Normal\n",
      "GB-DBN Training loss for 54th epoch 0.4399999976158142\tEstimate time : 2.8999783992767334\tAcc : 72.0\t\tBest Acc : 82.0\tSVM Predict : ?\tTrue : ?\n",
      "GB-DBN Training loss for 55th epoch 0.47999998927116394\tEstimate time : 2.8867626190185547\tAcc : 82.0\t\tBest Acc : 82.0\tSVM Predict : Normal\tTrue : Abnormal\n",
      "GB-DBN Training loss for 56th epoch 0.5799999833106995\tEstimate time : 2.8967173099517822\tAcc : 60.0\t\tBest Acc : 82.0\tSVM Predict : Normal\tTrue : Normal\n",
      "GB-DBN Training loss for 57th epoch 0.5\tEstimate time : 2.9061152935028076\tAcc : 57.99999999999999\t\tBest Acc : 82.0\tSVM Predict : Abnormal\tTrue : Normal\n",
      "GB-DBN Training loss for 58th epoch 0.4599999785423279\tEstimate time : 2.9114341735839844\tAcc : 70.0\t\tBest Acc : 82.0\tSVM Predict : Normal\tTrue : Normal\n",
      "GB-DBN Training loss for 59th epoch 0.4399999976158142\tEstimate time : 2.920429229736328\tAcc : 76.0\t\tBest Acc : 82.0\tSVM Predict : Abnormal\tTrue : Normal\n",
      "GB-DBN Training loss for 60th epoch 0.5600000023841858\tEstimate time : 2.9314522743225098\tAcc : 84.0\t\tBest Acc : 84.0\tSVM Predict : Normal\tTrue : Normal\n",
      "GB-DBN Training loss for 61th epoch 0.4399999976158142\tEstimate time : 3.0532283782958984\tAcc : 70.0\t\tBest Acc : 84.0\tSVM Predict : Normal\tTrue : Normal\n",
      "GB-DBN Training loss for 62th epoch 0.5399999618530273\tEstimate time : 2.949275255203247\tAcc : 62.0\t\tBest Acc : 84.0\tSVM Predict : Normal\tTrue : Normal\n",
      "GB-DBN Training loss for 63th epoch 0.6200000047683716\tEstimate time : 2.9263503551483154\tAcc : 70.0\t\tBest Acc : 84.0\tSVM Predict : Normal\tTrue : Abnormal\n",
      "GB-DBN Training loss for 64th epoch 0.4399999976158142\tEstimate time : 2.902275800704956\tAcc : 62.0\t\tBest Acc : 84.0\tSVM Predict : Normal\tTrue : Abnormal\n",
      "GB-DBN Training loss for 65th epoch 0.35999998450279236\tEstimate time : 2.983672857284546\tAcc : 64.0\t\tBest Acc : 84.0\tSVM Predict : Normal\tTrue : Normal\n",
      "GB-DBN Training loss for 66th epoch 0.4399999976158142\tEstimate time : 2.9233083724975586\tAcc : 74.0\t\tBest Acc : 84.0\tSVM Predict : Abnormal\tTrue : Abnormal\n",
      "GB-DBN Training loss for 67th epoch 0.47999998927116394\tEstimate time : 2.9249422550201416\tAcc : 70.0\t\tBest Acc : 84.0\tSVM Predict : Normal\tTrue : Normal\n",
      "GB-DBN Training loss for 68th epoch 0.4399999976158142\tEstimate time : 2.907163143157959\tAcc : 76.0\t\tBest Acc : 84.0\tSVM Predict : Abnormal\tTrue : Normal\n",
      "GB-DBN Training loss for 69th epoch 0.3799999952316284\tEstimate time : 2.9497084617614746\tAcc : 62.0\t\tBest Acc : 84.0\tSVM Predict : Normal\tTrue : Normal\n",
      "GB-DBN Training loss for 70th epoch 0.5600000023841858\tEstimate time : 2.9785850048065186\tAcc : 84.0\t\tBest Acc : 84.0\tSVM Predict : ?\tTrue : ?\n",
      "GB-DBN Training loss for 71th epoch 0.5600000023841858\tEstimate time : 2.934175491333008\tAcc : 64.0\t\tBest Acc : 84.0\tSVM Predict : ?\tTrue : ?\n",
      "GB-DBN Training loss for 72th epoch 0.4599999785423279\tEstimate time : 2.9575133323669434\tAcc : 74.0\t\tBest Acc : 84.0\tSVM Predict : Normal\tTrue : Abnormal\n",
      "GB-DBN Training loss for 73th epoch 0.5\tEstimate time : 2.944023370742798\tAcc : 70.0\t\tBest Acc : 84.0\tSVM Predict : Abnormal\tTrue : Abnormal\n",
      "GB-DBN Training loss for 74th epoch 0.47999998927116394\tEstimate time : 2.9123711585998535\tAcc : 80.0\t\tBest Acc : 84.0\tSVM Predict : Normal\tTrue : Normal\n",
      "GB-DBN Training loss for 75th epoch 0.4399999976158142\tEstimate time : 2.937358856201172\tAcc : 64.0\t\tBest Acc : 84.0\tSVM Predict : Normal\tTrue : Normal\n",
      "GB-DBN Training loss for 76th epoch 0.5\tEstimate time : 2.985992193222046\tAcc : 72.0\t\tBest Acc : 84.0\tSVM Predict : Normal\tTrue : Normal\n",
      "GB-DBN Training loss for 77th epoch 0.5600000023841858\tEstimate time : 2.954068660736084\tAcc : 62.0\t\tBest Acc : 84.0\tSVM Predict : Normal\tTrue : Normal\n",
      "GB-DBN Training loss for 78th epoch 0.5399999618530273\tEstimate time : 2.9209256172180176\tAcc : 74.0\t\tBest Acc : 84.0\tSVM Predict : Normal\tTrue : Normal\n",
      "GB-DBN Training loss for 79th epoch 0.6200000047683716\tEstimate time : 2.936753988265991\tAcc : 84.0\t\tBest Acc : 84.0\tSVM Predict : Normal\tTrue : Abnormal\n",
      "GB-DBN Training loss for 80th epoch 0.5199999809265137\tEstimate time : 2.952880382537842\tAcc : 57.99999999999999\t\tBest Acc : 84.0\tSVM Predict : Abnormal\tTrue : Normal\n",
      "GB-DBN Training loss for 81th epoch 0.5199999809265137\tEstimate time : 2.9709954261779785\tAcc : 74.0\t\tBest Acc : 84.0\tSVM Predict : Normal\tTrue : Normal\n",
      "GB-DBN Training loss for 82th epoch 0.5199999809265137\tEstimate time : 3.0496532917022705\tAcc : 62.0\t\tBest Acc : 84.0\tSVM Predict : Normal\tTrue : Normal\n",
      "GB-DBN Training loss for 83th epoch 0.5399999618530273\tEstimate time : 3.110053062438965\tAcc : 56.00000000000001\t\tBest Acc : 84.0\tSVM Predict : Normal\tTrue : Normal\n",
      "GB-DBN Training loss for 84th epoch 0.35999998450279236\tEstimate time : 3.0987741947174072\tAcc : 74.0\t\tBest Acc : 84.0\tSVM Predict : Normal\tTrue : Normal\n",
      "GB-DBN Training loss for 85th epoch 0.3999999761581421\tEstimate time : 3.0319571495056152\tAcc : 64.0\t\tBest Acc : 84.0\tSVM Predict : Normal\tTrue : Normal\n",
      "GB-DBN Training loss for 86th epoch 0.5600000023841858\tEstimate time : 2.971417188644409\tAcc : 66.0\t\tBest Acc : 84.0\tSVM Predict : Normal\tTrue : Normal\n",
      "GB-DBN Training loss for 87th epoch 0.41999998688697815\tEstimate time : 3.09651780128479\tAcc : 64.0\t\tBest Acc : 84.0\tSVM Predict : Normal\tTrue : Normal\n",
      "GB-DBN Training loss for 88th epoch 0.4399999976158142\tEstimate time : 3.3463587760925293\tAcc : 70.0\t\tBest Acc : 84.0\tSVM Predict : ?\tTrue : ?\n",
      "GB-DBN Training loss for 89th epoch 0.47999998927116394\tEstimate time : 3.302659034729004\tAcc : 68.0\t\tBest Acc : 84.0\tSVM Predict : ?\tTrue : ?\n",
      "GB-DBN Training loss for 90th epoch 0.3999999761581421\tEstimate time : 3.0707058906555176\tAcc : 72.0\t\tBest Acc : 84.0\tSVM Predict : Normal\tTrue : Normal\n",
      "GB-DBN Training loss for 91th epoch 0.47999998927116394\tEstimate time : 3.0127956867218018\tAcc : 74.0\t\tBest Acc : 84.0\tSVM Predict : Normal\tTrue : Normal\n",
      "GB-DBN Training loss for 92th epoch 0.4399999976158142\tEstimate time : 2.991504430770874\tAcc : 72.0\t\tBest Acc : 84.0\tSVM Predict : Normal\tTrue : Normal\n",
      "GB-DBN Training loss for 93th epoch 0.41999998688697815\tEstimate time : 3.199321985244751\tAcc : 60.0\t\tBest Acc : 84.0\tSVM Predict : Normal\tTrue : Normal\n",
      "GB-DBN Training loss for 94th epoch 0.47999998927116394\tEstimate time : 3.4117894172668457\tAcc : 82.0\t\tBest Acc : 84.0\tSVM Predict : Normal\tTrue : Normal\n",
      "GB-DBN Training loss for 95th epoch 0.6399999856948853\tEstimate time : 3.099911689758301\tAcc : 66.0\t\tBest Acc : 84.0\tSVM Predict : Normal\tTrue : Normal\n",
      "GB-DBN Training loss for 96th epoch 0.5600000023841858\tEstimate time : 3.5278289318084717\tAcc : 70.0\t\tBest Acc : 84.0\tSVM Predict : Normal\tTrue : Normal\n",
      "GB-DBN Training loss for 97th epoch 0.4599999785423279\tEstimate time : 3.3127777576446533\tAcc : 76.0\t\tBest Acc : 84.0\tSVM Predict : Normal\tTrue : Normal\n",
      "GB-DBN Training loss for 98th epoch 0.5199999809265137\tEstimate time : 3.307216167449951\tAcc : 70.0\t\tBest Acc : 84.0\tSVM Predict : Normal\tTrue : Normal\n",
      "GB-DBN Training loss for 99th epoch 0.5\tEstimate time : 3.063128709793091\tAcc : 68.0\t\tBest Acc : 84.0\tSVM Predict : Normal\tTrue : Normal\n",
      "GB-DBN Training loss for 100th epoch 0.3400000035762787\tEstimate time : 3.1604034900665283\tAcc : 66.0\t\tBest Acc : 84.0\tSVM Predict : ?\tTrue : ?\n"
     ]
    }
   ],
   "source": [
    "'''BBRBM Train Part'''\n",
    "\n",
    "loss_ = []\n",
    "output_bb = []\n",
    "temp_v1 = []\n",
    "model_path_str = str()\n",
    "\n",
    "svm_cnt = 0\n",
    "svm_correct = 0\n",
    "\n",
    "print(\"RBM START!\")\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    tmp_acc = float()\n",
    "    run_acc = float()\n",
    "\n",
    "    if epoch == 0:\n",
    "        ann_for_svm = (rs.list_to_list(db1_resampled_ann)[0 : 50])\n",
    "        \n",
    "    else:\n",
    "        ann_for_svm = rs.list_to_list(db1_resampled_ann)[50 * (epoch - 1) : 50 * epoch]\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    for i, (data) in enumerate(train_dataloader):\n",
    "        data = Variable(torch.tensor(data[0].to(device), dtype=torch.float32))\n",
    "        sample_data = torch.normal(mean=data, std=gaussian_std).view(-1, 50).to(device=device)\n",
    "        \n",
    "        # tensor binary\n",
    "        vog_first, v1 = gbrbm_first(sample_data)\n",
    "        temp_v1.append(v1)\n",
    "        \n",
    "        omse_loss = mse_loss(vog_first, v1)\n",
    "        \n",
    "        gb_first_train_op.zero_grad()\n",
    "        gb_first_train_op.step()\n",
    "        omse_loss.backward()\n",
    "\n",
    "    for _, (data) in enumerate(v1):         \n",
    "        data = Variable(torch.tensor(data, dtype=torch.float32))\n",
    "        sample_data = torch.normal(mean=data, std=gaussian_std).to(device=device)\n",
    "\n",
    "        # tensor binary\n",
    "        vog_second, v2 = gbrbm_second(sample_data)\n",
    "        omse_loss = mse_loss(vog_second, v2)\n",
    "\n",
    "        gb_second_train_op.zero_grad()\n",
    "        omse_loss.backward()\n",
    "        gb_second_train_op.step()\n",
    "\n",
    "    for _, (data) in enumerate(v2):\n",
    "        data = Variable(torch.tensor(data).type(torch.float32))\n",
    "        sample_data = torch.normal(mean=data, std=gaussian_std).to(device=device)\n",
    "\n",
    "        vog_third, v3_e = gbrbm_third(sample_data)\n",
    "        omse_loss = mse_loss(vog_third, v3_e)\n",
    "        \n",
    "        gb_third_train_op.zero_grad()\n",
    "        gb_third_train_op.step()\n",
    "        omse_loss.backward()\n",
    "\n",
    "        '''First bbrbm'''\n",
    "        data = Variable(torch.tensor(v3_e, dtype=torch.float32)).uniform_(0, 1)\n",
    "        sample_data = torch.bernoulli(data).to(device=device)\n",
    "        \n",
    "        # tensor binary\n",
    "        fvog_first, v1 = bbrbm_first(sample_data)\n",
    "        omse_loss = mse_loss(fvog_first, v1)\n",
    "\n",
    "        first_train_op.zero_grad()\n",
    "        first_train_op.step()\n",
    "        omse_loss.backward()\n",
    "\n",
    "        #  **** Second BBRBM **** #\n",
    "        data = Variable(torch.tensor(v1, dtype=torch.float32)).uniform_(0, 1)\n",
    "        sample_data = torch.bernoulli(data).to(device=device)\n",
    "\n",
    "        # tensor binary\n",
    "        vog_second, v2 = bbrbm_second(sample_data)\n",
    "        omse_loss = mse_loss(vog_second, v2)\n",
    "\n",
    "        second_train_op.zero_grad()\n",
    "        omse_loss.backward()\n",
    "        second_train_op.step()\n",
    "\n",
    "        # **** THIRD BBRBM **** #\n",
    "        data = Variable(torch.tensor(v2, dtype=torch.float32)).uniform_(0, 1)\n",
    "        sample_data = torch.bernoulli(data).to(device=device)\n",
    "\n",
    "        vog_third, v3 = bbrbm_third(sample_data)\n",
    "        omse_loss = mse_loss(vog_third, v3)\n",
    "        \n",
    "        third_train_op.zero_grad()\n",
    "        omse_loss.backward()\n",
    "        third_train_op.step()\n",
    "        \n",
    "        run_acc += (sample_data == v3).sum().item()\n",
    "\n",
    "        # **** SVM PART **** #\n",
    "        x = torch.tensor(v3, requires_grad=False).cpu().tolist()\n",
    "        y = ann_for_svm\n",
    "\n",
    "        x = np.array(x)\n",
    "        y = np.array(y)\n",
    "\n",
    "    if (1 in y):\n",
    "        clf.fit(x.reshape(-1, 1), y)\n",
    "        svm_pre = clf.predict([[ann_for_svm[-1]]])\n",
    "        svm_true = ann_for_svm[-1]\n",
    "    \n",
    "        if svm_pre == 0: svm_pre = \"Normal\"\n",
    "        else: svm_pre = \"Abnormal\"\n",
    "    \n",
    "        if svm_true == 0: svm_true = \"Normal\"\n",
    "        else: svm_true = \"Abnormal\"\n",
    "        \n",
    "        svm_cnt += 1\n",
    "        \n",
    "        if svm_pre == svm_true:\n",
    "            svm_correct += 1\n",
    "    else:\n",
    "        svm_pre = \"?\"\n",
    "        svm_true = \"?\"\n",
    "\n",
    "    acc_v = (vog_third >= 0).float()\n",
    "    acc = get_acc(\n",
    "        acc_v, v3\n",
    "    ) * 100\n",
    "    \n",
    "    if acc > best_acc:\n",
    "        best_acc = acc    \n",
    "        \n",
    "        path = \"./say_cheese/ahh_saveMode_through_\" + str(epoch) + \"_\" + str(acc) + \"GBRBM.pth\"\n",
    "        model_path_str = path\n",
    "        torch.save(gbrbm_third.state_dict(), path)\n",
    "\n",
    "    output_gb.append(v3.data)\n",
    "\n",
    "    print(\"GB-DBN Training loss for {0}th epoch {1}\\tEstimate time : {2}\\tAcc : {3}\\t\\tBest Acc : {4}\\tSVM Predict : {5}\\tTrue : {6}\".format(epoch + 1, omse_loss, time.time() - start, acc, best_acc, svm_pre, svm_true))\n",
    "    \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.85\n"
     ]
    }
   ],
   "source": [
    "print(svm_correct / svm_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "al_list = list()\n",
    "for i in range(len(db1_resampled_sig)):\n",
    "    al_temp_list = list()\n",
    "    al_temp_list.append(db1_resampled_sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        , ..., -0.00882361,\n",
       "        0.0079621 , -0.00715083])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(rs.list_to_list(db1_resampled_sig[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53014"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(db1_resampled_sig[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[      0 1767129 1767130 1767131 1767132 1767133 1767134 1767135 1767128\n",
      " 1767136 1767138 1767139 1767140 1767141 1767142 1767143 1767144 1767137\n",
      " 1767145 1767127]\n",
      "[      0 1767129 1767130 1767131 1767132 1767133 1767134 1767135 1767128\n",
      " 1767136 1767138 1767139 1767140 1767141 1767142 1767143 1767144 1767137\n",
      " 1767145 1767127]\n",
      "[      0 1767129 1767130 1767131 1767132 1767133 1767134 1767135 1767128\n",
      " 1767136 1767138 1767139 1767140 1767141 1767142 1767143 1767144 1767137\n",
      " 1767145 1767127]\n",
      "[      0 1767129 1767130 1767131 1767132 1767133 1767134 1767135 1767128\n",
      " 1767136 1767138 1767139 1767140 1767141 1767142 1767143 1767144 1767137\n",
      " 1767145 1767127]\n",
      "[      0 1767129 1767130 1767131 1767132 1767133 1767134 1767135 1767128\n",
      " 1767136 1767138 1767139 1767140 1767141 1767142 1767143 1767144 1767137\n",
      " 1767145 1767127]\n",
      "[      0 1767129 1767130 1767131 1767132 1767133 1767134 1767135 1767128\n",
      " 1767136 1767138 1767139 1767140 1767141 1767142 1767143 1767144 1767137\n",
      " 1767145 1767127]\n",
      "[      0 1767129 1767130 1767131 1767132 1767133 1767134 1767135 1767128\n",
      " 1767136 1767138 1767139 1767140 1767141 1767142 1767143 1767144 1767137\n",
      " 1767145 1767127]\n",
      "[      0 1767129 1767130 1767131 1767132 1767133 1767134 1767135 1767128\n",
      " 1767136 1767138 1767139 1767140 1767141 1767142 1767143 1767144 1767137\n",
      " 1767145 1767127]\n",
      "[      0 1767129 1767130 1767131 1767132 1767133 1767134 1767135 1767128\n",
      " 1767136 1767138 1767139 1767140 1767141 1767142 1767143 1767144 1767137\n",
      " 1767145 1767127]\n",
      "[      0 1767129 1767130 1767131 1767132 1767133 1767134 1767135 1767128\n",
      " 1767136 1767138 1767139 1767140 1767141 1767142 1767143 1767144 1767137\n",
      " 1767145 1767127]\n",
      "[      0 1767129 1767130 1767131 1767132 1767133 1767134 1767135 1767128\n",
      " 1767136 1767138 1767139 1767140 1767141 1767142 1767143 1767144 1767137\n",
      " 1767145 1767127]\n",
      "[      0 1767129 1767130 1767131 1767132 1767133 1767134 1767135 1767128\n",
      " 1767136 1767138 1767139 1767140 1767141 1767142 1767143 1767144 1767137\n",
      " 1767145 1767127]\n",
      "[      0 1767129 1767130 1767131 1767132 1767133 1767134 1767135 1767128\n",
      " 1767136 1767138 1767139 1767140 1767141 1767142 1767143 1767144 1767137\n",
      " 1767145 1767127]\n",
      "[      0 1767129 1767130 1767131 1767132 1767133 1767134 1767135 1767128\n",
      " 1767136 1767138 1767139 1767140 1767141 1767142 1767143 1767144 1767137\n",
      " 1767145 1767127]\n",
      "[      0 1767129 1767130 1767131 1767132 1767133 1767134 1767135 1767128\n",
      " 1767136 1767138 1767139 1767140 1767141 1767142 1767143 1767144 1767137\n",
      " 1767145 1767127]\n",
      "[      0 1767129 1767130 1767131 1767132 1767133 1767134 1767135 1767128\n",
      " 1767136 1767138 1767139 1767140 1767141 1767142 1767143 1767144 1767137\n",
      " 1767145 1767127]\n",
      "[      0 1767129 1767130 1767131 1767132 1767133 1767134 1767135 1767128\n",
      " 1767136 1767138 1767139 1767140 1767141 1767142 1767143 1767144 1767137\n",
      " 1767145 1767127]\n",
      "[      0 1767129 1767130 1767131 1767132 1767133 1767134 1767135 1767128\n",
      " 1767136 1767138 1767139 1767140 1767141 1767142 1767143 1767144 1767137\n",
      " 1767145 1767127]\n",
      "[      0 1767129 1767130 1767131 1767132 1767133 1767134 1767135 1767128\n",
      " 1767136 1767138 1767139 1767140 1767141 1767142 1767143 1767144 1767137\n",
      " 1767145 1767127]\n",
      "[      0 1767129 1767130 1767131 1767132 1767133 1767134 1767135 1767128\n",
      " 1767136 1767138 1767139 1767140 1767141 1767142 1767143 1767144 1767137\n",
      " 1767145 1767127]\n",
      "[      0 1767129 1767130 1767131 1767132 1767133 1767134 1767135 1767128\n",
      " 1767136 1767138 1767139 1767140 1767141 1767142 1767143 1767144 1767137\n",
      " 1767145 1767127]\n",
      "[      0 1767129 1767130 1767131 1767132 1767133 1767134 1767135 1767128\n",
      " 1767136 1767138 1767139 1767140 1767141 1767142 1767143 1767144 1767137\n",
      " 1767145 1767127]\n"
     ]
    }
   ],
   "source": [
    "AL = ActiveLearner(strategy='entropy')\n",
    "for i in range(len(db1_resampled_sig)):\n",
    "    print(AL.rank(clf, np.array(rs.list_to_list(db1_resampled_sig[i])).reshape(-1, 1), num_queries=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6c3cabcf2f29820bdd7faae982b59d335e0d215fb5382d93f3312fa3292e9b7f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
