{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-28 19:45:39.036358 model.py code start\n",
      "NVIDIA GeForce RTX 3080 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "from ignite.contrib.metrics.regression import *\n",
    "from ignite.contrib.metrics import *\n",
    "from ignite.handlers import *\n",
    "from ignite.metrics import *\n",
    "from ignite.engine import *\n",
    "from ignite.utils import *\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "\n",
    "from active_learning.active_learning import ActiveLearner\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from collections import OrderedDict\n",
    "from scipy import signal\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pickle\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "\n",
    "print(datetime.datetime.now(), \"model.py code start\")\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "EPOCH = 200\n",
    "LEARNING_RATE = 0.2\n",
    "ANNEALING_RATE = 0.999\n",
    "HIDDEN_UNITS = [180, 200, 250, 80, 100, 120]\n",
    "K_FOLD = 1\n",
    "VISIBLE_NUM = 50\n",
    "MAT_PATH = \"C:/Users/HILAB_Labtop_02/Desktop/insung/ecg-dbn/data/mit.mat\"\n",
    "\n",
    "PATH = \"./pickle/mit\"\n",
    "DB1_LIST = [101, 106, 108, 109, 112, 114, 116, 118, 119, 122, 124, 201, 203, 205, 207, 208, 209, 215, 220, 221, 223, 230]\n",
    "DB2_LIST = [100, 103, 105, 111, 113, 117, 121, 123, 200, 202, 210, 212, 213, 214, 219, 221, 222, 228, 231, 232, 233, 234]\n",
    "\n",
    "db1_sig, db1_ann = list(), list()\n",
    "db1_resampled_sig, db1_resampled_ann = list(), list()\n",
    "db3_resampled_sig, db3_resampled_ann = list(), list()\n",
    "\n",
    "device = torch.device('cuda')\n",
    "cpu = torch.device('cpu')\n",
    "print(torch.cuda.get_device_name(device))\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBM(nn.Module): \n",
    "    with torch.cuda.device(0):\n",
    "        def __init__(self, n_vis, n_hid, k, batch):\n",
    "            super(RBM, self).__init__()\n",
    "            self.W      = nn.Parameter(torch.randn(1, batch, device=device) * 1e-2)\n",
    "            self.n_vis  = n_vis\n",
    "            self.n_hid  = n_hid\n",
    "            self.k      = k\n",
    "            self.batch  = batch\n",
    "            self.v_bias = nn.Parameter(torch.zeros(n_vis, 1, device=device))\n",
    "            self.h_bias = nn.Parameter(torch.zeros(n_hid, 1, device=device))\n",
    "        \n",
    "        def sample_from_p(self, p):\n",
    "            return F.relu(\n",
    "                torch.sign(\n",
    "                    p - Variable(torch.randn(p.size(), device=device))\n",
    "                )\n",
    "            ).to(device=device)\n",
    "\n",
    "        ''' ISSUE PART '''\n",
    "        def v_to_h(self, v):\n",
    "            w = (self.W.clone())\n",
    "            # print(\"v -> h\", v.size(), w.size())\n",
    "            \n",
    "            p_h = F.sigmoid(\n",
    "                F.linear(v, w)\n",
    "            ).to(device=device)\n",
    "\n",
    "            sample_h = self.sample_from_p(p_h)\n",
    "            return p_h, sample_h\n",
    "\n",
    "        def h_to_v(self, h):\n",
    "            w = self.W.t().clone()\n",
    "            # print(\"h -> v\", h.size(), w.size())\n",
    "\n",
    "            p_v = F.sigmoid(\n",
    "                F.linear(h, w)\n",
    "            ).to(device=device)\n",
    "\n",
    "            sample_v = self.sample_from_p(p_v)\n",
    "            return p_v, sample_v\n",
    "        \n",
    "        def forward(self, v):\n",
    "            pre_h1, h1 = self.v_to_h(v)\n",
    "            h_ = h1\n",
    "\n",
    "            for _ in range(self.k):\n",
    "                pre_v_, v_ = self.h_to_v(h_)\n",
    "                pre_h_, h_ = self.v_to_h(v_)\n",
    "            return v, v_\n",
    "        \n",
    "        def get_weight(self):\n",
    "            return self.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_step(engine, batch):\n",
    "    return batch\n",
    "\n",
    "default_model = nn.Sequential(OrderedDict([\n",
    "    ('base', nn.Linear(4, 2)),\n",
    "    ('fc', nn.Linear(2, 1))\n",
    "]))\n",
    "\n",
    "default_evaluator = Engine(eval_step)\n",
    "\n",
    "def get_acc(y_true, y_pred):\n",
    "    metric = Accuracy()\n",
    "    metric.attach(default_evaluator, \"accuracy\")\n",
    "    state = default_evaluator.run([[y_pred, y_true]])\n",
    "    return state.metrics[\"accuracy\"]\n",
    "    \n",
    "def to_categorical(y, num_classes):\n",
    "    return np.eye(num_classes, dtype='uint8')[y]\n",
    "\n",
    "def svm(x, w, b):\n",
    "    h = (w*x).sum(1) + b\n",
    "    return h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM_Model(nn.Module):\n",
    "    def __init__ (self, D_in, D_out):\n",
    "        super(SVM_Model, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, D_out, device=device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 22\n",
      "22 22\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(DB1_LIST)):\n",
    "    pickle_path = PATH + str(DB1_LIST[i]) + \".pkl\"\n",
    "    with open(pickle_path, 'rb') as f:\n",
    "        db1_sig.append(pickle.load(f)[0])\n",
    "        \n",
    "    with open(pickle_path, 'rb') as f:\n",
    "        db1_ann.append(pickle.load(f)[1])\n",
    "        \n",
    "print(len(db1_sig), len(db1_ann))\n",
    "\n",
    "temp_sg, temp_ann = list(), list()\n",
    "\n",
    "for i in range(len(db1_sig)):\n",
    "    for j in range(len(db1_sig[i])):\n",
    "        temp_sg.append(signal.resample(db1_sig[i][j], 50))\n",
    "    db1_resampled_sig.append(temp_sg)\n",
    "\n",
    "for i in range(len(db1_ann)):\n",
    "    for j in range(len(db1_ann[i])):\n",
    "        if db1_ann[i][j] == \"N\":\n",
    "            # temp_ann.append(db1_ann[i][j])\n",
    "            temp_ann.append(0)\n",
    "        \n",
    "        elif db1_ann[i][j] == \"S\" or \"V\" or \"F\":\n",
    "            # temp_ann.append(db1_ann[i][j])\n",
    "            temp_ann.append(1)\n",
    "        \n",
    "        else:\n",
    "            temp_ann.append(1)\n",
    "    \n",
    "    db1_resampled_ann.append(temp_ann)\n",
    "\n",
    "print(len(db1_resampled_sig), len(db1_resampled_ann))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RSLT]\t\t\t Export records ...\n",
      "100 100\n",
      "100 100\n"
     ]
    }
   ],
   "source": [
    "PATH = \"./svdb_pic/\"\n",
    "\n",
    "with open(PATH + 'RECORDS') as f:\n",
    "    record_lines = f.readlines()\n",
    "\n",
    "pre_records = []\n",
    "for x in record_lines:\n",
    "    pre_records.append(x.strip())\n",
    "print(\"[RSLT]\\t\\t\\t Export records ...\")\n",
    "\n",
    "for i in range(len(pre_records)):\n",
    "    pickle_path = PATH + \"svdb\" + str(pre_records[i]) + \".pkl\"\n",
    "    with open(pickle_path, 'rb') as f:\n",
    "        db1_sig.append(pickle.load(f)[0])\n",
    "        \n",
    "    with open(pickle_path, 'rb') as f:\n",
    "        db1_ann.append(pickle.load(f)[1])\n",
    "        \n",
    "print(len(db1_sig), len(db1_ann))\n",
    "\n",
    "temp_sg, temp_ann = list(), list()\n",
    "\n",
    "for i in range(len(db1_sig)):\n",
    "    for j in range(len(db1_sig[i])):\n",
    "        temp_sg.append(signal.resample(db1_sig[i][j], 50))\n",
    "    db3_resampled_sig.append(temp_sg)\n",
    "\n",
    "for i in range(len(db1_ann)):\n",
    "    for j in range(len(db1_ann[i])):\n",
    "        if db1_ann[i][j] == \"N\":\n",
    "            # temp_ann.append(db1_ann[i][j])\n",
    "            temp_ann.append(0)\n",
    "        \n",
    "        elif db1_ann[i][j] == \"S\" or \"V\" or \"F\":\n",
    "            # temp_ann.append(db1_ann[i][j])\n",
    "            temp_ann.append(1)\n",
    "        \n",
    "        else:\n",
    "            temp_ann.append(\"F\")\n",
    "    \n",
    "    db3_resampled_ann.append(temp_ann)\n",
    "\n",
    "print(len(db3_resampled_sig), len(db3_resampled_ann))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50])\n"
     ]
    }
   ],
   "source": [
    "bbrbm_first = RBM(n_vis=VISIBLE_NUM, n_hid=HIDDEN_UNITS[3], k=K_FOLD, batch=BATCH_SIZE).to(device=device)\n",
    "bbrbm_second = RBM(n_vis=VISIBLE_NUM, n_hid=HIDDEN_UNITS[4], k=K_FOLD, batch=BATCH_SIZE).to(device=device)\n",
    "bbrbm_third = RBM(n_vis=VISIBLE_NUM, n_hid=HIDDEN_UNITS[5], k=K_FOLD, batch=BATCH_SIZE).to(device=device)\n",
    "\n",
    "gbrbm_first = RBM(n_vis=VISIBLE_NUM, n_hid=HIDDEN_UNITS[0], k=K_FOLD, batch=BATCH_SIZE).to(device=device)\n",
    "gbrbm_second = RBM(n_vis=VISIBLE_NUM, n_hid=HIDDEN_UNITS[1], k=K_FOLD, batch=BATCH_SIZE).to(device=device)\n",
    "gbrbm_third = RBM(n_vis=VISIBLE_NUM, n_hid=HIDDEN_UNITS[2], k=K_FOLD, batch=BATCH_SIZE).to(device=device)\n",
    "\n",
    "first_train_op = optim.Adagrad(bbrbm_first.parameters(), LEARNING_RATE)\n",
    "second_train_op = optim.Adagrad(bbrbm_second.parameters(), LEARNING_RATE)\n",
    "third_train_op = optim.Adagrad(bbrbm_third.parameters(), LEARNING_RATE)\n",
    "\n",
    "gb_first_train_op = optim.Adagrad(gbrbm_first.parameters(), LEARNING_RATE)\n",
    "gb_second_train_op = optim.Adagrad(gbrbm_second.parameters(), LEARNING_RATE)\n",
    "gb_third_train_op = optim.Adagrad(gbrbm_third.parameters(), LEARNING_RATE)\n",
    "\n",
    "omse_loss = list()\n",
    "output_gb = list()\n",
    "best_acc = float()\n",
    "svm_best_acc = float()\n",
    "mse_loss = nn.MSELoss().to(device=device)\n",
    "        \n",
    "clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "\n",
    "gaussian_std = torch.arange(1, 0, -0.02, device=device)\n",
    "print(gaussian_std.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = DataLoader(db1_resampled_sig,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              num_workers=0,\n",
    "                              shuffle=True)\n",
    "\n",
    "print(len(train_dataloader.dataset))\n",
    "\n",
    "test_dataloader = DataLoader(db3_resampled_sig,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             num_workers=0,\n",
    "                             shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RBM START!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HILAB_~1\\AppData\\Local\\Temp/ipykernel_17500/3339944148.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data = Variable(torch.tensor(data[0].to(device), dtype=torch.float32))\n",
      "c:\\Users\\HILAB_Labtop_02\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "C:\\Users\\HILAB_~1\\AppData\\Local\\Temp/ipykernel_17500/3339944148.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data = Variable(torch.tensor(data, dtype=torch.float32))\n",
      "C:\\Users\\HILAB_~1\\AppData\\Local\\Temp/ipykernel_17500/3339944148.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data = Variable(torch.tensor(data).type(torch.float32))\n",
      "C:\\Users\\HILAB_~1\\AppData\\Local\\Temp/ipykernel_17500/3339944148.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data = Variable(torch.tensor(v3_e, dtype=torch.float32)).uniform_(0, 1)\n",
      "C:\\Users\\HILAB_~1\\AppData\\Local\\Temp/ipykernel_17500/3339944148.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data = Variable(torch.tensor(v1, dtype=torch.float32)).uniform_(0, 1)\n",
      "C:\\Users\\HILAB_~1\\AppData\\Local\\Temp/ipykernel_17500/3339944148.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data = Variable(torch.tensor(v2, dtype=torch.float32)).uniform_(0, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GB-DBN Training loss for 1th epoch 0.4599999785423279\tEstimate time : 4.377434730529785\tAcc : 72.0\t\tBest Acc : 72.0\tSVM Predict : \n",
      "GB-DBN Training loss for 2th epoch 0.3799999952316284\tEstimate time : 2.92136812210083\tAcc : 70.0\t\tBest Acc : 72.0\tSVM Predict : \n",
      "GB-DBN Training loss for 3th epoch 0.41999998688697815\tEstimate time : 3.0484375953674316\tAcc : 72.0\t\tBest Acc : 72.0\tSVM Predict : \n",
      "GB-DBN Training loss for 4th epoch 0.41999998688697815\tEstimate time : 2.9729421138763428\tAcc : 72.0\t\tBest Acc : 72.0\tSVM Predict : \n",
      "GB-DBN Training loss for 5th epoch 0.4599999785423279\tEstimate time : 2.942330837249756\tAcc : 78.0\t\tBest Acc : 78.0\tSVM Predict : \n",
      "GB-DBN Training loss for 6th epoch 0.5799999833106995\tEstimate time : 2.888166904449463\tAcc : 76.0\t\tBest Acc : 78.0\tSVM Predict : \n",
      "GB-DBN Training loss for 7th epoch 0.5999999642372131\tEstimate time : 2.90726637840271\tAcc : 62.0\t\tBest Acc : 78.0\tSVM Predict : \n",
      "GB-DBN Training loss for 8th epoch 0.5\tEstimate time : 3.1196184158325195\tAcc : 68.0\t\tBest Acc : 78.0\tSVM Predict : \n",
      "GB-DBN Training loss for 9th epoch 0.3799999952316284\tEstimate time : 2.973532199859619\tAcc : 84.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 10th epoch 0.5199999809265137\tEstimate time : 3.0696747303009033\tAcc : 74.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 11th epoch 0.5600000023841858\tEstimate time : 2.888390064239502\tAcc : 72.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 12th epoch 0.5399999618530273\tEstimate time : 2.9541263580322266\tAcc : 74.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 13th epoch 0.6200000047683716\tEstimate time : 2.8458609580993652\tAcc : 68.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 14th epoch 0.5199999809265137\tEstimate time : 3.1652541160583496\tAcc : 70.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 15th epoch 0.5999999642372131\tEstimate time : 3.0444674491882324\tAcc : 72.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 16th epoch 0.5999999642372131\tEstimate time : 2.8262808322906494\tAcc : 70.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 17th epoch 0.5600000023841858\tEstimate time : 2.8182942867279053\tAcc : 57.99999999999999\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 18th epoch 0.5399999618530273\tEstimate time : 2.887592315673828\tAcc : 68.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 19th epoch 0.5399999618530273\tEstimate time : 2.8393383026123047\tAcc : 70.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 20th epoch 0.4599999785423279\tEstimate time : 2.795447826385498\tAcc : 70.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 21th epoch 0.35999998450279236\tEstimate time : 2.788001775741577\tAcc : 60.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 22th epoch 0.3799999952316284\tEstimate time : 2.9008290767669678\tAcc : 64.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 23th epoch 0.6399999856948853\tEstimate time : 2.9047775268554688\tAcc : 66.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 24th epoch 0.4599999785423279\tEstimate time : 2.8757355213165283\tAcc : 70.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 25th epoch 0.5199999809265137\tEstimate time : 2.857534646987915\tAcc : 66.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 26th epoch 0.3799999952316284\tEstimate time : 2.8293251991271973\tAcc : 70.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 27th epoch 0.5399999618530273\tEstimate time : 3.046931505203247\tAcc : 68.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 28th epoch 0.5\tEstimate time : 3.021153450012207\tAcc : 76.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 29th epoch 0.47999998927116394\tEstimate time : 2.96889066696167\tAcc : 74.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 30th epoch 0.4599999785423279\tEstimate time : 2.806420087814331\tAcc : 72.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 31th epoch 0.41999998688697815\tEstimate time : 2.7906711101531982\tAcc : 64.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 32th epoch 0.5600000023841858\tEstimate time : 2.5808403491973877\tAcc : 64.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 33th epoch 0.3999999761581421\tEstimate time : 2.7531280517578125\tAcc : 64.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 34th epoch 0.4599999785423279\tEstimate time : 2.7928240299224854\tAcc : 84.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 35th epoch 0.47999998927116394\tEstimate time : 2.5462307929992676\tAcc : 62.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 36th epoch 0.6599999666213989\tEstimate time : 2.7354352474212646\tAcc : 66.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 37th epoch 0.41999998688697815\tEstimate time : 2.768307685852051\tAcc : 74.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 38th epoch 0.5\tEstimate time : 2.7601897716522217\tAcc : 72.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 39th epoch 0.5399999618530273\tEstimate time : 2.7619221210479736\tAcc : 72.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 40th epoch 0.4399999976158142\tEstimate time : 2.7657549381256104\tAcc : 78.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 41th epoch 0.5399999618530273\tEstimate time : 2.7653675079345703\tAcc : 80.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 42th epoch 0.4599999785423279\tEstimate time : 2.7566134929656982\tAcc : 70.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 43th epoch 0.5799999833106995\tEstimate time : 2.7702062129974365\tAcc : 68.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 44th epoch 0.41999998688697815\tEstimate time : 2.7561988830566406\tAcc : 76.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 45th epoch 0.4599999785423279\tEstimate time : 2.758072853088379\tAcc : 68.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 46th epoch 0.5399999618530273\tEstimate time : 2.7456369400024414\tAcc : 78.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 47th epoch 0.4599999785423279\tEstimate time : 2.7491064071655273\tAcc : 64.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 48th epoch 0.5799999833106995\tEstimate time : 2.750241994857788\tAcc : 68.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 49th epoch 0.4399999976158142\tEstimate time : 2.757327079772949\tAcc : 76.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 50th epoch 0.5\tEstimate time : 2.7725136280059814\tAcc : 72.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 51th epoch 0.4599999785423279\tEstimate time : 2.7896993160247803\tAcc : 70.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 52th epoch 0.5\tEstimate time : 2.773948907852173\tAcc : 70.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 53th epoch 0.5199999809265137\tEstimate time : 2.7653543949127197\tAcc : 72.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 54th epoch 0.4599999785423279\tEstimate time : 2.751021385192871\tAcc : 62.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 55th epoch 0.47999998927116394\tEstimate time : 2.75372052192688\tAcc : 64.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 56th epoch 0.4399999976158142\tEstimate time : 2.747931957244873\tAcc : 72.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 57th epoch 0.5\tEstimate time : 2.75122332572937\tAcc : 66.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 58th epoch 0.5199999809265137\tEstimate time : 2.7754459381103516\tAcc : 62.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 59th epoch 0.3999999761581421\tEstimate time : 2.7646567821502686\tAcc : 64.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 60th epoch 0.3799999952316284\tEstimate time : 2.775002956390381\tAcc : 74.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 61th epoch 0.3799999952316284\tEstimate time : 2.754826068878174\tAcc : 74.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 62th epoch 0.5199999809265137\tEstimate time : 2.7763657569885254\tAcc : 76.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 63th epoch 0.5399999618530273\tEstimate time : 2.748725414276123\tAcc : 64.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 64th epoch 0.3999999761581421\tEstimate time : 2.760382652282715\tAcc : 74.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 65th epoch 0.4399999976158142\tEstimate time : 2.7572169303894043\tAcc : 76.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 66th epoch 0.5799999833106995\tEstimate time : 2.7491631507873535\tAcc : 54.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 67th epoch 0.3799999952316284\tEstimate time : 2.759406805038452\tAcc : 78.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 68th epoch 0.5799999833106995\tEstimate time : 2.7603867053985596\tAcc : 76.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 69th epoch 0.5199999809265137\tEstimate time : 2.7547385692596436\tAcc : 66.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 70th epoch 0.5399999618530273\tEstimate time : 2.771871328353882\tAcc : 78.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 71th epoch 0.4399999976158142\tEstimate time : 2.7672111988067627\tAcc : 68.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 72th epoch 0.5\tEstimate time : 2.7692627906799316\tAcc : 64.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 73th epoch 0.47999998927116394\tEstimate time : 2.788681745529175\tAcc : 66.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 74th epoch 0.5199999809265137\tEstimate time : 2.768092632293701\tAcc : 64.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 75th epoch 0.5799999833106995\tEstimate time : 2.7742300033569336\tAcc : 52.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 76th epoch 0.5\tEstimate time : 2.764960765838623\tAcc : 56.00000000000001\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 77th epoch 0.41999998688697815\tEstimate time : 2.7662980556488037\tAcc : 80.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 78th epoch 0.5600000023841858\tEstimate time : 2.7934484481811523\tAcc : 64.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 79th epoch 0.4399999976158142\tEstimate time : 2.762491464614868\tAcc : 78.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 80th epoch 0.47999998927116394\tEstimate time : 2.793300151824951\tAcc : 74.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 81th epoch 0.5399999618530273\tEstimate time : 2.7706105709075928\tAcc : 70.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 82th epoch 0.5600000023841858\tEstimate time : 2.812299966812134\tAcc : 76.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 83th epoch 0.5399999618530273\tEstimate time : 2.7817490100860596\tAcc : 66.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 84th epoch 0.4399999976158142\tEstimate time : 2.8015778064727783\tAcc : 82.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 85th epoch 0.4399999976158142\tEstimate time : 2.774533748626709\tAcc : 66.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 86th epoch 0.5799999833106995\tEstimate time : 2.770829439163208\tAcc : 64.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 87th epoch 0.6399999856948853\tEstimate time : 2.77097487449646\tAcc : 80.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 88th epoch 0.47999998927116394\tEstimate time : 2.7646267414093018\tAcc : 57.99999999999999\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 89th epoch 0.41999998688697815\tEstimate time : 2.7815351486206055\tAcc : 70.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 90th epoch 0.5199999809265137\tEstimate time : 2.768784761428833\tAcc : 70.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 91th epoch 0.5\tEstimate time : 2.7831966876983643\tAcc : 72.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 92th epoch 0.5399999618530273\tEstimate time : 2.791057586669922\tAcc : 70.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 93th epoch 0.47999998927116394\tEstimate time : 2.7791390419006348\tAcc : 82.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 94th epoch 0.41999998688697815\tEstimate time : 2.921083688735962\tAcc : 68.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 95th epoch 0.4399999976158142\tEstimate time : 2.8325178623199463\tAcc : 62.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 96th epoch 0.4599999785423279\tEstimate time : 2.801013469696045\tAcc : 74.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 97th epoch 0.5199999809265137\tEstimate time : 2.7900185585021973\tAcc : 74.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 98th epoch 0.35999998450279236\tEstimate time : 2.790998935699463\tAcc : 64.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 99th epoch 0.3999999761581421\tEstimate time : 2.7792086601257324\tAcc : 72.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 100th epoch 0.47999998927116394\tEstimate time : 2.77056622505188\tAcc : 78.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 101th epoch 0.5399999618530273\tEstimate time : 2.792391061782837\tAcc : 60.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 102th epoch 0.5199999809265137\tEstimate time : 2.8352251052856445\tAcc : 72.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 103th epoch 0.5199999809265137\tEstimate time : 2.7886617183685303\tAcc : 66.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 104th epoch 0.5\tEstimate time : 2.785154342651367\tAcc : 68.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 105th epoch 0.5600000023841858\tEstimate time : 2.775599718093872\tAcc : 64.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 106th epoch 0.5399999618530273\tEstimate time : 2.782003879547119\tAcc : 72.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 107th epoch 0.5199999809265137\tEstimate time : 2.796485662460327\tAcc : 82.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 108th epoch 0.5199999809265137\tEstimate time : 2.798001289367676\tAcc : 68.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 109th epoch 0.47999998927116394\tEstimate time : 2.788430690765381\tAcc : 52.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 110th epoch 0.3999999761581421\tEstimate time : 2.782064914703369\tAcc : 68.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 111th epoch 0.5199999809265137\tEstimate time : 2.787003755569458\tAcc : 70.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 112th epoch 0.5399999618530273\tEstimate time : 2.7872402667999268\tAcc : 68.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 113th epoch 0.5799999833106995\tEstimate time : 2.7829947471618652\tAcc : 72.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 114th epoch 0.3400000035762787\tEstimate time : 2.7906336784362793\tAcc : 74.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 115th epoch 0.5\tEstimate time : 2.7848305702209473\tAcc : 66.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 116th epoch 0.5999999642372131\tEstimate time : 2.7663161754608154\tAcc : 68.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 117th epoch 0.7199999690055847\tEstimate time : 2.796602249145508\tAcc : 72.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 118th epoch 0.5399999618530273\tEstimate time : 2.7929985523223877\tAcc : 66.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 119th epoch 0.47999998927116394\tEstimate time : 2.791194438934326\tAcc : 68.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 120th epoch 0.4599999785423279\tEstimate time : 2.806209087371826\tAcc : 66.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 121th epoch 0.5\tEstimate time : 2.7896130084991455\tAcc : 68.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 122th epoch 0.5999999642372131\tEstimate time : 2.780994176864624\tAcc : 62.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 123th epoch 0.3799999952316284\tEstimate time : 2.797304630279541\tAcc : 54.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 124th epoch 0.5199999809265137\tEstimate time : 2.7934842109680176\tAcc : 74.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 125th epoch 0.47999998927116394\tEstimate time : 2.8111414909362793\tAcc : 60.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 126th epoch 0.5199999809265137\tEstimate time : 2.7842154502868652\tAcc : 66.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 127th epoch 0.4599999785423279\tEstimate time : 2.833997964859009\tAcc : 74.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 128th epoch 0.5600000023841858\tEstimate time : 2.8184781074523926\tAcc : 78.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 129th epoch 0.5199999809265137\tEstimate time : 2.819661855697632\tAcc : 78.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 130th epoch 0.6399999856948853\tEstimate time : 2.811283588409424\tAcc : 68.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 131th epoch 0.5\tEstimate time : 2.796344757080078\tAcc : 74.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 132th epoch 0.47999998927116394\tEstimate time : 2.8149983882904053\tAcc : 64.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 133th epoch 0.5999999642372131\tEstimate time : 2.7969818115234375\tAcc : 74.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 134th epoch 0.41999998688697815\tEstimate time : 2.808000326156616\tAcc : 57.99999999999999\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 135th epoch 0.5799999833106995\tEstimate time : 2.7967514991760254\tAcc : 48.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 136th epoch 0.5999999642372131\tEstimate time : 2.8073232173919678\tAcc : 68.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 137th epoch 0.41999998688697815\tEstimate time : 2.8010079860687256\tAcc : 66.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 138th epoch 0.4399999976158142\tEstimate time : 2.8099942207336426\tAcc : 64.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 139th epoch 0.3999999761581421\tEstimate time : 2.8039956092834473\tAcc : 74.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 140th epoch 0.5399999618530273\tEstimate time : 2.8079946041107178\tAcc : 78.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 141th epoch 0.5600000023841858\tEstimate time : 2.809080123901367\tAcc : 66.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 142th epoch 0.5600000023841858\tEstimate time : 2.8073577880859375\tAcc : 64.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 143th epoch 0.5199999809265137\tEstimate time : 2.8190040588378906\tAcc : 70.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 144th epoch 0.5199999809265137\tEstimate time : 2.8220367431640625\tAcc : 57.99999999999999\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 145th epoch 0.5999999642372131\tEstimate time : 2.8089144229888916\tAcc : 66.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 146th epoch 0.3999999761581421\tEstimate time : 2.83422589302063\tAcc : 66.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 147th epoch 0.47999998927116394\tEstimate time : 2.8239431381225586\tAcc : 64.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 148th epoch 0.47999998927116394\tEstimate time : 2.8226513862609863\tAcc : 74.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 149th epoch 0.4599999785423279\tEstimate time : 2.8180646896362305\tAcc : 76.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 150th epoch 0.5\tEstimate time : 2.826539993286133\tAcc : 78.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 151th epoch 0.5600000023841858\tEstimate time : 2.821298599243164\tAcc : 62.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 152th epoch 0.6200000047683716\tEstimate time : 2.8244402408599854\tAcc : 68.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 153th epoch 0.5999999642372131\tEstimate time : 2.839909076690674\tAcc : 68.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 154th epoch 0.5399999618530273\tEstimate time : 2.8258421421051025\tAcc : 72.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 155th epoch 0.47999998927116394\tEstimate time : 2.814807415008545\tAcc : 74.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 156th epoch 0.5600000023841858\tEstimate time : 2.8136463165283203\tAcc : 78.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 157th epoch 0.4399999976158142\tEstimate time : 2.880983352661133\tAcc : 66.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 158th epoch 0.5199999809265137\tEstimate time : 2.8111655712127686\tAcc : 70.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 159th epoch 0.4399999976158142\tEstimate time : 2.8360583782196045\tAcc : 64.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 160th epoch 0.5600000023841858\tEstimate time : 2.824004650115967\tAcc : 57.99999999999999\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 161th epoch 0.5199999809265137\tEstimate time : 2.826242208480835\tAcc : 70.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 162th epoch 0.41999998688697815\tEstimate time : 2.8255789279937744\tAcc : 72.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 163th epoch 0.5600000023841858\tEstimate time : 2.8266594409942627\tAcc : 76.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 164th epoch 0.3999999761581421\tEstimate time : 2.850497245788574\tAcc : 82.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 165th epoch 0.47999998927116394\tEstimate time : 2.8159167766571045\tAcc : 64.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 166th epoch 0.5\tEstimate time : 2.8291003704071045\tAcc : 76.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 167th epoch 0.5999999642372131\tEstimate time : 2.8218464851379395\tAcc : 62.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 168th epoch 0.47999998927116394\tEstimate time : 2.8138813972473145\tAcc : 74.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 169th epoch 0.4599999785423279\tEstimate time : 2.8469982147216797\tAcc : 66.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 170th epoch 0.41999998688697815\tEstimate time : 2.8437085151672363\tAcc : 70.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 171th epoch 0.47999998927116394\tEstimate time : 2.8290042877197266\tAcc : 64.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 172th epoch 0.3999999761581421\tEstimate time : 2.828275680541992\tAcc : 60.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 173th epoch 0.5199999809265137\tEstimate time : 2.8342580795288086\tAcc : 68.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 174th epoch 0.4399999976158142\tEstimate time : 2.838770866394043\tAcc : 48.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 175th epoch 0.5199999809265137\tEstimate time : 2.8552944660186768\tAcc : 80.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 176th epoch 0.4399999976158142\tEstimate time : 2.8380041122436523\tAcc : 66.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 177th epoch 0.5600000023841858\tEstimate time : 2.8345909118652344\tAcc : 60.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 178th epoch 0.5\tEstimate time : 2.8397693634033203\tAcc : 74.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 179th epoch 0.5\tEstimate time : 2.844878911972046\tAcc : 68.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 180th epoch 0.5199999809265137\tEstimate time : 2.8366341590881348\tAcc : 66.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 181th epoch 0.4399999976158142\tEstimate time : 2.830998659133911\tAcc : 80.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 182th epoch 0.5399999618530273\tEstimate time : 2.833789348602295\tAcc : 60.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 183th epoch 0.3799999952316284\tEstimate time : 2.830390691757202\tAcc : 72.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 184th epoch 0.47999998927116394\tEstimate time : 2.836268424987793\tAcc : 72.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 185th epoch 0.5600000023841858\tEstimate time : 2.8329977989196777\tAcc : 60.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 186th epoch 0.41999998688697815\tEstimate time : 2.8261895179748535\tAcc : 76.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 187th epoch 0.35999998450279236\tEstimate time : 2.8476428985595703\tAcc : 72.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 188th epoch 0.4599999785423279\tEstimate time : 2.8383114337921143\tAcc : 74.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 189th epoch 0.3799999952316284\tEstimate time : 2.89709734916687\tAcc : 72.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 190th epoch 0.4399999976158142\tEstimate time : 2.937058925628662\tAcc : 62.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 191th epoch 0.4399999976158142\tEstimate time : 2.867612838745117\tAcc : 68.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 192th epoch 0.41999998688697815\tEstimate time : 2.8519980907440186\tAcc : 82.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 193th epoch 0.3199999928474426\tEstimate time : 2.837123155593872\tAcc : 60.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 194th epoch 0.41999998688697815\tEstimate time : 2.845893621444702\tAcc : 68.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 195th epoch 0.5399999618530273\tEstimate time : 2.855004072189331\tAcc : 78.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 196th epoch 0.5\tEstimate time : 2.8426432609558105\tAcc : 72.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 197th epoch 0.5199999809265137\tEstimate time : 2.851994037628174\tAcc : 70.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 198th epoch 0.5199999809265137\tEstimate time : 2.89847731590271\tAcc : 62.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 199th epoch 0.5600000023841858\tEstimate time : 2.9936940670013428\tAcc : 76.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "GB-DBN Training loss for 200th epoch 0.5399999618530273\tEstimate time : 2.8486392498016357\tAcc : 68.0\t\tBest Acc : 84.0\tSVM Predict : \n",
      "torch.Size([50])\n"
     ]
    }
   ],
   "source": [
    "'''BBRBM Train Part'''\n",
    "\n",
    "loss_ = []\n",
    "output_bb = []\n",
    "temp_v1 = []\n",
    "model_path_str = str()\n",
    "\n",
    "print(\"RBM START!\")\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    tmp_acc = float()\n",
    "    run_acc = float()\n",
    "    start = time.time()\n",
    "\n",
    "    for i, (data) in enumerate(train_dataloader):\n",
    "        data = Variable(torch.tensor(data[0].to(device), dtype=torch.float32))\n",
    "        sample_data = torch.normal(mean=data, std=gaussian_std).view(-1, 50).to(device=device)\n",
    "        \n",
    "        # tensor binary\n",
    "        vog_first, v1 = gbrbm_first(sample_data)\n",
    "        temp_v1.append(v1)\n",
    "        \n",
    "        omse_loss = mse_loss(vog_first, v1)\n",
    "        \n",
    "        gb_first_train_op.zero_grad()\n",
    "        gb_first_train_op.step()\n",
    "        omse_loss.backward()\n",
    "\n",
    "    for _, (data) in enumerate(v1):         \n",
    "        data = Variable(torch.tensor(data, dtype=torch.float32))\n",
    "        sample_data = torch.normal(mean=data, std=gaussian_std).to(device=device)\n",
    "\n",
    "        # tensor binary\n",
    "        vog_second, v2 = gbrbm_second(sample_data)\n",
    "        omse_loss = mse_loss(vog_second, v2)\n",
    "\n",
    "        gb_second_train_op.zero_grad()\n",
    "        omse_loss.backward()\n",
    "        gb_second_train_op.step()\n",
    "\n",
    "    for _, (data) in enumerate(v2):\n",
    "        data = Variable(torch.tensor(data).type(torch.float32))\n",
    "        sample_data = torch.normal(mean=data, std=gaussian_std).to(device=device)\n",
    "\n",
    "        vog_third, v3_e = gbrbm_third(sample_data)\n",
    "        omse_loss = mse_loss(vog_third, v3_e)\n",
    "        \n",
    "        gb_third_train_op.zero_grad()\n",
    "        gb_third_train_op.step()\n",
    "        omse_loss.backward()\n",
    "\n",
    "        '''First bbrbm'''\n",
    "        ''' ISSUE PART '''\n",
    "        data = Variable(torch.tensor(v3_e, dtype=torch.float32)).uniform_(0, 1)\n",
    "        sample_data = torch.bernoulli(data).to(device=device)\n",
    "        \n",
    "        # tensor binary\n",
    "        fvog_first, v1 = bbrbm_first(sample_data)\n",
    "        omse_loss = mse_loss(fvog_first, v1)\n",
    "\n",
    "        first_train_op.zero_grad()\n",
    "        first_train_op.step()\n",
    "        omse_loss.backward()\n",
    "\n",
    "        #  **** Second BBRBM **** #\n",
    "        data = Variable(torch.tensor(v1, dtype=torch.float32)).uniform_(0, 1)\n",
    "        sample_data = torch.bernoulli(data).to(device=device)\n",
    "\n",
    "        # tensor binary\n",
    "        vog_second, v2 = bbrbm_second(sample_data)\n",
    "        omse_loss = mse_loss(vog_second, v2)\n",
    "\n",
    "        second_train_op.zero_grad()\n",
    "        omse_loss.backward()\n",
    "        second_train_op.step()\n",
    "\n",
    "        # **** THIRD BBRBM **** #\n",
    "        data = Variable(torch.tensor(v2, dtype=torch.float32)).uniform_(0, 1)\n",
    "\n",
    "        sample_data = torch.bernoulli(data).to(device=device)\n",
    "\n",
    "        vog_third, v3 = bbrbm_third(sample_data)\n",
    "        omse_loss = mse_loss(vog_third, v3)\n",
    "        \n",
    "        third_train_op.zero_grad()\n",
    "        omse_loss.backward()\n",
    "        third_train_op.step()\n",
    "\n",
    "        run_acc += (sample_data == v3).sum().item()\n",
    "\n",
    "        # for i in range(len(v3)):\n",
    "        #     x = (v3.cpu().detach()).numpy()\n",
    "        #     y = np.array(db1_resampled_ann)\n",
    "            \n",
    "        #     clf.fit(x, y)\n",
    "        #     svm_pre = clf.predict(x, y)\n",
    "            \n",
    "    svm_pre = \"?\"\n",
    "    acc_v = (vog_third >= 0).float()\n",
    "    acc = get_acc(\n",
    "        acc_v, v3\n",
    "    ) * 100\n",
    "    \n",
    "    if acc > best_acc:\n",
    "        best_acc = acc    \n",
    "        \n",
    "        path = \"./say_cheese/ahh_saveMode_through_\" + str(epoch) + \"_\" + str(acc) + \"GBRBM.pth\"\n",
    "        model_path_str = path\n",
    "        torch.save(gbrbm_third.state_dict(), path)\n",
    "    output_gb.append(v3)\n",
    "\n",
    "    print(\"GB-DBN Training loss for {0}th epoch {1}\\tEstimate time : {2}\\tAcc : {3}\\t\\tBest Acc : {4}\\tSVM Predict : \".format(epoch + 1, omse_loss, time.time() - start, acc, best_acc, svm_pre))\n",
    "    \n",
    "    gc.collect()\n",
    "print(v3.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "predict_proba is not available when  probability=False",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\HILAB_~1\\AppData\\Local\\Temp/ipykernel_17500/2943476737.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mAL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mActiveLearner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'entropy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mAL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdb1_resampled_sig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_queries\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\HILAB_Labtop_02\\Desktop\\insung\\ecg-dbn\\restart\\active_learning\\active_learning.py\u001b[0m in \u001b[0;36mrank\u001b[1;34m(self, clf, X_unlabeled, num_queries)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrategy\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_uncertainty_sampling_frameworks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m             \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__uncertainty_sampling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_unlabeled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrategy\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_query_by_committee_frameworks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\HILAB_Labtop_02\\Desktop\\insung\\ecg-dbn\\restart\\active_learning\\active_learning.py\u001b[0m in \u001b[0;36m__uncertainty_sampling\u001b[1;34m(self, clf, X_unlabeled)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__uncertainty_sampling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_unlabeled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         \u001b[0mprobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_unlabeled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrategy\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'least_confident'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\HILAB_Labtop_02\\anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\u001b[0m in \u001b[0;36m__get__\u001b[1;34m(self, obj, type)\u001b[0m\n\u001b[0;32m    112\u001b[0m                     \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m                     \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattribute_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\HILAB_Labtop_02\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    664\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    665\u001b[0m         \"\"\"\n\u001b[1;32m--> 666\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    667\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predict_proba\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\HILAB_Labtop_02\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36m_check_proba\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    632\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobability\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 633\u001b[1;33m             raise AttributeError(\"predict_proba is not available when \"\n\u001b[0m\u001b[0;32m    634\u001b[0m                                  \" probability=False\")\n\u001b[0;32m    635\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_impl\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'c_svc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'nu_svc'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: predict_proba is not available when  probability=False"
     ]
    }
   ],
   "source": [
    "clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "\n",
    "AL = ActiveLearner(strategy='entropy')\n",
    "AL.rank(clf, db1_resampled_sig, num_queries=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6c3cabcf2f29820bdd7faae982b59d335e0d215fb5382d93f3312fa3292e9b7f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
