{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Pre-processing for make clean\n",
      "2022-05-14 13:03:04.159846 model.py code start\n"
     ]
    }
   ],
   "source": [
    "import torch.distributions.distribution as D\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import data.read_samples as rs\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import datetime\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "\n",
    "print(datetime.datetime.now(), \"model.py code start\")\n",
    "\n",
    "BATCH_SIZE = 10\n",
    "EPOCH = 90\n",
    "LEARNING_RATE = 0.2\n",
    "ANNEALING_RATE = 0.999\n",
    "VISIBLE_UNITS = [180, 200, 250]\n",
    "HIDDEN_UNITS = [80, 100, 120]\n",
    "K_FOLD = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 3080 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "print(torch.cuda.get_device_name(device))\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBM(nn.Module): \n",
    "    with torch.cuda.device(0):\n",
    "        def __init__(self, n_vis, n_hid, k, batch):\n",
    "            super(RBM, self).__init__()\n",
    "            self.W      = nn.Parameter(torch.randn(1, batch, device=device) * 1e-2)\n",
    "            self.n_vis  = n_vis\n",
    "            self.n_hid  = n_hid\n",
    "            self.k      = k\n",
    "            self.batch  = batch\n",
    "            self.v_bias = nn.Parameter(torch.zeros(batch, device=device))\n",
    "            self.h_bias = nn.Parameter(torch.zeros(batch, device=device))\n",
    "        \n",
    "        def sample_from_p(self, p):\n",
    "            return F.relu(\n",
    "                torch.sign(\n",
    "                    p - Variable(torch.randn(p.size(), device=device))\n",
    "                )\n",
    "            ).to(device=device)\n",
    "\n",
    "        ''' ISSUE PART '''\n",
    "        def v_to_h(self, v):\n",
    "            w = (self.W.clone())\n",
    "\n",
    "            p_h = F.sigmoid(\n",
    "                F.linear(v, w)\n",
    "            ).to(device=device)\n",
    "\n",
    "            sample_h = self.sample_from_p(p_h)\n",
    "            return p_h, sample_h\n",
    "\n",
    "        def h_to_v(self, h):\n",
    "            w = self.W.t().clone()\n",
    "\n",
    "            p_v = F.sigmoid(\n",
    "                F.linear(h, w).to(device=device)\n",
    "            ).to(device=device)\n",
    "\n",
    "            sample_v = self.sample_from_p(p_v)\n",
    "            return p_v, sample_v\n",
    "        \n",
    "        def forward(self, v):\n",
    "            pre_h1, h1 = self.v_to_h(v)\n",
    "            h_ = h1\n",
    "\n",
    "            for _ in range(self.k):\n",
    "                pre_v_, v_ = self.h_to_v(h_)\n",
    "                pre_h_, h_ = self.v_to_h(v_)\n",
    "            return v, v_\n",
    "        \n",
    "        def get_weight(self):\n",
    "            return self.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MODL] Model main code is starting....\n",
      "[INFO] Read train data, cross-vaildation data and test data from median filtering code\n",
      "[INFO] Read records file from  ./data/db1/\n",
      "[RSLT]\t\t\t Export records ...\n",
      "\t\t ['100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '111', '112', '113', '114', '115', '116', '117', '118', '119', '121', '122', '123', '124']\n",
      "[INFO]./rdsamp commending start\n",
      "[IWIP]\t\trdsamp Converting 100\n",
      "[IWIP]\t\trdsamp Converting 101\n",
      "[IWIP]\t\trdsamp Converting 102\n",
      "[IWIP]\t\trdsamp Converting 103\n",
      "[IWIP]\t\trdsamp Converting 104\n",
      "[IWIP]\t\trdsamp Converting 105\n",
      "[IWIP]\t\trdsamp Converting 106\n",
      "[IWIP]\t\trdsamp Converting 107\n",
      "[IWIP]\t\trdsamp Converting 108\n",
      "[IWIP]\t\trdsamp Converting 109\n",
      "[IWIP]\t\trdsamp Converting 111\n",
      "[IWIP]\t\trdsamp Converting 112\n",
      "[IWIP]\t\trdsamp Converting 113\n",
      "[IWIP]\t\trdsamp Converting 114\n",
      "[IWIP]\t\trdsamp Converting 115\n",
      "[IWIP]\t\trdsamp Converting 116\n",
      "[IWIP]\t\trdsamp Converting 117\n",
      "[IWIP]\t\trdsamp Converting 118\n",
      "[IWIP]\t\trdsamp Converting 119\n",
      "[IWIP]\t\trdsamp Converting 121\n",
      "[IWIP]\t\trdsamp Converting 122\n",
      "[IWIP]\t\trdsamp Converting 123\n",
      "[IWIP]\t\trdsamp Converting 124\n",
      "[INFO] Read records file from  ./data/db2/\n",
      "[RSLT]\t\t\t Export records ...\n",
      "\t\t ['200', '201', '202', '203', '205', '207', '208', '209', '210', '212', '213', '214', '215', '217', '219', '220', '221', '222', '223', '228', '230', '231', '232', '233', '234']\n",
      "[INFO]./rdsamp commending start\n",
      "[IWIP]\t\trdsamp Converting 200 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 201 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 202 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 203 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 205 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 207 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 208 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 209 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 210 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 212 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 213 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 214 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 215 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 217 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 219 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 220 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 221 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 222 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 223 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 228 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 230 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 231 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 232 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 233 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 234 ./data/db2/\n",
      "[INFO] Read records file from  ./data/db3/svdb/\n",
      "[RSLT]\t\t\t Export records ...\n",
      "\t\t ['800', '801', '802', '803', '804', '805', '806', '807', '808', '809', '810', '811', '812', '820', '821', '822', '823', '824', '825', '826', '827', '828', '829', '840', '841', '842', '843', '844', '845', '846', '847', '848', '849', '850', '851', '852', '853', '854', '855', '856', '857', '858', '859', '860', '861', '862', '863', '864', '865', '866', '867', '868', '869', '870', '871', '872', '873', '874', '875', '876', '877', '878', '879', '880', '881', '882', '883', '884', '885', '886', '887', '888', '889', '890', '891', '892', '893', '894']\n",
      "[INFO]./rdsamp commending start\n",
      "[IWIP]\t\trdsamp Converting 800 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 801 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 802 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 803 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 804 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 805 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 806 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 807 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 808 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 809 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 810 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 811 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 812 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 820 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 821 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 822 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 823 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 824 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 825 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 826 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 827 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 828 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 829 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 840 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 841 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 842 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 843 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 844 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 845 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 846 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 847 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 848 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 849 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 850 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 851 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 852 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 853 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 854 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 855 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 856 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 857 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 858 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 859 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 860 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 861 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 862 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 863 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 864 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 865 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 866 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 867 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 868 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 869 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 870 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 871 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 872 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 873 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 874 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 875 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 876 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 877 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 878 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 879 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 880 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 881 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 882 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 883 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 884 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 885 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 886 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 887 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 888 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 889 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 890 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 891 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 892 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 893 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 894 ./data/db3/svdb/\n",
      "[INFO] DB1 Filtering...\n",
      "[INFO] DB2 Filtering...\n",
      "[INFO] DB3 Filtering...\n",
      "DB1 butter size : 23, DB1 Anno size : 23\n",
      " DB2 butter size : 25, DB2 Anno size : 25\n",
      " DB3 butter size : 78, DB3 Anno size : 78\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"[MODL] Model main code is starting....\")\n",
    "\n",
    "print(\"[INFO] Read train data, cross-vaildation data and test data from median filtering code\")\n",
    "db1_sig, db1_label, db2_sig, db2_label, db3_sig, db3_label = rs.return_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(db1_sig + db2_sig,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              num_workers=0, \n",
    "                              collate_fn=lambda x: x,\n",
    "                              shuffle=True)\n",
    "                              \n",
    "test_dataloader = DataLoader(db2_sig + db3_sig,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             num_workers=0, \n",
    "                             collate_fn=lambda x: x,\n",
    "                             shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbm_first = RBM(n_vis=VISIBLE_UNITS[0], n_hid=HIDDEN_UNITS[0], k=K_FOLD, batch=BATCH_SIZE).to(device=device)\n",
    "rbm_second = RBM(n_vis=VISIBLE_UNITS[1], n_hid=HIDDEN_UNITS[1], k=K_FOLD, batch=BATCH_SIZE).to(device=device)\n",
    "rbm_third = RBM(n_vis=VISIBLE_UNITS[2], n_hid=HIDDEN_UNITS[2], k=K_FOLD, batch=BATCH_SIZE).to(device=device)\n",
    "\n",
    "\n",
    "first_train_op = optim.Adagrad(rbm_first.parameters(), 0.1)\n",
    "second_train_op = optim.Adagrad(rbm_second.parameters(), 0.1)\n",
    "third_train_op = optim.Adagrad(rbm_third.parameters(), 0.1)\n",
    "\n",
    "gb_first_train_op = optim.Adagrad(rbm_first.parameters(), 0.1)\n",
    "gb_second_train_op = optim.Adagrad(rbm_second.parameters(), 0.1)\n",
    "gb_third_train_op = optim.Adagrad(rbm_third.parameters(), 0.1)\n",
    "\n",
    "output_from_first = list()\n",
    "output_from_second = list()\n",
    "output_from_third = list()\n",
    "\n",
    "omse_loss = list()\n",
    "mse_loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HILAB_~1\\AppData\\Local\\Temp/ipykernel_6020/1758822892.py:9: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:210.)\n",
      "  torch.tensor(data, dtype=torch.float32)\n",
      "c:\\Users\\HILAB_Labtop_02\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1ST BBrbm_first Training loss for 0 epoch 0.5001083016395569\tEstimate time : 8.059645891189575\n",
      "1ST BBrbm_first Training loss for 1 epoch 0.5001542568206787\tEstimate time : 6.599932909011841\n",
      "1ST BBrbm_first Training loss for 2 epoch 0.49979662895202637\tEstimate time : 7.953039169311523\n",
      "1ST BBrbm_first Training loss for 3 epoch 0.5000635385513306\tEstimate time : 7.329309940338135\n",
      "1ST BBrbm_first Training loss for 4 epoch 0.5002885460853577\tEstimate time : 6.155386447906494\n",
      "1ST BBrbm_first Training loss for 5 epoch 0.4999956786632538\tEstimate time : 6.170346021652222\n",
      "1ST BBrbm_first Training loss for 6 epoch 0.5001070499420166\tEstimate time : 6.150375604629517\n",
      "1ST BBrbm_first Training loss for 7 epoch 0.5000065565109253\tEstimate time : 7.198919296264648\n",
      "1ST BBrbm_first Training loss for 8 epoch 0.4998948276042938\tEstimate time : 6.718794107437134\n",
      "1ST BBrbm_first Training loss for 9 epoch 0.49986395239830017\tEstimate time : 6.217736721038818\n",
      "1ST BBrbm_first Training loss for 10 epoch 0.4998859763145447\tEstimate time : 6.214132785797119\n",
      "1ST BBrbm_first Training loss for 11 epoch 0.49995672702789307\tEstimate time : 111.78174924850464\n",
      "1ST BBrbm_first Training loss for 12 epoch 0.4999150037765503\tEstimate time : 6.7949912548065186\n",
      "1ST BBrbm_first Training loss for 13 epoch 0.4998006820678711\tEstimate time : 5.605436325073242\n",
      "1ST BBrbm_first Training loss for 14 epoch 0.4999496340751648\tEstimate time : 5.485515117645264\n",
      "1ST BBrbm_first Training loss for 15 epoch 0.5001080632209778\tEstimate time : 4.977218151092529\n",
      "1ST BBrbm_first Training loss for 16 epoch 0.5001028180122375\tEstimate time : 6.190812826156616\n",
      "1ST BBrbm_first Training loss for 17 epoch 0.500058650970459\tEstimate time : 5.628371477127075\n",
      "1ST BBrbm_first Training loss for 18 epoch 0.5000053644180298\tEstimate time : 5.536784410476685\n",
      "1ST BBrbm_first Training loss for 19 epoch 0.5002762675285339\tEstimate time : 5.332586050033569\n",
      "1ST BBrbm_first Training loss for 20 epoch 0.4999644160270691\tEstimate time : 5.8952531814575195\n",
      "1ST BBrbm_first Training loss for 21 epoch 0.499965101480484\tEstimate time : 6.128360986709595\n",
      "1ST BBrbm_first Training loss for 22 epoch 0.4998877942562103\tEstimate time : 4.8150107860565186\n",
      "1ST BBrbm_first Training loss for 23 epoch 0.5000187754631042\tEstimate time : 5.721954345703125\n",
      "1ST BBrbm_first Training loss for 24 epoch 0.5000336766242981\tEstimate time : 4.6646435260772705\n",
      "1ST BBrbm_first Training loss for 25 epoch 0.5002191662788391\tEstimate time : 4.9866721630096436\n",
      "1ST BBrbm_first Training loss for 26 epoch 0.5002055764198303\tEstimate time : 333.83218812942505\n",
      "1ST BBrbm_first Training loss for 27 epoch 0.5000816583633423\tEstimate time : 20.82475209236145\n",
      "1ST BBrbm_first Training loss for 28 epoch 0.4998582899570465\tEstimate time : 10.397649049758911\n",
      "1ST BBrbm_first Training loss for 29 epoch 0.4998454749584198\tEstimate time : 7.193264007568359\n",
      "1ST BBrbm_first Training loss for 30 epoch 0.4998877942562103\tEstimate time : 6.531117677688599\n",
      "1ST BBrbm_first Training loss for 31 epoch 0.49988463521003723\tEstimate time : 6.090685606002808\n",
      "1ST BBrbm_first Training loss for 32 epoch 0.5001829862594604\tEstimate time : 5.7475244998931885\n",
      "1ST BBrbm_first Training loss for 33 epoch 0.4999440312385559\tEstimate time : 6.276504755020142\n",
      "1ST BBrbm_first Training loss for 34 epoch 0.5000923871994019\tEstimate time : 11.664943933486938\n",
      "1ST BBrbm_first Training loss for 35 epoch 0.4998951852321625\tEstimate time : 9.15919303894043\n",
      "1ST BBrbm_first Training loss for 36 epoch 0.5002713799476624\tEstimate time : 7.292892932891846\n",
      "1ST BBrbm_first Training loss for 37 epoch 0.5004961490631104\tEstimate time : 6.937408685684204\n",
      "1ST BBrbm_first Training loss for 38 epoch 0.5003358721733093\tEstimate time : 7.212781667709351\n",
      "1ST BBrbm_first Training loss for 39 epoch 0.5000855922698975\tEstimate time : 7.27446436882019\n",
      "1ST BBrbm_first Training loss for 40 epoch 0.5001410841941833\tEstimate time : 6.375427722930908\n",
      "1ST BBrbm_first Training loss for 41 epoch 0.49996423721313477\tEstimate time : 6.418688535690308\n",
      "1ST BBrbm_first Training loss for 42 epoch 0.49989867210388184\tEstimate time : 5.973853826522827\n",
      "1ST BBrbm_first Training loss for 43 epoch 0.5000557899475098\tEstimate time : 5.708191633224487\n",
      "1ST BBrbm_first Training loss for 44 epoch 0.500141441822052\tEstimate time : 581.1836278438568\n",
      "1ST BBrbm_first Training loss for 45 epoch 0.5002015233039856\tEstimate time : 9.324292421340942\n",
      "1ST BBrbm_first Training loss for 46 epoch 0.49992626905441284\tEstimate time : 5.364994764328003\n",
      "1ST BBrbm_first Training loss for 47 epoch 0.4999532699584961\tEstimate time : 5.870134353637695\n",
      "1ST BBrbm_first Training loss for 48 epoch 0.49980607628822327\tEstimate time : 4.573541641235352\n",
      "1ST BBrbm_first Training loss for 49 epoch 0.5000764727592468\tEstimate time : 4.450266599655151\n",
      "1ST BBrbm_first Training loss for 50 epoch 0.5001534819602966\tEstimate time : 4.4605488777160645\n",
      "1ST BBrbm_first Training loss for 51 epoch 0.5001333951950073\tEstimate time : 4.884108304977417\n",
      "1ST BBrbm_first Training loss for 52 epoch 0.5001967549324036\tEstimate time : 5.993358612060547\n",
      "1ST BBrbm_first Training loss for 53 epoch 0.5003105998039246\tEstimate time : 4.715425491333008\n",
      "1ST BBrbm_first Training loss for 54 epoch 0.4998931884765625\tEstimate time : 9.321232318878174\n",
      "1ST BBrbm_first Training loss for 55 epoch 0.49986183643341064\tEstimate time : 8.371945858001709\n",
      "1ST BBrbm_first Training loss for 56 epoch 0.4998912513256073\tEstimate time : 6.0969719886779785\n",
      "1ST BBrbm_first Training loss for 57 epoch 0.49981462955474854\tEstimate time : 7.831204175949097\n",
      "1ST BBrbm_first Training loss for 58 epoch 0.4999934732913971\tEstimate time : 6.6639580726623535\n",
      "1ST BBrbm_first Training loss for 59 epoch 0.5002577900886536\tEstimate time : 6.118553638458252\n",
      "1ST BBrbm_first Training loss for 60 epoch 0.5001530051231384\tEstimate time : 7.7424187660217285\n",
      "1ST BBrbm_first Training loss for 61 epoch 0.5001223087310791\tEstimate time : 6.937847137451172\n",
      "1ST BBrbm_first Training loss for 62 epoch 0.4997730851173401\tEstimate time : 6.409060001373291\n",
      "1ST BBrbm_first Training loss for 63 epoch 0.5001036524772644\tEstimate time : 7.4889235496521\n",
      "1ST BBrbm_first Training loss for 64 epoch 0.49994462728500366\tEstimate time : 7.794186353683472\n",
      "1ST BBrbm_first Training loss for 65 epoch 0.5000591278076172\tEstimate time : 7.673623085021973\n",
      "1ST BBrbm_first Training loss for 66 epoch 0.49972087144851685\tEstimate time : 8.314622163772583\n"
     ]
    }
   ],
   "source": [
    "'''Train Part'''\n",
    "\n",
    "loss_ = []\n",
    "for epoch in range(EPOCH):\n",
    "    start = time.time()\n",
    "    '''First bbrbm'''\n",
    "    for i, (data) in enumerate(train_dataloader):\n",
    "        data = Variable(\n",
    "                torch.tensor(data, dtype=torch.float32)\n",
    "        ).uniform_(0, 1)\n",
    "        \n",
    "        sample_data = torch.bernoulli(data).view(-1, 10).to(device=device)\n",
    "\n",
    "        # tensor binary\n",
    "        vog_first, v1 = rbm_first(sample_data)\n",
    "        omse_loss = mse_loss(vog_first, v1)\n",
    "\n",
    "        first_train_op.zero_grad()\n",
    "        first_train_op.step()\n",
    "        omse_loss.backward()\n",
    "    output_from_first.append(v1.tolist())\n",
    "    print(\"1ST BBrbm_first Training loss for {0} epoch {1}\\tEstimate time : {2}\".format(epoch, omse_loss, time.time() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HILAB_~1\\AppData\\Local\\Temp/ipykernel_13900/2910812646.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  output_from_first = torch.tensor(output_from_first)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([90, 1040000, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HILAB_~1\\AppData\\Local\\Temp/ipykernel_13900/2910812646.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(data, dtype=torch.float32)\n",
      "c:\\Users\\HILAB_Labtop_02\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2ST BBrbm_first Training loss for 0 epoch 0.5000348091125488\tEstimate time : 17.201966762542725\n",
      "2ST BBrbm_first Training loss for 1 epoch 0.5001343488693237\tEstimate time : 15.996168851852417\n",
      "2ST BBrbm_first Training loss for 2 epoch 0.4999842345714569\tEstimate time : 15.885012865066528\n",
      "2ST BBrbm_first Training loss for 3 epoch 0.5001266598701477\tEstimate time : 16.309028148651123\n",
      "2ST BBrbm_first Training loss for 4 epoch 0.5000200271606445\tEstimate time : 21.31819486618042\n",
      "2ST BBrbm_first Training loss for 5 epoch 0.5002897381782532\tEstimate time : 19.883362770080566\n",
      "2ST BBrbm_first Training loss for 6 epoch 0.5000717639923096\tEstimate time : 19.558963537216187\n",
      "2ST BBrbm_first Training loss for 7 epoch 0.5000256896018982\tEstimate time : 22.277770042419434\n",
      "2ST BBrbm_first Training loss for 8 epoch 0.5001466274261475\tEstimate time : 21.28045392036438\n",
      "2ST BBrbm_first Training loss for 9 epoch 0.5004359483718872\tEstimate time : 18.190877437591553\n",
      "2ST BBrbm_first Training loss for 10 epoch 0.5001306533813477\tEstimate time : 19.427730560302734\n",
      "2ST BBrbm_first Training loss for 11 epoch 0.5000616312026978\tEstimate time : 17.629395246505737\n",
      "2ST BBrbm_first Training loss for 12 epoch 0.49996280670166016\tEstimate time : 17.45871090888977\n",
      "2ST BBrbm_first Training loss for 13 epoch 0.5001558661460876\tEstimate time : 19.703540563583374\n",
      "2ST BBrbm_first Training loss for 14 epoch 0.5001041293144226\tEstimate time : 20.24290657043457\n",
      "2ST BBrbm_first Training loss for 15 epoch 0.49995002150535583\tEstimate time : 16.7870934009552\n",
      "2ST BBrbm_first Training loss for 16 epoch 0.49977365136146545\tEstimate time : 18.30365490913391\n",
      "2ST BBrbm_first Training loss for 17 epoch 0.5001087784767151\tEstimate time : 17.32264232635498\n",
      "2ST BBrbm_first Training loss for 18 epoch 0.49985548853874207\tEstimate time : 19.08724856376648\n"
     ]
    }
   ],
   "source": [
    "output_from_first = torch.tensor(output_from_first)\n",
    "print(output_from_first.size())\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    '''Secnd bbrbm'''\n",
    "    start = time.time()\n",
    "    for _, (data) in enumerate(output_from_first): \n",
    "        data = Variable(\n",
    "                torch.tensor(data, dtype=torch.float32)\n",
    "        ).uniform_(0, 1)\n",
    "\n",
    "        sample_data = torch.bernoulli(data).view(-1, 10).to(device=device)\n",
    "\n",
    "        # tensor binary\n",
    "        vog_second, v2 = rbm_second(sample_data)\n",
    "        omse_loss = mse_loss(vog_second, v2)\n",
    "\n",
    "        second_train_op.zero_grad()\n",
    "        omse_loss.backward()\n",
    "        second_train_op.step()\n",
    "\n",
    "    end = time.time()\n",
    "    output_from_second.append(v2.tolist())\n",
    "    print(\"2ST BBrbm_first Training loss for {0} epoch {1}\\tEstimate time : {2}\".format(epoch, omse_loss, end - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_from_second = torch.tensor(output_from_second)\n",
    "for epoch in range(EPOCH):\n",
    "    '''Third bbrbm'''\n",
    "    for _, (data) in enumerate(output_from_second):\n",
    "        start = time.time()\n",
    "        data = Variable(\n",
    "                torch.tensor(data, dtype=torch.float32)\n",
    "        ).uniform_(0, 1)\n",
    "\n",
    "        sample_data = torch.bernoulli(data).view(-1, 10).to(device=device)\n",
    "\n",
    "        vog_third, v3 = rbm_third(sample_data)\n",
    "        omse_loss = mse_loss(vog_third, v3)\n",
    "        \n",
    "        third_train_op.zero_grad()\n",
    "        omse_loss.backward()\n",
    "        third_train_op.step()\n",
    "    \n",
    "    end = time.time()\n",
    "    output_from_third.append(v3)\n",
    "    print(\"3ST BBrbm_first Training loss for {0} epoch {1}\\tEstimate time : {2}\".format(epoch, omse_loss, end - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output_from_first)\n",
    "print(output_from_second)\n",
    "print(output_from_third)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rbm_first = RBM(n_vis=VISIBLE_UNITS[0], n_hid=HIDDEN_UNITS[0], k=K_FOLD, batch=BATCH_SIZE).to(device=device)\n",
    "# rbm_second = RBM(n_vis=VISIBLE_UNITS[1], n_hid=HIDDEN_UNITS[1], k=K_FOLD, batch=BATCH_SIZE).to(device=device)\n",
    "# rbm_third = RBM(n_vis=VISIBLE_UNITS[2], n_hid=HIDDEN_UNITS[2], k=K_FOLD, batch=BATCH_SIZE).to(device=device)\n",
    "\n",
    "output_from_first = list()\n",
    "output_from_second = list()\n",
    "\n",
    "omse_loss = list()\n",
    "mse_loss = nn.MSELoss()\n",
    "gaussian_std = torch.arange(1, 0, -0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' ** ISSUE PART ** '''\n",
    "\n",
    "loss_ = []\n",
    "for epoch in range(EPOCH):\n",
    "    '''First gbrbm'''\n",
    "    for i, (data) in enumerate(output_from_third):\n",
    "        data = Variable(\n",
    "                torch.tensor(data, dtype=torch.float32)\n",
    "        ).uniform_(0, 1)\n",
    "        \n",
    "        sample_data = torch.normal(mean=data, std=gaussian_std).view(-1, 10).to(device=device)\n",
    "\n",
    "        # tensor binary\n",
    "        vog_first, v1 = rbm_first(sample_data)\n",
    "        omse_loss = mse_loss(vog_first, v1)\n",
    "\n",
    "        first_train_op.zero_grad()\n",
    "        first_train_op.step()\n",
    "        omse_loss.backward()\n",
    "    output_from_first.append(v1.tolist())\n",
    "    print(\"1ST GBrbm_first Training loss for {0} epoch {1}\\tEstimate time : {2}\".format(epoch, omse_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_from_first = torch.tensor(output_from_first)\n",
    "output_from_third = list()\n",
    "print(output_from_first.size())\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    '''Secnd gbrbm'''\n",
    "    start = time.time()\n",
    "    for _, (data) in enumerate(output_from_first): \n",
    "        data = Variable(\n",
    "                torch.tensor(data, dtype=torch.float32)\n",
    "        ).uniform_(0, 1)\n",
    "\n",
    "        sample_data = torch.normal(mean=data, std=gaussian_std).view(-1, 10)\n",
    "\n",
    "        # tensor binary\n",
    "        vog_second, v2 = rbm_second(sample_data)\n",
    "        omse_loss = mse_loss(vog_second, v2)\n",
    "\n",
    "        second_train_op.zero_grad()\n",
    "        omse_loss.backward()\n",
    "        second_train_op.step()\n",
    "\n",
    "    end = time.time()\n",
    "    output_from_second.append(v2.tolist())\n",
    "    print(\"2ST BBrbm_first Training loss for {0} epoch {1}\\tEstimate time : {2}\".format(epoch, omse_loss, end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_from_second = torch.tensor(output_from_second)\n",
    "for epoch in range(EPOCH):\n",
    "    '''Third gbrbm'''\n",
    "    for _, (data) in enumerate(output_from_second):\n",
    "        start = time.time()\n",
    "        data = Variable(\n",
    "                torch.tensor(data, dtype=torch.float32)\n",
    "        ).uniform_(0, 1)\n",
    "\n",
    "        sample_data = torch.bernoulli(data).view(-1, 10)\n",
    "\n",
    "        vog_third, v3 = rbm_third(sample_data)\n",
    "        omse_loss = mse_loss(vog_third, v3)\n",
    "        \n",
    "        third_train_op.zero_grad()\n",
    "        omse_loss.backward()\n",
    "        third_train_op.step()\n",
    "    \n",
    "    end = time.time()\n",
    "    output_from_third.append(v3)\n",
    "    print(\"3ST BBrbm_first Training loss for {0} epoch {1}\\tEstimate time : {2}\".format(epoch, omse_loss, end - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6c3cabcf2f29820bdd7faae982b59d335e0d215fb5382d93f3312fa3292e9b7f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
