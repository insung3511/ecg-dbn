{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Pre-processing for make clean\n",
      "2022-05-13 09:32:57.880626 model.py code start\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, train_test_split\n",
    "import torch.distributions.distribution as D\n",
    "import data.medain_filtering_class as mf\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from new_RBM import new_RBM as RBM\n",
    "import matplotlib.pyplot as plt\n",
    "import data.read_samples as rs\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import datetime\n",
    "import torch\n",
    "\n",
    "print(datetime.datetime.now(), \"model.py code start\")\n",
    "\n",
    "BATCH_SIZE = 10\n",
    "EPOCH = 100\n",
    "LEARNING_RATE = 0.2\n",
    "ANNEALING_RATE = 0.999\n",
    "VISIBLE_UNITS = [180, 200, 250]\n",
    "HIDDEN_UNITS = [80, 100, 120]\n",
    "K_FOLD = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MODL] Model main code is starting....\n",
      "[INFO] Read train data, cross-vaildation data and test data from median filtering code\n",
      "[INFO] Read records file from  ./data/db1/\n",
      "[RSLT]\t\t\t Export records ...\n",
      "\t\t ['100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '111', '112', '113', '114', '115', '116', '117', '118', '119', '121', '122', '123', '124']\n",
      "[INFO]./rdsamp commending start\n",
      "[IWIP]\t\trdsamp Converting 100\n",
      "[IWIP]\t\trdsamp Converting 101\n",
      "[IWIP]\t\trdsamp Converting 102\n",
      "[IWIP]\t\trdsamp Converting 103\n",
      "[IWIP]\t\trdsamp Converting 104\n",
      "[IWIP]\t\trdsamp Converting 105\n",
      "[IWIP]\t\trdsamp Converting 106\n",
      "[IWIP]\t\trdsamp Converting 107\n",
      "[IWIP]\t\trdsamp Converting 108\n",
      "[IWIP]\t\trdsamp Converting 109\n",
      "[IWIP]\t\trdsamp Converting 111\n",
      "[IWIP]\t\trdsamp Converting 112\n",
      "[IWIP]\t\trdsamp Converting 113\n",
      "[IWIP]\t\trdsamp Converting 114\n",
      "[IWIP]\t\trdsamp Converting 115\n",
      "[IWIP]\t\trdsamp Converting 116\n",
      "[IWIP]\t\trdsamp Converting 117\n",
      "[IWIP]\t\trdsamp Converting 118\n",
      "[IWIP]\t\trdsamp Converting 119\n",
      "[IWIP]\t\trdsamp Converting 121\n",
      "[IWIP]\t\trdsamp Converting 122\n",
      "[IWIP]\t\trdsamp Converting 123\n",
      "[IWIP]\t\trdsamp Converting 124\n",
      "[INFO] Read records file from  ./data/db2/\n",
      "[RSLT]\t\t\t Export records ...\n",
      "\t\t ['200', '201', '202', '203', '205', '207', '208', '209', '210', '212', '213', '214', '215', '217', '219', '220', '221', '222', '223', '228', '230', '231', '232', '233', '234']\n",
      "[INFO]./rdsamp commending start\n",
      "[IWIP]\t\trdsamp Converting 200 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 201 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 202 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 203 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 205 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 207 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 208 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 209 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 210 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 212 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 213 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 214 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 215 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 217 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 219 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 220 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 221 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 222 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 223 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 228 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 230 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 231 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 232 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 233 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 234 ./data/db2/\n",
      "[INFO] Read records file from  ./data/db3/svdb/\n",
      "[RSLT]\t\t\t Export records ...\n",
      "\t\t ['800', '801', '802', '803', '804', '805', '806', '807', '808', '809', '810', '811', '812', '820', '821', '822', '823', '824', '825', '826', '827', '828', '829', '840', '841', '842', '843', '844', '845', '846', '847', '848', '849', '850', '851', '852', '853', '854', '855', '856', '857', '858', '859', '860', '861', '862', '863', '864', '865', '866', '867', '868', '869', '870', '871', '872', '873', '874', '875', '876', '877', '878', '879', '880', '881', '882', '883', '884', '885', '886', '887', '888', '889', '890', '891', '892', '893', '894']\n",
      "[INFO]./rdsamp commending start\n",
      "[IWIP]\t\trdsamp Converting 800 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 801 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 802 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 803 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 804 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 805 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 806 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 807 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 808 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 809 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 810 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 811 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 812 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 820 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 821 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 822 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 823 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 824 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 825 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 826 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 827 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 828 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 829 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 840 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 841 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 842 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 843 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 844 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 845 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 846 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 847 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 848 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 849 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 850 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 851 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 852 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 853 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 854 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 855 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 856 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 857 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 858 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 859 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 860 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 861 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 862 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 863 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 864 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 865 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 866 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 867 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 868 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 869 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 870 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 871 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 872 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 873 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 874 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 875 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 876 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 877 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 878 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 879 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 880 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 881 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 882 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 883 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 884 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 885 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 886 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 887 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 888 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 889 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 890 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 891 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 892 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 893 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 894 ./data/db3/svdb/\n",
      "[INFO] DB1 Filtering...\n",
      "[INFO] DB2 Filtering...\n",
      "[INFO] DB3 Filtering...\n",
      "DB1 butter size : 23, DB1 Anno size : 23\n",
      " DB2 butter size : 25, DB2 Anno size : 25\n",
      " DB3 butter size : 78, DB3 Anno size : 78\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"[MODL] Model main code is starting....\")\n",
    "\n",
    "print(\"[INFO] Read train data, cross-vaildation data and test data from median filtering code\")\n",
    "db1_sig, db1_label, db2_sig, db2_label, db3_sig, db3_label = rs.return_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     (db1_sig + db2_sig), \n",
    "#     (db1_label + db2_label),\n",
    "#     test_size=0.33,\n",
    "#     shuffle=True\n",
    "# )\n",
    "\n",
    "train_dataloader = DataLoader(db1_sig + db2_sig,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              num_workers=0, \n",
    "                              collate_fn=lambda x: x,\n",
    "                              shuffle=True)\n",
    "                              \n",
    "# test_dataloader = DataLoader(X_test,\n",
    "#                              batch_size=BATCH_SIZE,\n",
    "#                              num_workers=0, \n",
    "#                              collate_fn=lambda x: x,\n",
    "#                              shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbm_first = RBM(n_vis=VISIBLE_UNITS[0], n_hid=HIDDEN_UNITS[0], k=K_FOLD, batch=BATCH_SIZE)\n",
    "rbm_second = RBM(n_vis=VISIBLE_UNITS[1], n_hid=HIDDEN_UNITS[1], k=K_FOLD, batch=BATCH_SIZE)\n",
    "rbm_third = RBM(n_vis=VISIBLE_UNITS[2], n_hid=HIDDEN_UNITS[2], k=K_FOLD, batch=BATCH_SIZE)\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     first_train_op = optim.Adagrad(rbm_first.parameters(), 0.1)\n",
    "#     second_train_op = optim.Adagrad(rbm_second.parameters(), 0.1)\n",
    "#     third_train_op = optim.Adagrad(rbm_third.parameters(), 0.1)\n",
    "\n",
    "#     gb_first_train_op = optim.Adagrad(rbm_first.parameters(), 0.1)\n",
    "#     gb_second_train_op = optim.Adagrad(rbm_second.parameters(), 0.1)\n",
    "#     gb_third_train_op = optim.Adagrad(rbm_third.parameters(), 0.1)`\n",
    "\n",
    "first_train_op = optim.SGD(rbm_first.parameters(), 0.1)\n",
    "second_train_op = optim.SGD(rbm_second.parameters(), 0.1)\n",
    "third_train_op = optim.SGD(rbm_third.parameters(), 0.1)\n",
    "\n",
    "gb_first_train_op = optim.SGD(rbm_first.parameters(), 0.1)\n",
    "gb_second_train_op = optim.SGD(rbm_second.parameters(), 0.1)\n",
    "gb_third_train_op = optim.SGD(rbm_third.parameters(), 0.1)\n",
    "\n",
    "output_from_first = list()\n",
    "output_from_second = list()\n",
    "output_from_third = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HILAB_~1\\AppData\\Local\\Temp/ipykernel_20700/50990569.py:10: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:210.)\n",
      "  torch.tensor(data, dtype=torch.float32)\n",
      "c:\\Users\\HILAB_Labtop_02\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1ST BBrbm_first Training loss for 0 epoch 11.630414009094238\tEstimate time : 0:00:00.301427\n",
      "1ST BBrbm_first Training loss for 1 epoch 5.7153120040893555\tEstimate time : 0:00:00.287040\n",
      "1ST BBrbm_first Training loss for 2 epoch 3.695061683654785\tEstimate time : 0:00:00.350825\n",
      "1ST BBrbm_first Training loss for 3 epoch 2.6481986045837402\tEstimate time : 0:00:00.296952\n",
      "1ST BBrbm_first Training loss for 4 epoch 1.9908363819122314\tEstimate time : 0:00:00.288037\n",
      "1ST BBrbm_first Training loss for 5 epoch 1.5287971496582031\tEstimate time : 0:00:00.298999\n",
      "1ST BBrbm_first Training loss for 6 epoch 1.1784073114395142\tEstimate time : 0:00:00.284052\n",
      "1ST BBrbm_first Training loss for 7 epoch 0.8971835374832153\tEstimate time : 0:00:00.354813\n",
      "1ST BBrbm_first Training loss for 8 epoch 0.6624702215194702\tEstimate time : 0:00:00.298003\n",
      "1ST BBrbm_first Training loss for 9 epoch 0.45980319380760193\tEstimate time : 0:00:00.281063\n",
      "1ST BBrbm_first Training loss for 10 epoch 0.2806672155857086\tEstimate time : 0:00:00.316115\n",
      "1ST BBrbm_first Training loss for 11 epoch 0.11958853155374527\tEstimate time : 0:00:00.312519\n",
      "1ST BBrbm_first Training loss for 12 epoch -0.027817578986287117\tEstimate time : 0:00:00.294011\n",
      "1ST BBrbm_first Training loss for 13 epoch -0.16486692428588867\tEstimate time : 0:00:00.316940\n",
      "1ST BBrbm_first Training loss for 14 epoch -0.2929864525794983\tEstimate time : 0:00:00.297008\n",
      "1ST BBrbm_first Training loss for 15 epoch -0.4140913784503937\tEstimate time : 0:00:00.305401\n",
      "1ST BBrbm_first Training loss for 16 epoch -0.5295034050941467\tEstimate time : 0:00:00.280063\n",
      "1ST BBrbm_first Training loss for 17 epoch -0.6405313611030579\tEstimate time : 0:00:00.285047\n",
      "1ST BBrbm_first Training loss for 18 epoch -0.746905505657196\tEstimate time : 0:00:00.295016\n",
      "1ST BBrbm_first Training loss for 19 epoch -0.8504562377929688\tEstimate time : 0:00:00.283638\n",
      "1ST BBrbm_first Training loss for 20 epoch -0.9512173533439636\tEstimate time : 0:00:00.288037\n",
      "1ST BBrbm_first Training loss for 21 epoch -1.04875648021698\tEstimate time : 0:00:00.274082\n",
      "1ST BBrbm_first Training loss for 22 epoch -1.1442943811416626\tEstimate time : 0:00:00.307984\n",
      "1ST BBrbm_first Training loss for 23 epoch -1.2380317449569702\tEstimate time : 0:00:00.289035\n",
      "1ST BBrbm_first Training loss for 24 epoch -1.3303974866867065\tEstimate time : 0:00:00.270089\n",
      "1ST BBrbm_first Training loss for 25 epoch -1.420816421508789\tEstimate time : 0:00:00.288035\n",
      "1ST BBrbm_first Training loss for 26 epoch -1.5102754831314087\tEstimate time : 0:00:00.291364\n",
      "1ST BBrbm_first Training loss for 27 epoch -1.5985536575317383\tEstimate time : 0:00:00.303629\n",
      "1ST BBrbm_first Training loss for 28 epoch -1.6847130060195923\tEstimate time : 0:00:00.284049\n",
      "1ST BBrbm_first Training loss for 29 epoch -1.770248532295227\tEstimate time : 0:00:00.296057\n",
      "1ST BBrbm_first Training loss for 30 epoch -1.8555119037628174\tEstimate time : 0:00:00.287042\n",
      "1ST BBrbm_first Training loss for 31 epoch -1.9394659996032715\tEstimate time : 0:00:00.273769\n",
      "1ST BBrbm_first Training loss for 32 epoch -2.023352861404419\tEstimate time : 0:00:00.298005\n",
      "1ST BBrbm_first Training loss for 33 epoch -2.1066243648529053\tEstimate time : 0:00:00.304977\n",
      "1ST BBrbm_first Training loss for 34 epoch -2.1890628337860107\tEstimate time : 0:00:00.289032\n",
      "1ST BBrbm_first Training loss for 35 epoch -2.2707390785217285\tEstimate time : 0:00:00.295012\n",
      "1ST BBrbm_first Training loss for 36 epoch -2.352389335632324\tEstimate time : 0:00:00.287591\n",
      "1ST BBrbm_first Training loss for 37 epoch -2.4334652423858643\tEstimate time : 0:00:00.293023\n",
      "1ST BBrbm_first Training loss for 38 epoch -2.5144100189208984\tEstimate time : 0:00:00.302988\n",
      "1ST BBrbm_first Training loss for 39 epoch -2.594036340713501\tEstimate time : 0:00:00.303983\n",
      "1ST BBrbm_first Training loss for 40 epoch -2.673969030380249\tEstimate time : 0:00:00.318947\n",
      "1ST BBrbm_first Training loss for 41 epoch -2.7535088062286377\tEstimate time : 0:00:00.310964\n",
      "1ST BBrbm_first Training loss for 42 epoch -2.832145929336548\tEstimate time : 0:00:00.320496\n",
      "1ST BBrbm_first Training loss for 43 epoch -2.9107255935668945\tEstimate time : 0:00:00.292021\n",
      "1ST BBrbm_first Training loss for 44 epoch -2.989135980606079\tEstimate time : 0:00:00.325486\n",
      "1ST BBrbm_first Training loss for 45 epoch -3.0673165321350098\tEstimate time : 0:00:00.287340\n",
      "1ST BBrbm_first Training loss for 46 epoch -3.1455678939819336\tEstimate time : 0:00:00.291607\n",
      "1ST BBrbm_first Training loss for 47 epoch -3.2238998413085938\tEstimate time : 0:00:00.317942\n",
      "1ST BBrbm_first Training loss for 48 epoch -3.3010222911834717\tEstimate time : 0:00:00.304980\n",
      "1ST BBrbm_first Training loss for 49 epoch -3.3789281845092773\tEstimate time : 0:00:00.346959\n",
      "1ST BBrbm_first Training loss for 50 epoch -3.456009864807129\tEstimate time : 0:00:00.327903\n",
      "1ST BBrbm_first Training loss for 51 epoch -3.5325419902801514\tEstimate time : 0:00:00.286043\n",
      "1ST BBrbm_first Training loss for 52 epoch -3.6093013286590576\tEstimate time : 0:00:00.328900\n",
      "1ST BBrbm_first Training loss for 53 epoch -3.6851353645324707\tEstimate time : 0:00:00.323194\n",
      "1ST BBrbm_first Training loss for 54 epoch -3.7618308067321777\tEstimate time : 0:00:41.310373\n",
      "1ST BBrbm_first Training loss for 55 epoch -3.837750196456909\tEstimate time : 0:00:00.311810\n",
      "1ST BBrbm_first Training loss for 56 epoch -3.913700819015503\tEstimate time : 0:00:00.299038\n",
      "1ST BBrbm_first Training loss for 57 epoch -3.989935874938965\tEstimate time : 0:00:00.288037\n",
      "1ST BBrbm_first Training loss for 58 epoch -4.065786838531494\tEstimate time : 0:00:00.301993\n",
      "1ST BBrbm_first Training loss for 59 epoch -4.142000198364258\tEstimate time : 0:00:00.290827\n",
      "1ST BBrbm_first Training loss for 60 epoch -4.2177534103393555\tEstimate time : 0:00:00.301569\n",
      "1ST BBrbm_first Training loss for 61 epoch -4.293417453765869\tEstimate time : 0:00:00.301097\n",
      "1ST BBrbm_first Training loss for 62 epoch -4.368605136871338\tEstimate time : 0:00:00.297543\n",
      "1ST BBrbm_first Training loss for 63 epoch -4.444087028503418\tEstimate time : 0:00:00.293020\n",
      "1ST BBrbm_first Training loss for 64 epoch -4.519439220428467\tEstimate time : 0:00:00.299997\n",
      "1ST BBrbm_first Training loss for 65 epoch -4.59477424621582\tEstimate time : 0:00:00.335878\n",
      "1ST BBrbm_first Training loss for 66 epoch -4.670168876647949\tEstimate time : 0:00:00.318935\n",
      "1ST BBrbm_first Training loss for 67 epoch -4.745279788970947\tEstimate time : 0:00:00.350838\n",
      "1ST BBrbm_first Training loss for 68 epoch -4.820168972015381\tEstimate time : 0:00:00.312470\n",
      "1ST BBrbm_first Training loss for 69 epoch -4.894371509552002\tEstimate time : 0:00:00.322921\n",
      "1ST BBrbm_first Training loss for 70 epoch -4.969473838806152\tEstimate time : 0:00:00.313628\n",
      "1ST BBrbm_first Training loss for 71 epoch -5.044240951538086\tEstimate time : 0:00:00.314523\n",
      "1ST BBrbm_first Training loss for 72 epoch -5.118949890136719\tEstimate time : 0:00:00.301875\n",
      "1ST BBrbm_first Training loss for 73 epoch -5.193983554840088\tEstimate time : 0:00:00.295195\n",
      "1ST BBrbm_first Training loss for 74 epoch -5.269360542297363\tEstimate time : 0:00:00.324007\n",
      "1ST BBrbm_first Training loss for 75 epoch -5.343254566192627\tEstimate time : 0:00:00.317972\n",
      "1ST BBrbm_first Training loss for 76 epoch -5.417316913604736\tEstimate time : 0:00:00.296725\n",
      "1ST BBrbm_first Training loss for 77 epoch -5.492000579833984\tEstimate time : 0:00:00.300995\n",
      "1ST BBrbm_first Training loss for 78 epoch -5.566478729248047\tEstimate time : 0:00:00.295490\n",
      "1ST BBrbm_first Training loss for 79 epoch -5.6406636238098145\tEstimate time : 0:00:00.309985\n",
      "1ST BBrbm_first Training loss for 80 epoch -5.715291500091553\tEstimate time : 0:00:00.305008\n",
      "1ST BBrbm_first Training loss for 81 epoch -5.789615631103516\tEstimate time : 0:00:00.306527\n",
      "1ST BBrbm_first Training loss for 82 epoch -5.86397123336792\tEstimate time : 0:00:00.306006\n",
      "1ST BBrbm_first Training loss for 83 epoch -5.938436985015869\tEstimate time : 0:00:00.300000\n",
      "1ST BBrbm_first Training loss for 84 epoch -6.012811183929443\tEstimate time : 0:00:00.321436\n",
      "1ST BBrbm_first Training loss for 85 epoch -6.086501121520996\tEstimate time : 0:00:00.314947\n",
      "1ST BBrbm_first Training loss for 86 epoch -6.160533905029297\tEstimate time : 0:00:00.306489\n",
      "1ST BBrbm_first Training loss for 87 epoch -6.2342071533203125\tEstimate time : 0:00:00.337916\n",
      "1ST BBrbm_first Training loss for 88 epoch -6.308098793029785\tEstimate time : 0:00:00.358331\n",
      "1ST BBrbm_first Training loss for 89 epoch -6.38183069229126\tEstimate time : 0:00:00.280636\n",
      "1ST BBrbm_first Training loss for 90 epoch -6.456018447875977\tEstimate time : 0:00:00.297438\n",
      "1ST BBrbm_first Training loss for 91 epoch -6.5299973487854\tEstimate time : 0:00:00.295446\n",
      "1ST BBrbm_first Training loss for 92 epoch -6.604147434234619\tEstimate time : 0:00:00.286560\n",
      "1ST BBrbm_first Training loss for 93 epoch -6.677801132202148\tEstimate time : 0:00:00.303987\n",
      "1ST BBrbm_first Training loss for 94 epoch -6.751283645629883\tEstimate time : 0:00:00.288033\n",
      "1ST BBrbm_first Training loss for 95 epoch -6.824642181396484\tEstimate time : 0:00:00.346894\n",
      "1ST BBrbm_first Training loss for 96 epoch -6.898709297180176\tEstimate time : 0:00:00.330505\n",
      "1ST BBrbm_first Training loss for 97 epoch -6.9723992347717285\tEstimate time : 0:00:00.311989\n",
      "1ST BBrbm_first Training loss for 98 epoch -7.0461106300354\tEstimate time : 0:00:00.288601\n",
      "1ST BBrbm_first Training loss for 99 epoch -7.120126724243164\tEstimate time : 0:00:00.299999\n"
     ]
    }
   ],
   "source": [
    "'''Train Part'''\n",
    "\n",
    "loss_ = []\n",
    "for epoch in range(EPOCH):\n",
    "    '''First bbrbm'''\n",
    "    for i, (data) in enumerate(train_dataloader):\n",
    "        if i == 4:\n",
    "            continue\n",
    "        data = Variable(\n",
    "                torch.tensor(data, dtype=torch.float32)\n",
    "        ).uniform_(0, 1)\n",
    "        \n",
    "        sample_data = torch.bernoulli(data)\n",
    "        sample_data = torch.flatten(sample_data.clone())\n",
    "\n",
    "        # tensor binary\n",
    "        vog_first, v1, mt = rbm_first(sample_data)\n",
    "        \n",
    "        loss_first = rbm_first.free_energy(vog_first) - rbm_first.free_energy(v1)\n",
    "        loss_.append(loss_first.data)\n",
    "        \n",
    "        first_train_op.zero_grad()\n",
    "        loss_first.backward()\n",
    "        first_train_op.step()\n",
    "    \n",
    "    output_from_first.append(v1.tolist())\n",
    "    print(\"1ST BBrbm_first Training loss for {0} epoch {1}\\tEstimate time : {2}\".format(epoch, np.mean(loss_), mt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(output_from_first))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HILAB_~1\\AppData\\Local\\Temp/ipykernel_20700/2540985761.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  output_from_first = torch.tensor(output_from_first)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "2ST BBrbm_first Training loss for 0 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 1 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 2 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 3 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 4 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 5 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 6 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 7 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 8 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 9 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 10 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 11 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 12 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 13 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 14 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 15 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 16 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 17 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 18 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 19 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 20 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 21 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 22 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 23 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 24 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 25 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 26 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 27 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 28 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 29 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 30 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 31 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 32 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 33 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 34 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 35 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 36 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 37 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 38 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 39 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 40 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 41 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 42 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 43 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 44 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 45 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 46 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 47 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 48 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 49 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 50 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 51 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 52 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 53 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 54 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 55 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 56 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 57 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 58 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 59 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 60 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 61 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 62 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 63 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 64 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 65 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 66 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 67 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 68 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 69 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 70 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 71 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 72 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 73 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 74 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 75 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 76 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 77 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 78 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 79 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 80 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 81 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 82 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 83 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 84 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 85 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 86 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 87 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 88 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 89 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 90 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 91 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 92 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 93 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 94 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 95 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 96 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 97 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 98 epoch -7.120126724243164\tEstimate time : \n",
      "2ST BBrbm_first Training loss for 99 epoch -7.120126724243164\tEstimate time : \n"
     ]
    }
   ],
   "source": [
    "output_from_first = torch.tensor(output_from_first)\n",
    "print(len(output_from_first))\n",
    "for epoch in range(EPOCH):\n",
    "    '''Secnd bbrbm'''\n",
    "    for _, (data) in enumerate(output_from_first):\n",
    "        if i == 4:\n",
    "            continue\n",
    "        data = Variable(\n",
    "                torch.tensor(data, dtype=torch.float32)\n",
    "        ).uniform_(0, 1)\n",
    "\n",
    "        sample_data = torch.bernoulli(data)\n",
    "        sample_data = torch.flatten(sample_data.clone())\n",
    "\n",
    "        vog_second, v2, mt = rbm_second(sample_data)\n",
    "        \n",
    "        loss_second = rbm_second.free_energy(vog_second) - rbm_second.free_energy(v2)\n",
    "        loss_.append(loss_second.data)\n",
    "        \n",
    "        second_train_op.zero_grad()\n",
    "        loss_second.backward()\n",
    "        second_train_op.step()\n",
    "\n",
    "    output_from_second.append(v2)\n",
    "    print(\"2ST BBrbm_first Training loss for {0} epoch {1}\\tEstimate time : \".format(epoch, np.mean(loss_), mt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_from_second = torch.tensor(output_from_second)\n",
    "for epoch in range(EPOCH):\n",
    "    '''Secnd bbrbm'''\n",
    "    for _, (data) in enumerate(output_from_second):\n",
    "        if i == 4:\n",
    "            continue\n",
    "        data = Variable(\n",
    "                torch.tensor(data, dtype=torch.float32)\n",
    "        ).uniform_(0, 1)\n",
    "\n",
    "        sample_data = torch.bernoulli(data)\n",
    "        sample_data = torch.flatten(sample_data.clone())\n",
    "\n",
    "        vog_third, v3, mt = rbm_third(sample_data)\n",
    "        \n",
    "        loss_third = rbm_third.free_energy(vog_third) - rbm_third.free_energy(v3)\n",
    "        loss_.append(loss_second.data)\n",
    "        \n",
    "        third_train_op.zero_grad()\n",
    "        loss_third.backward()\n",
    "        third_train_op.step()\n",
    "\n",
    "    output_from_third.append(v3.tolist())\n",
    "    print(\"3ST BBrbm_first Training loss for {0} epoch {1}\\tEstimate time : \".format(epoch, np.mean(loss_), mt))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6c3cabcf2f29820bdd7faae982b59d335e0d215fb5382d93f3312fa3292e9b7f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
