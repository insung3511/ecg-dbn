{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Pre-processing for make clean\n",
      "2022-05-16 23:28:35.632491 model.py code start\n"
     ]
    }
   ],
   "source": [
    "from ignite.contrib.metrics.regression import *\n",
    "from ignite.contrib.metrics import *\n",
    "from ignite.handlers import *\n",
    "from ignite.metrics import *\n",
    "from ignite.engine import *\n",
    "from ignite.utils import *\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import SVC\n",
    "import pickle\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import data.read_samples as rs\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datetime\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "\n",
    "print(datetime.datetime.now(), \"model.py code start\")\n",
    "\n",
    "BATCH_SIZE = 10\n",
    "EPOCH = 10\n",
    "LEARNING_RATE = 0.2\n",
    "ANNEALING_RATE = 0.999\n",
    "VISIBLE_UNITS = [180, 200, 250]\n",
    "HIDDEN_UNITS = [80, 100, 120]\n",
    "K_FOLD = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 3080 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "print(torch.cuda.get_device_name(device))\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBM(nn.Module): \n",
    "    with torch.cuda.device(0):\n",
    "        def __init__(self, n_vis, n_hid, k, batch):\n",
    "            super(RBM, self).__init__()\n",
    "            self.W      = nn.Parameter(torch.randn(1, batch, device=device) * 1e-2)\n",
    "            self.n_vis  = n_vis\n",
    "            self.n_hid  = n_hid\n",
    "            self.k      = k\n",
    "            self.batch  = batch\n",
    "            self.v_bias = nn.Parameter(torch.zeros(n_vis, device=device))\n",
    "            self.h_bias = nn.Parameter(torch.zeros(n_hid, device=device))\n",
    "        \n",
    "        def sample_from_p(self, p):\n",
    "            return F.relu(\n",
    "                torch.sign(\n",
    "                    p - Variable(torch.randn(p.size(), device=device))\n",
    "                )\n",
    "            ).to(device=device)\n",
    "\n",
    "        ''' ISSUE PART '''\n",
    "        def v_to_h(self, v):\n",
    "            w = (self.W.clone())\n",
    "\n",
    "            p_h = F.sigmoid(\n",
    "                # F.linear(v, w, self.h_bias)\n",
    "                F.linear(v, w)\n",
    "            ).to(device=device)\n",
    "\n",
    "            sample_h = self.sample_from_p(p_h)\n",
    "            return p_h, sample_h\n",
    "\n",
    "        def h_to_v(self, h):\n",
    "            w = self.W.t().clone()\n",
    "\n",
    "            p_v = F.sigmoid(\n",
    "                # F.linear(h, w, self.v_bias)\n",
    "                F.linear(h, w)\n",
    "            ).to(device=device)\n",
    "\n",
    "            sample_v = self.sample_from_p(p_v)\n",
    "            return p_v, sample_v\n",
    "        \n",
    "        def forward(self, v):\n",
    "            pre_h1, h1 = self.v_to_h(v)\n",
    "            h_ = h1\n",
    "\n",
    "            for _ in range(self.k):\n",
    "                pre_v_, v_ = self.h_to_v(h_)\n",
    "                pre_h_, h_ = self.v_to_h(v_)\n",
    "            return v, v_\n",
    "        \n",
    "        def get_weight(self):\n",
    "            return self.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_step(engine, batch):\n",
    "    return batch\n",
    "\n",
    "default_model = nn.Sequential(OrderedDict([\n",
    "    ('base', nn.Linear(4, 2)),\n",
    "    ('fc', nn.Linear(2, 1))\n",
    "]))\n",
    "\n",
    "default_evaluator = Engine(eval_step)\n",
    "\n",
    "def get_acc(y_true, y_pred):\n",
    "    metric = Accuracy()\n",
    "    metric.attach(default_evaluator, \"accuracy\")\n",
    "    state = default_evaluator.run([[y_pred, y_true]])\n",
    "    return state.metrics[\"accuracy\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MODL] Model main code is starting....\n",
      "[INFO] Read train data, cross-vaildation data and test data from median filtering code\n",
      "[INFO] Read records file from  ./data/db1/\n",
      "[RSLT]\t\t\t Export records ...\n",
      "\t\t ['101', '106', '108', '109', '112', '114', '115', '116', '118', '119', '122', '124', '201', '203', '205', '207', '208', '209', '215', '220', '223', '230']\n",
      "[INFO]./rdsamp commending start\n",
      "[IWIP]\t\trdsamp Converting 101\n",
      "[IWIP]\t\trdsamp Converting 106\n",
      "[IWIP]\t\trdsamp Converting 108\n",
      "[IWIP]\t\trdsamp Converting 109\n",
      "[IWIP]\t\trdsamp Converting 112\n",
      "[IWIP]\t\trdsamp Converting 114\n",
      "[IWIP]\t\trdsamp Converting 115\n",
      "[IWIP]\t\trdsamp Converting 116\n",
      "[IWIP]\t\trdsamp Converting 118\n",
      "[IWIP]\t\trdsamp Converting 119\n",
      "[IWIP]\t\trdsamp Converting 122\n",
      "[IWIP]\t\trdsamp Converting 124\n",
      "[IWIP]\t\trdsamp Converting 201\n",
      "[IWIP]\t\trdsamp Converting 203\n",
      "[IWIP]\t\trdsamp Converting 205\n",
      "[IWIP]\t\trdsamp Converting 207\n",
      "[IWIP]\t\trdsamp Converting 208\n",
      "[IWIP]\t\trdsamp Converting 209\n",
      "[IWIP]\t\trdsamp Converting 215\n",
      "[IWIP]\t\trdsamp Converting 220\n",
      "[IWIP]\t\trdsamp Converting 223\n",
      "[IWIP]\t\trdsamp Converting 230\n",
      "[INFO] Read records file from  ./data/db2/\n",
      "[RSLT]\t\t\t Export records ...\n",
      "\t\t ['100', '103', '105', '111', '113', '117', '121', '123', '200', '202', '210', '212', '213', '214', '219', '221', '222', '228', '231', '232', '233', '234']\n",
      "[INFO]./rdsamp commending start\n",
      "[IWIP]\t\trdsamp Converting 100 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 103 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 105 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 111 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 113 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 117 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 121 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 123 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 200 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 202 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 210 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 212 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 213 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 214 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 219 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 221 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 222 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 228 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 231 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 232 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 233 ./data/db2/\n",
      "[IWIP]\t\trdsamp Converting 234 ./data/db2/\n",
      "[INFO] Read records file from  ./data/db3/svdb/\n",
      "[RSLT]\t\t\t Export records ...\n",
      "\t\t ['800', '801', '802', '803', '804', '805', '806', '807', '808', '809', '810', '811', '812', '820', '821', '822', '823', '824', '825', '826', '827', '828', '829', '840', '841', '842', '843', '844', '845', '846', '847', '848', '849', '850', '851', '852', '853', '854', '855', '856', '857', '858', '859', '860', '861', '862', '863', '864', '865', '866', '867', '868', '869', '870', '871', '872', '873', '874', '875', '876', '877', '878', '879', '880', '881', '882', '883', '884', '885', '886', '887', '888', '889', '890', '891', '892', '893', '894']\n",
      "[INFO]./rdsamp commending start\n",
      "[IWIP]\t\trdsamp Converting 800 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 801 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 802 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 803 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 804 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 805 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 806 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 807 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 808 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 809 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 810 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 811 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 812 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 820 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 821 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 822 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 823 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 824 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 825 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 826 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 827 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 828 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 829 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 840 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 841 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 842 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 843 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 844 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 845 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 846 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 847 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 848 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 849 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 850 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 851 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 852 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 853 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 854 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 855 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 856 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 857 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 858 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 859 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 860 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 861 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 862 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 863 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 864 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 865 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 866 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 867 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 868 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 869 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 870 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 871 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 872 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 873 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 874 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 875 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 876 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 877 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 878 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 879 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 880 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 881 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 882 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 883 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 884 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 885 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 886 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 887 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 888 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 889 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 890 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 891 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 892 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 893 ./data/db3/svdb/\n",
      "[IWIP]\t\trdsamp Converting 894 ./data/db3/svdb/\n",
      "[INFO] DB1 Filtering...\n",
      "[INFO] DB2 Filtering...\n",
      "[INFO] DB3 Filtering...\n",
      "DB1 butter size : 22, DB1 Anno size : 22\n",
      " DB2 butter size : 22, DB2 Anno size : 22\n",
      " DB3 butter size : 78, DB3 Anno size : 78\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"[MODL] Model main code is starting....\")\n",
    "\n",
    "print(\"[INFO] Read train data, cross-vaildation data and test data from median filtering code\")\n",
    "db1_sig, db1_label, db2_sig, db2_label, db3_sig, db3_label = rs.return_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = []\n",
    "cross_dataset = []\n",
    "test_dataset = []\n",
    "\n",
    "for i in range(len(db1_sig)):\n",
    "    train_dataset.append([db1_sig[i], db1_label[i]])\n",
    "\n",
    "for i in range(len(db2_sig)):\n",
    "    cross_dataset.append([db2_sig[i], db2_label[i]])\n",
    "\n",
    "for i in range(len(db3_sig)):\n",
    "    test_dataset.append([db3_sig[i], db3_label[i]])\n",
    "\n",
    "train_dataloader = DataLoader(db1_sig,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              num_workers=0, \n",
    "                              collate_fn=lambda x: x,\n",
    "                              shuffle=True)\n",
    "\n",
    "cross_dataloader = DataLoader(db2_sig,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              num_workers=0,\n",
    "                              collate_fn=lambda x: x,\n",
    "                              shuffle=True)  \n",
    "                            \n",
    "test_dataloader = DataLoader(db3_sig,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             num_workers=0, \n",
    "                             collate_fn=lambda x: x,\n",
    "                             shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbm_first = RBM(n_vis=VISIBLE_UNITS[0], n_hid=HIDDEN_UNITS[0], k=K_FOLD, batch=BATCH_SIZE).to(device=device)\n",
    "rbm_second = RBM(n_vis=VISIBLE_UNITS[1], n_hid=HIDDEN_UNITS[1], k=K_FOLD, batch=BATCH_SIZE).to(device=device)\n",
    "rbm_third = RBM(n_vis=VISIBLE_UNITS[2], n_hid=HIDDEN_UNITS[2], k=K_FOLD, batch=BATCH_SIZE).to(device=device)\n",
    "\n",
    "first_train_op = optim.Adagrad(rbm_first.parameters(), LEARNING_RATE)\n",
    "second_train_op = optim.Adagrad(rbm_second.parameters(), LEARNING_RATE)\n",
    "third_train_op = optim.Adagrad(rbm_third.parameters(), LEARNING_RATE)\n",
    "\n",
    "gb_first_train_op = optim.Adagrad(rbm_first.parameters(), LEARNING_RATE)\n",
    "gb_second_train_op = optim.Adagrad(rbm_second.parameters(), LEARNING_RATE)\n",
    "gb_third_train_op = optim.Adagrad(rbm_third.parameters(), LEARNING_RATE)\n",
    "\n",
    "omse_loss = list()\n",
    "output_gb = list()\n",
    "best_acc = float()\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "gaussian_std = torch.arange(1, 0, -0.1, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GB-DBN Train Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RBM START!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HILAB_~1\\AppData\\Local\\Temp/ipykernel_25128/1478556841.py:14: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:210.)\n",
      "  torch.tensor(data, dtype=torch.float32)\n",
      "c:\\Users\\HILAB_Labtop_02\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First BBRBM Passed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HILAB_~1\\AppData\\Local\\Temp/ipykernel_25128/1478556841.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(data, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second BBRBM Passed\n",
      "Third BBRBM Passed\n",
      "First GBRBM Passed\n",
      "Second GBRBM Passed\n",
      "Third GBRBM Passed\n",
      "GB-DBN Training loss for 1th epoch 0.20000000298023224\tEstimate time : 0.0030012130737304688\tAcc : 80.0\tBest Acc : 80.0\t\tIgnite Acc: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HILAB_~1\\AppData\\Local\\Temp/ipykernel_25128/1478556841.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(data, dtype=torch.float32)\n",
      "C:\\Users\\HILAB_~1\\AppData\\Local\\Temp/ipykernel_25128/1478556841.py:76: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(data, dtype=torch.float32)\n",
      "C:\\Users\\HILAB_~1\\AppData\\Local\\Temp/ipykernel_25128/1478556841.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(data, dtype=torch.float32)\n",
      "C:\\Users\\HILAB_~1\\AppData\\Local\\Temp/ipykernel_25128/1478556841.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(data, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First BBRBM Passed\n",
      "Second BBRBM Passed\n",
      "Third BBRBM Passed\n",
      "First GBRBM Passed\n",
      "Second GBRBM Passed\n",
      "Third GBRBM Passed\n",
      "GB-DBN Training loss for 2th epoch 0.5\tEstimate time : 0.0025773048400878906\tAcc : 50.0\tBest Acc : 80.0\t\tIgnite Acc: 0.0\n",
      "First BBRBM Passed\n",
      "Second BBRBM Passed\n",
      "Third BBRBM Passed\n",
      "First GBRBM Passed\n",
      "Second GBRBM Passed\n",
      "Third GBRBM Passed\n",
      "GB-DBN Training loss for 3th epoch 0.30000001192092896\tEstimate time : 0.003054380416870117\tAcc : 70.0\tBest Acc : 80.0\t\tIgnite Acc: 0.0\n",
      "First BBRBM Passed\n",
      "Second BBRBM Passed\n",
      "Third BBRBM Passed\n",
      "First GBRBM Passed\n",
      "Second GBRBM Passed\n",
      "Third GBRBM Passed\n",
      "GB-DBN Training loss for 4th epoch 0.6000000238418579\tEstimate time : 0.002999544143676758\tAcc : 40.0\tBest Acc : 80.0\t\tIgnite Acc: 0.0\n",
      "First BBRBM Passed\n",
      "Second BBRBM Passed\n",
      "Third BBRBM Passed\n",
      "First GBRBM Passed\n",
      "Second GBRBM Passed\n",
      "Third GBRBM Passed\n",
      "GB-DBN Training loss for 5th epoch 0.699999988079071\tEstimate time : 0.0050051212310791016\tAcc : 30.0\tBest Acc : 80.0\t\tIgnite Acc: 0.0\n",
      "First BBRBM Passed\n",
      "Second BBRBM Passed\n",
      "Third BBRBM Passed\n",
      "First GBRBM Passed\n",
      "Second GBRBM Passed\n",
      "Third GBRBM Passed\n",
      "GB-DBN Training loss for 6th epoch 0.20000000298023224\tEstimate time : 0.003999948501586914\tAcc : 80.0\tBest Acc : 80.0\t\tIgnite Acc: 0.0\n",
      "First BBRBM Passed\n",
      "Second BBRBM Passed\n",
      "Third BBRBM Passed\n",
      "First GBRBM Passed\n",
      "Second GBRBM Passed\n",
      "Third GBRBM Passed\n",
      "GB-DBN Training loss for 7th epoch 0.30000001192092896\tEstimate time : 0.00500178337097168\tAcc : 70.0\tBest Acc : 80.0\t\tIgnite Acc: 0.0\n",
      "First BBRBM Passed\n",
      "Second BBRBM Passed\n",
      "Third BBRBM Passed\n",
      "First GBRBM Passed\n",
      "Second GBRBM Passed\n",
      "Third GBRBM Passed\n",
      "GB-DBN Training loss for 8th epoch 0.5\tEstimate time : 0.0039958953857421875\tAcc : 50.0\tBest Acc : 80.0\t\tIgnite Acc: 0.0\n",
      "First BBRBM Passed\n",
      "Second BBRBM Passed\n",
      "Third BBRBM Passed\n",
      "First GBRBM Passed\n",
      "Second GBRBM Passed\n",
      "Third GBRBM Passed\n",
      "GB-DBN Training loss for 9th epoch 0.800000011920929\tEstimate time : 0.004001617431640625\tAcc : 20.0\tBest Acc : 80.0\t\tIgnite Acc: 0.0\n",
      "First BBRBM Passed\n",
      "Second BBRBM Passed\n",
      "Third BBRBM Passed\n",
      "First GBRBM Passed\n",
      "Second GBRBM Passed\n",
      "Third GBRBM Passed\n",
      "GB-DBN Training loss for 10th epoch 0.6000000238418579\tEstimate time : 0.005503654479980469\tAcc : 40.0\tBest Acc : 80.0\t\tIgnite Acc: 0.0\n"
     ]
    }
   ],
   "source": [
    "'''BBRBM Train Part'''\n",
    "\n",
    "loss_ = []\n",
    "output_bb = []\n",
    "print(\"RBM START!\")\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    tmp_acc = float()\n",
    "    run_acc = float()\n",
    "    start = time.time()\n",
    "    '''First bbrbm'''\n",
    "    for i, (data) in enumerate(train_dataloader):\n",
    "        data = Variable(\n",
    "                torch.tensor(data, dtype=torch.float32)\n",
    "        ).uniform_(0, 1)\n",
    "        \n",
    "        sample_data = torch.bernoulli(data).view(-1, 10).to(device=device)\n",
    "\n",
    "        # tensor binary\n",
    "        vog_first, v1 = rbm_first(sample_data)\n",
    "        omse_loss = mse_loss(vog_first, v1)\n",
    "\n",
    "        first_train_op.zero_grad()\n",
    "        first_train_op.step()\n",
    "        omse_loss.backward()\n",
    "    print(\"First BBRBM Passed\")\n",
    "\n",
    "    for _, (data) in enumerate(v1): \n",
    "        data = Variable(\n",
    "                torch.tensor(data, dtype=torch.float32)\n",
    "        ).uniform_(0, 1)\n",
    "\n",
    "        sample_data = torch.bernoulli(data).view(-1, 10).to(device=device)\n",
    "\n",
    "        # tensor binary\n",
    "        vog_second, v2 = rbm_second(sample_data)\n",
    "        omse_loss = mse_loss(vog_second, v2)\n",
    "\n",
    "        second_train_op.zero_grad()\n",
    "        omse_loss.backward()\n",
    "        second_train_op.step()\n",
    "    print(\"Second BBRBM Passed\")\n",
    "    \n",
    "    for _, (data) in enumerate(v2):\n",
    "        start = time.time()\n",
    "        data = Variable(\n",
    "                torch.tensor(data, dtype=torch.float32)\n",
    "        ).uniform_(0, 1)\n",
    "\n",
    "        sample_data = torch.bernoulli(data).view(-1, 10).to(device=device)\n",
    "\n",
    "        vog_third, v3 = rbm_third(sample_data)\n",
    "        omse_loss = mse_loss(vog_third, v3)\n",
    "        \n",
    "        third_train_op.zero_grad()\n",
    "        omse_loss.backward()\n",
    "        third_train_op.step()\n",
    "\n",
    "        run_acc += (sample_data == v3).sum().item()\n",
    "    \n",
    "    print(\"Third BBRBM Passed\")\n",
    "    acc = (run_acc / v3.size()[1]) * 100\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "\n",
    "    path = \"./saveMode_through_BBRBM.pth\"\n",
    "    torch.save(rbm_second.state_dict(), path)\n",
    "    output_bb.append(v3)\n",
    "\n",
    "    '''\n",
    "GBRBM GBRBM GBRBM GBRBM GBRBM GBRBM GBRBM \n",
    "    '''\n",
    "\n",
    "    for i, (data) in enumerate(output_bb):\n",
    "        data = Variable(\n",
    "                torch.tensor(data, dtype=torch.float32)\n",
    "        ).uniform_(0, 1)\n",
    "        \n",
    "        sample_data = torch.normal(mean=data, std=gaussian_std).view(-1, 10).to(device=device)\n",
    "\n",
    "        # tensor binary\n",
    "        vog_first, v1 = rbm_first(sample_data)\n",
    "        omse_loss = mse_loss(vog_first, v1)\n",
    "\n",
    "        first_train_op.zero_grad()\n",
    "        first_train_op.step()\n",
    "        omse_loss.backward()\n",
    "    print(\"First GBRBM Passed\")\n",
    "\n",
    "    for _, (data) in enumerate(v1): \n",
    "        data = Variable(\n",
    "                torch.tensor(data, dtype=torch.float32)\n",
    "        ).uniform_(0, 1)\n",
    "\n",
    "        sample_data = torch.normal(mean=data, std=gaussian_std).view(-1, 10).to(device=device)\n",
    "\n",
    "        # tensor binary\n",
    "        vog_second, v2 = rbm_second(sample_data)\n",
    "        omse_loss = mse_loss(vog_second, v2)\n",
    "\n",
    "        second_train_op.zero_grad()\n",
    "        omse_loss.backward()\n",
    "        second_train_op.step()\n",
    "    print(\"Second GBRBM Passed\")\n",
    "\n",
    "\n",
    "    for _, (data) in enumerate(v2):\n",
    "        start = time.time()\n",
    "        data = Variable(\n",
    "                torch.tensor(data, dtype=torch.float32)\n",
    "        ).uniform_(0, 1)\n",
    "\n",
    "        sample_data = torch.bernoulli(data).view(-1, 10).to(device=device)\n",
    "\n",
    "        vog_third, v3_e = rbm_third(sample_data)\n",
    "        omse_loss = mse_loss(vog_third, v3_e)\n",
    "        \n",
    "        third_train_op.zero_grad()\n",
    "        omse_loss.backward()\n",
    "        third_train_op.step()\n",
    "        run_acc += (torch.bernoulli(data).view(-1, 10).to(device=device) == v3_e).sum().item()\n",
    "    print(\"Third GBRBM Passed\")\n",
    "\n",
    "    acc = get_acc(vog_third, v3_e) * 100\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc    \n",
    "\n",
    "    path = \"./saveMode_through_GBRBM.pth\"\n",
    "    torch.save(rbm_third.state_dict(), path)\n",
    "    output_gb.append(v3_e)\n",
    "\n",
    "    print(\"GB-DBN Training loss for {0}th epoch {1}\\tEstimate time : {2}\\tAcc : {3}\\tBest Acc : {4}\\t\\tIgnite Acc: {5}\" \\\n",
    "        .format(epoch + 1, omse_loss, time.time() - start, acc, best_acc, tmp_acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Accuracy :  40.0 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Last Accuracy : \", acc, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HILAB_~1\\AppData\\Local\\Temp/ipykernel_25128/495881628.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(output_gb[i], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "output_gb_cp = list()\n",
    "cpu = torch.device('cpu')\n",
    "\n",
    "for i in range(len(output_gb)):\n",
    "    output_gb_cp.append(\n",
    "        Variable(torch.flatten(\n",
    "            torch.tensor(output_gb[i], dtype=torch.float64)\n",
    "        ).to(device=cpu), requires_grad=True).detach().numpy()\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n",
      "78\n",
      "2434\n"
     ]
    }
   ],
   "source": [
    "print(len(output_gb_cp))\n",
    "print(len(output_gb_cp[-1]))\n",
    "\n",
    "print(len(db3_label))\n",
    "print(len(db3_label[-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "db3_label_chan = list()\n",
    "v_model_acc = list()\n",
    "\n",
    "for i in range(len(db3_label)):\n",
    "    temp_list = []\n",
    "    temp_bool = []\n",
    "    for j in range(2577):\n",
    "        try:\n",
    "            temp_str = db3_label[i][j]\n",
    "        except IndexError:\n",
    "            temp_str = \"\"\n",
    "\n",
    "        if temp_str == \"V\":\n",
    "            temp_list.append(0)\n",
    "            temp_bool.append(True)\n",
    "\n",
    "        else:\n",
    "            temp_list.append(1)\n",
    "            temp_bool.append(False)\n",
    "\n",
    "    db3_label_chan.append(temp_list)\n",
    "    v_model_acc.append(temp_bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HILAB_~1\\AppData\\Local\\Temp/ipykernel_25128/866390277.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x)\n",
      "C:\\Users\\HILAB_~1\\AppData\\Local\\Temp/ipykernel_25128/866390277.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(y)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (20) must match the size of tensor b (201006) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\HILAB_~1\\AppData\\Local\\Temp/ipykernel_25128/866390277.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mpredicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         correct += float(\n\u001b[1;32m---> 41\u001b[1;33m             \u001b[0mpredicted\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlist_to_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdb3_label_chan\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m         )\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (20) must match the size of tensor b (201006) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "class SVM(nn.Module):\n",
    "    def __init__(self, lr, n_x):\n",
    "        super(SVM, self).__init__()\n",
    "        self.lr = lr\n",
    "        self.fully = nn.Linear(n_x, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        fwd = self.fully(x)\n",
    "        return fwd\n",
    "\n",
    "X = torch.FloatTensor(db3_label_chan)\n",
    "Y = torch.FloatTensor(db3_label_chan)\n",
    "N = len(Y)\n",
    "\n",
    "model = SVM(lr=LEARNING_RATE, n_x=2577)\n",
    "optimizer = optim.Adagrad(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "model.train()\n",
    "\n",
    "correct = 0.\n",
    "cnt_tot = 0\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    perm = torch.randperm(N)\n",
    "    for  i in range(0, N, BATCH_SIZE):\n",
    "        x = X[perm[i:i + BATCH_SIZE]]\n",
    "        y = Y[perm[i:i + BATCH_SIZE]]\n",
    "\n",
    "        x = torch.tensor(x)\n",
    "        y = torch.tensor(y)\n",
    "\n",
    "        # Forward\n",
    "        output = model(x)\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()        \n",
    "        optimizer.step()\n",
    "\n",
    "        predicted = output.data >= 0\n",
    "        correct += float(\n",
    "            predicted.view(-1) == torch.tensor(rs.list_to_list(db3_label_chan), dtype=torch.float32)\n",
    "        )\n",
    "\n",
    "        cnt_tot += 1\n",
    "    print(\"Epoch: {}\\tLoss: {}\\tTotal Cnt: {}\".format(epoch, correct, cnt_tot))\n",
    "\n",
    "x_out, y_out = [], []\n",
    "for i in range(len(output.data.tolist())):\n",
    "    x_out.append(output.data.tolist()[i][0])\n",
    "    y_out.append(output.data.tolist()[i][1])\n",
    "\n",
    "plt.scatter(x_out, y_out)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Code GB-DBN Start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HILAB_Labtop_02\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "C:\\Users\\HILAB_~1\\AppData\\Local\\Temp/ipykernel_25128/148068256.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(data, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GB-DBN Training loss for 10th epoch 1.016919493675232\tEstimate time : 39426.670011758804\tAcc : 0.3\tBest Acc : 0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HILAB_~1\\AppData\\Local\\Temp/ipykernel_25128/148068256.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(data, dtype=torch.float32)\n",
      "C:\\Users\\HILAB_~1\\AppData\\Local\\Temp/ipykernel_25128/148068256.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(data, dtype=torch.float32)\n",
      "C:\\Users\\HILAB_~1\\AppData\\Local\\Temp/ipykernel_25128/148068256.py:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(data, dtype=torch.float32)\n",
      "C:\\Users\\HILAB_~1\\AppData\\Local\\Temp/ipykernel_25128/148068256.py:82: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(data, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "run_acc = float()\n",
    "best_acc = float()\n",
    "print(\"Test Code GB-DBN Start\")\n",
    "for i, (data) in enumerate(test_dataloader):\n",
    "        data = Variable(\n",
    "                torch.tensor(data, dtype=torch.float32)\n",
    "        ).uniform_(0, 1)\n",
    "        \n",
    "        sample_data = torch.bernoulli(data).view(-1, 10).to(device=device)\n",
    "\n",
    "        # tensor binary\n",
    "        vog_first, v1 = rbm_first(sample_data)\n",
    "        omse_loss = mse_loss(vog_first, v1)\n",
    "\n",
    "        first_train_op.zero_grad()\n",
    "        first_train_op.step()\n",
    "        omse_loss.backward()\n",
    "\n",
    "for i, (data) in enumerate(v1):\n",
    "        data = Variable(\n",
    "                torch.tensor(data, dtype=torch.float32)\n",
    "        ).uniform_(0, 1)\n",
    "        \n",
    "        sample_data = torch.bernoulli(data).view(-1, 10).to(device=device)\n",
    "\n",
    "        # tensor binary\n",
    "        vog_second, v2 = rbm_first(sample_data)\n",
    "        omse_loss = mse_loss(vog_second, v2)\n",
    "\n",
    "        second_train_op.zero_grad()\n",
    "        second_train_op.step()\n",
    "        omse_loss.backward()\n",
    "\n",
    "for i, (data) in enumerate(v2):\n",
    "        data = Variable(\n",
    "                torch.tensor(data, dtype=torch.float32)\n",
    "        ).uniform_(0, 1)\n",
    "        \n",
    "        sample_data = torch.bernoulli(data).view(-1, 10).to(device=device)\n",
    "\n",
    "        # tensor binary\n",
    "        vog_second, v3 = rbm_first(sample_data)\n",
    "        omse_loss = mse_loss(vog_second, v3)\n",
    "\n",
    "        second_train_op.zero_grad()\n",
    "        second_train_op.step()\n",
    "        omse_loss.backward()\n",
    "        run_acc += (sample_data == v3).sum().item()\n",
    " \n",
    "for _, (data) in enumerate(v3): \n",
    "        data = Variable(\n",
    "                torch.tensor(data, dtype=torch.float32)\n",
    "        ).uniform_(0, 1)\n",
    "\n",
    "        sample_data = torch.normal(mean=data, std=gaussian_std).view(-1, 10).to(device=device)\n",
    "\n",
    "        # tensor binary\n",
    "        vog_second, v1 = rbm_first(sample_data)\n",
    "        omse_loss = mse_loss(vog_second, v1)\n",
    "\n",
    "        second_train_op.zero_grad()\n",
    "        omse_loss.backward()\n",
    "        second_train_op.step()\n",
    "\n",
    "for _, (data) in enumerate(v1): \n",
    "        data = Variable(\n",
    "                torch.tensor(data, dtype=torch.float32)\n",
    "        ).uniform_(0, 1)\n",
    "\n",
    "        sample_data = torch.normal(mean=data, std=gaussian_std).view(-1, 10).to(device=device)\n",
    "\n",
    "        # tensor binary\n",
    "        vog_second, v2 = rbm_second(sample_data)\n",
    "        omse_loss = mse_loss(vog_second, v2)\n",
    "\n",
    "        second_train_op.zero_grad()\n",
    "        omse_loss.backward()\n",
    "        second_train_op.step()\n",
    "        \n",
    "for _, (data) in enumerate(v2): \n",
    "        data = Variable(\n",
    "                torch.tensor(data, dtype=torch.float32)\n",
    "        ).uniform_(0, 1)\n",
    "\n",
    "        sample_data = torch.normal(mean=data, std=gaussian_std).view(-1, 10).to(device=device)\n",
    "\n",
    "        # tensor binary\n",
    "        vog_second, v3 = rbm_third(sample_data)\n",
    "        omse_loss = mse_loss(vog_second, v3)\n",
    "\n",
    "        second_train_op.zero_grad()\n",
    "        omse_loss.backward()\n",
    "        second_train_op.step()\n",
    "        \n",
    "correct = 0.\n",
    "total = 0.\n",
    "\n",
    "acc = get_acc(vog_third, v3)\n",
    "if acc > best_acc:\n",
    "    best_acc = acc\n",
    "\n",
    "path = \"./saveMode_through_GBRBM.pth\"\n",
    "torch.save(rbm_third.state_dict(), path)\n",
    "\n",
    "output_gb.append(v3)\n",
    "print(\"GB-DBN Training loss for {0}th epoch {1}\\tEstimate time : {2}\\tAcc : {3}\\tBest Acc : {4}\" \\\n",
    "    .format(epoch + 1, omse_loss, time.time() - start, acc, best_acc))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6c3cabcf2f29820bdd7faae982b59d335e0d215fb5382d93f3312fa3292e9b7f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
